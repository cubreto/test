{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing cleaner.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile cleaner.py\n",
    "# %load cleaner.py\n",
    "from config import Config\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "class Cleaner:\n",
    "    \"\"\"\n",
    "    A class used to clean and preprocess the data.\n",
    "    ...\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        a pandas DataFrame containing the data to be cleaned\n",
    "    table1 : pd.DataFrame\n",
    "        a pandas DataFrame used for storing intermediate results\n",
    "        \n",
    "    Methods\n",
    "    -------\n",
    "    \n",
    "    select_columns()\n",
    "        Selects relevant columns from the DataFrame.\n",
    "    get_alias()\n",
    "    \n",
    "        Extracts aliases from the notes.\n",
    "    remove_newlines()\n",
    "    \n",
    "        Removes newline characters from the notes.\n",
    "    remove_urls()\n",
    "    \n",
    "        Removes URLs from the notes.\n",
    "    to_lowercase()\n",
    "    \n",
    "        Converts all text in the DataFrame to lowercase.\n",
    "    ...\n",
    "    \"\"\"\n",
    "        \n",
    "    def __init__(self,df):\n",
    "        self.df = df.copy()\n",
    "        self.record_drops = []\n",
    "        \n",
    "    def select_columns(self):\n",
    "        \"\"\"\n",
    "        Selects relevant columns from the DataFrame.\n",
    "        Returns\n",
    "        -------\n",
    "        self\n",
    "        \"\"\"\n",
    "        old_len = len(self.df)\n",
    "        self.df = self.df[['BUSINESS_UNIT', 'HIT_ID','MATCH_TAG', 'MATCH_TEXT','MATCH_PATTERN_VALUE', 'MATCH_VALUE', 'L1_MAKER_LISTTYPE', 'MATCHABLE_DESCRIPTION',\\\n",
    "                           'OFFSET', 'L1_MAKER_ACTION', 'L1_MAKER_NOTE', 'L1_MAKER_INPUTTYPE', 'L2_MAKER_ACTION', 'L2_MAKER_NOTE']]\n",
    "        self.df.columns = self.df.columns.str.lower()\n",
    "        new_len = len(self.df)\n",
    "        self.record_drops.append(('Select Columns', old_len - new_len))\n",
    "        return self\n",
    "    \n",
    "    def filter_II_combinations(self):\n",
    "        \"\"\"\n",
    "        Filters the dataframe to only keep rows where both l1_maker_inputtype \n",
    "        and l1_maker_listtype are 'Individual'.\n",
    "        \"\"\"\n",
    "        self.df = self.df[\n",
    "            (self.df['l1_maker_inputtype'] == 'Individual') & \n",
    "            (self.df['l1_maker_listtype'] == 'Individual')\n",
    "        ]\n",
    "        return self\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_alias(text):\n",
    "        cleaned=re.findall(r'[^#!@,~;.-]+', text)\n",
    "        cleaned= ' '.join(cleaned)\n",
    "        aka=re.findall(r'AKA (.*?)TYPE', cleaned)\n",
    "        aka=' '.join(aka)\n",
    "        return aka.strip()\n",
    "    \n",
    "    def get_alias(self):\n",
    "        self.df[\"aka\"] = self.df[\"matchable_description\"].astype(str).apply(self.extract_alias)\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def preprocess_matchable_description(text):\n",
    "        cleaned = re.sub(r'\\b[\\w\\-.]+?@\\w+?\\.\\w{2,4}\\b', 'emailaddr', text)\n",
    "        cleaned = re.sub(r'(http[s]?\\S+)|(\\w+\\.[A-Za-z]{2,4}\\S*)', 'httpaddr',\n",
    "                         cleaned)\n",
    "        cleaned = re.sub(r'£|\\$', 'moneysymb', cleaned)\n",
    "        cleaned = re.sub(\n",
    "            r'\\b(\\+\\d{1,2}\\s)?\\d?[\\-(.]?\\d{3}\\)?[\\s.-]?\\d{3}[\\s.-]?\\d{4}\\b',\n",
    "            'phonenumbr', cleaned)\n",
    "        cleaned = re.sub(r'\\d+(\\.\\d+)?', 'numbr', cleaned)\n",
    "        cleaned = re.sub(r'[^\\w\\d\\s]', ' ', cleaned)\n",
    "        cleaned = re.sub(r'\\s+', ' ', cleaned)\n",
    "        cleaned = re.sub(r'^\\s+|\\s+?$', '', cleaned.lower())\n",
    "        return cleaned.strip()\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_list_fullname(Matchable_Description):\n",
    "        cleaned= re.sub(r'\\bINPUT_DATE\\b.+', '.', Matchable_Description)\n",
    "        cleaned=re.findall(r'[^''#!@,]+', cleaned)\n",
    "        cleaned= ' '.join(cleaned)\n",
    "        hit=re.findall(r'(.*?)RISK_ID', cleaned)\n",
    "        hit=' '.join(hit)\n",
    "        hit=hit.replace(\"'\",\"\")\n",
    "        list_name=re.sub(r'\\s+',' ',hit).strip()\n",
    "        return list_name\n",
    "    \n",
    "    def get_list_name(self):\n",
    "        self.df[\"list_name\"] = self.df[\"matchable_description\"].astype(str).apply(self.get_list_fullname)\n",
    "        return self\n",
    "    \n",
    "    def clean_key_columns(self):\n",
    "        key_columns = ['match_tag', 'match_value', 'match_text', 'match_pattern_value', 'list_name']\n",
    "    \n",
    "        # Replace punctuation in key columns\n",
    "        for col in key_columns:\n",
    "            self.df[col] = self.df[col].str.replace(',', '', regex=True)\n",
    "    \n",
    "        return self  \n",
    "\n",
    "    def remove_inconclusive_matches(self):\n",
    "        self.df = self.df[self.df['l1_maker_action'] != 'Potential Inconclusive Match']\n",
    "        new_len = len(self.df)\n",
    "        self.record_drops.append(('Remove Inconclusive Matches', old_len - new_len))\n",
    "        return self\n",
    "    \n",
    "    def clean_l1_maker_note(self):\n",
    "        self.df = self.df[~self.df['l1_maker_note'].str.lower().str.contains('pvt|guidance', na=False)]\n",
    "        return self\n",
    "    \n",
    "    def clean_l2_maker_note(self):\n",
    "        self.df = self.df[~self.df['l2_maker_note'].str.lower().str.contains('external|google|https', na=False)]\n",
    "        return self\n",
    "    \n",
    "\n",
    "    def remove_newlines(self):\n",
    "        self.df[\"l1_maker_note\"] = self.df[\"l1_maker_note\"].str.replace(\"\\n\", \" \", regex=True)\n",
    "        return self\n",
    "\n",
    "    def remove_urls(self):\n",
    "        self.df[\"l1_maker_note\"] = self.df[\"l1_maker_note\"].str.replace(r\"http\\S+|www\\S+\", \"\", regex=True)\n",
    "        return self\n",
    "\n",
    "    def to_lowercase(self):\n",
    "        self.df[\"l1_maker_note\"] = self.df[\"l1_maker_note\"].str.lower()\n",
    "        return self\n",
    "\n",
    "    def standardize_phrases(self):\n",
    "        # Define a dictionary of phrases to standardize\n",
    "        \n",
    "        phrases_to_standardize = {\n",
    "        \"cannot exclude without additional investigation\": \"cannot exclude without further investigation\",\n",
    "        \"wykluczam bazujac na caloksztalcie informacji\": \"cannot exclude without further investigation\",\n",
    "        \"wykluczam hit bazurac na cakoksztaicie informacii\": \"cannot exclude without further investigation\",\n",
    "        \"wykluczam hit bazujac na catoksztaicie informac]a dotyczy os\": \"cannot exclude without further investigation\",\n",
    "        \"input name has additional name components that do not match those of the list entry (including aliases).\":\"input name has additional components not matching list entry\",\n",
    "        \"taxonomy (e.g., icrm sanctions) specific guidance applies.\": \"taxonomy specific guidance applies\",\n",
    "        \"dob mismatch input entry dob\": \"date of birth mismatch\",\n",
    "        \"dotyczy imienia/nazwiska płatność do szkoły / przedszkola za dziecko; brak związku z listą sdn\": \"payment for school/preschool child not related to sdn list\",\n",
    "        \"dotyczy imienia nazwiska platnosc do szkoly przedszkola za dziecko brak zwiazku z lista sdn\": \"payment for school/preschool child not related to sdn list\",\n",
    "        \"input is another known input type provided in a known unstructured format (free form field) that clearly indicates it i not the same type of information as the list entry\": \"input is a different type of information than list entry\",\n",
    "        \"input is solely a single name component and list entry is multi name component\": \"input is a single name component and list entry is multi name component\",\n",
    "        \"input name has additional name components that do not match those of the list entry (including aliases)\": \"input name has additional components not matching list entry\",\n",
    "        \"wykluczam hit bazujac na caloksztalcie informacji\": \"cannot exclude without further information\",\n",
    "        \"wykluczam hit bazujac na catoksztaicie informac]a dotyczy os\": \"cannot exclude based on the totality of information\",\n",
    "        'false match: clarification on file': 'false match',\n",
    "        'transaction processed as it meets all criteria of policy waiver': 'transaction processed under policy waiver',\n",
    "        'gender mismatch': 'gender does not match',\n",
    "        'dob mismatch input entry': 'date of birth mismatch',\n",
    "        }\n",
    "        \n",
    "\n",
    "\n",
    "        # Apply the standardizations\n",
    "        for old_phrase, new_phrase in phrases_to_standardize.items():\n",
    "            self.df[\"l1_maker_note\"] = self.df[\"l1_maker_note\"].astype(str).str.replace(old_phrase, new_phrase, regex=True)\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def remove_examples(self):\n",
    "        self.df[\"l1_maker_note\"] = self.df[\"l1_maker_note\"].astype(str).str.replace(r\"ex: .*\", \"\", regex=True)\n",
    "        return self\n",
    "\n",
    "    def remove_rows_with_keywords(self):\n",
    "        for keyword in Config.KEYWORDS:\n",
    "            old_len = len(self.df)\n",
    "            self.df = self.df[~self.df[\"l1_maker_note\"].astype(str).str.contains(keyword)]\n",
    "            new_len = len(self.df)\n",
    "            self.record_drops.append((f'Remove Rows with Keyword: {keyword}', old_len - new_len))\n",
    "        return self\n",
    "\n",
    "\n",
    "    def standardize_potential_match_notes(self):\n",
    "        self.df.loc[self.df[\"l1_maker_note\"].str.contains(\"potential match\", case=False, na=False), \"l1_maker_note\"] = \"potential match\"\n",
    "        return self\n",
    "\n",
    "    def remove_common_phrases(self, common_phrases):\n",
    "        for phrase in common_phrases:\n",
    "            self.df[\"l1_maker_note\"] = self.df[\"l1_maker_note\"].astype(str).str.replace(phrase, \"\", regex=True)\n",
    "        return self\n",
    "\n",
    "    def split_notes(self, separator=\".\"):\n",
    "        self.df[\"l1_maker_note\"] = self.df[\"l1_maker_note\"].astype(str).str.split(separator).str[0]\n",
    "        return self\n",
    "\n",
    "    def apply_guidelines(self):\n",
    "        self.df[\"guideline_action\"], self.df[\"guideline_reason\"] = zip(*self.df.apply(lambda row: self.get_guidelines(row[\"l1_maker_inputtype\"], row[\"l1_maker_listtype\"]), axis=1))\n",
    "        self.df.loc[self.df['guideline_action'].isna(), 'guideline_action'] = self.df.loc[self.df['guideline_action'].isna(), 'l1_maker_action']\n",
    "        self.df.loc[self.df['guideline_reason'].isna(), 'guideline_reason'] = self.df.loc[self.df['guideline_reason'].isna(), 'l1_maker_note']\n",
    "        return self\n",
    "\n",
    "    @staticmethod\n",
    "    def get_guidelines(input_type, list_type):\n",
    "        cross_match_types = [\n",
    "            (\"Individual\", \"Entity\"),\n",
    "            (\"Individual\", \"Vessel/Aircraft\"),\n",
    "            (\"Entity\", \"Vessel/Aircraft\"),\n",
    "            (\"Place/Location\", \"Individual\"),\n",
    "            (\"Place/Location\", \"Entity\"),\n",
    "            (\"Place/Location\", \"Vessel/Aircraft\"),\n",
    "            (\"Vessel/Aircraft\", \"Individual\"),\n",
    "        ]\n",
    "        \n",
    "        out_of_scope_types = [(\"Other Input Type\", \"Individual\")]\n",
    "\n",
    "        if (input_type, list_type) in cross_match_types:\n",
    "            return \"False Match\", \"Cross Match\"\n",
    "        elif (input_type, list_type) in out_of_scope_types:\n",
    "            return \"Out of Scope\", \"Out of Scope\"\n",
    "        else:\n",
    "            return None, None\n",
    "\n",
    "    def is_english(self, text):\n",
    "        try:\n",
    "            str(text).encode(encoding='utf-8').decode('ascii')\n",
    "        except UnicodeDecodeError:\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "\n",
    "    def detect_language(self):\n",
    "        self.df[\"note_in_english\"] = self.df[\"l1_maker_note\"].apply(self.is_english)\n",
    "        return self\n",
    "\n",
    "    def preprocess_polish_notes(self):\n",
    "        self.df[\"guideline_reason\"] = self.df[\"guideline_reason\"].apply(self.preprocess_note)\n",
    "        return self\n",
    "    \n",
    "    @staticmethod\n",
    "    def isEnglish(s):\n",
    "        try:\n",
    "            str(s).encode(encoding='utf-8').decode('ascii')\n",
    "        except UnicodeDecodeError:\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def preprocess_note(note):\n",
    "        if not Cleaner.isEnglish(note):\n",
    "            # Remove punctuation\n",
    "            note = re.sub(r'[^\\w\\s]', ' ', note)\n",
    "            # Convert to lowercase\n",
    "            note = note.lower()\n",
    "            # Remove extra spaces\n",
    "            note = re.sub(r'\\s+', ' ', note).strip()\n",
    "            note = re.sub(r'\\s+', ' ', note).rstrip()\n",
    "\n",
    "        return note\n",
    "    \n",
    "    def translate_notes(self):\n",
    "        self.df[\"guideline_reason\"] = self.df[\"guideline_reason\"].apply(self.translate_polish_notes)\n",
    "        return self\n",
    "\n",
    "    @staticmethod\n",
    "    def translate_polish_notes(note):\n",
    "        if Cleaner.isEnglish(note):\n",
    "            return note\n",
    "        else:\n",
    "            for polish_phrase, english_translation in Config.POLISH_TO_ENGLISH.items():\n",
    "                note = note.replace(polish_phrase, english_translation)\n",
    "            return note   \n",
    "\n",
    "\n",
    "    def unify_string_formats(self):\n",
    "        # Define a list of regular expressions to find and replace\n",
    "        regex_replacements = [\n",
    "            (r'dob mismatch input entry dob \\d{2} \\d{2} \\d{2} star case id .*', 'date of birth mismatch'),\n",
    "            (r'Input name and list entry dates of birth do not match dob: .*', 'date of birth mismatch'),\n",
    "            (r'transaction processed as it meets all criteria of policy waiver id #\\d*', 'transaction processed as it meets all criteria of policy waiver'),\n",
    "            (r'input name and list entry \\[.*\\] do not match', 'input name and list entry do not match'),\n",
    "            (r'input name and list entry dates of birth do not match.*', 'input name and list entry dates of birth do not match'),\n",
    "            (r'input and list entry are both financial institutions and both the names of the financial institutions and first 4 charac ters of the swift bank identifier code \\(bic\\) do not match.*',  'input and list entry are both financial institutions and names and swift bic do not match'),\n",
    "            (r'na podstawie danych z przelewu wykluczam hit, .*', 'exclude hit'),\n",
    "            (r'input name has additional name components that do not match those of the list entry.*', 'input name has additional components not matching list entry'),\n",
    "            (r'na podstawie danych z przelewu wykluczam hit .*', 'exclude hit'),\n",
    "            (r'input name has additional name components that do not match those of the list entry.*', 'input name has additional components not matching list entry'),\n",
    "            (r'dob mismatch input entry.*', 'date of birth mismatch'),\n",
    "            (r'false match:.*', 'false match'),\n",
    "            (r'components that are matching are different and not possible variations.*', 'matching components are different and not possible variations'),\n",
    "            (r'dane wejsciowe zawieraja jeden element.*', 'contains one element'),\n",
    "            (r'do weryfikacji', 'to verification'),\n",
    "            (r'dotyczy platnosc.*brak zwiazku z lista sdn', 'pertains to child school payment'),\n",
    "            (r'dotyczy imienia/nazwiska platnos.*brak zwiazku z lista sdn', 'pertains to name, no connection to SDN list'),\n",
    "            (r'dotyczy płatności za dziecko*',  'pertains to child school payment'),\n",
    "            (r'input is another known input type.*', 'input is another known input type'),\n",
    "            (r'input name and list entry.*do not match', 'input name and list entry do not match'),\n",
    "            (r'input name has additional components not matching list entry', 'input name has additional components not matching list entry'),\n",
    "            (r'na podstawie danych wejsciowych wykluczam hit.*', 'exclude hit'),\n",
    "            (r'nazmisko powtórzone w polu nadawcy i w detalach platnosci',  'surname repeated in the sender field and in the payment details'),\n",
    "            (r'wykluczam bazujac na caloksztalcie informacji.*', 'exclude hit'),\n",
    "            (r'wykluczam hit dopasowane komponenty nie wykazuja.*', 'exclude hit'),\n",
    "            (r'laczenie wyrazów', 'combining words'),\n",
    "            ('łączenie wyrazów', 'combining words'),\n",
    "            (r'dane wejściowe zawierają jeden element, wówczas dopasowanie należy uznać za fałszywe', 'contains one element'),\n",
    "            (r'contains one element*', 'contains one element'),\n",
    "            (r'dob mismatch.*', 'date of birth mismatch'),\n",
    "            (r'dane wejsciowe zawieraja jeden element.*', 'contains one element'),\n",
    "            (r'dotyczy płatność.*','pertains to child school payment'),\n",
    "            (r'do wer', 'to verify'),\n",
    "            (r'wykluczam hit bazując na całokształcie.*','exclude hit'),\n",
    "            (r'wykluczam bazując na całokształcie .*', 'exclude hit'),\n",
    "            (r'dotyczy płatność do szkoły.*', 'pertains to child school payment'),\n",
    "            (r'payment to child.*', 'pertains to child school payment'),\n",
    "            (r'dotyczy.*', 'pertains to child school payment'),\n",
    "            (r'payment for school/preschool child not related to sdn list', 'pertains to child school payment'),\n",
    "            (r'brak możliwości wykluczenia bez przeprowadzenia dodatkowej inwestygacji','no foreclosure without additional investment'),\n",
    "            (r'contains one element.*','contains one element'),\n",
    "            (r'wykluczam hit.*', 'exclude hit'),\n",
    "            (r'female vs','gender does not match'),\n",
    "            (r'nazwisko powtórzone w polu nadawcy i w detalach płatności', 'name repeated in sender and payment details'),\n",
    "            (r'klie.*', 'client bhw'),\n",
    "            (r'iclient bhw', 'client bhw'),\n",
    "            (r'na podstawie danych wejściowych exclude hit','exclude hit'),\n",
    "            (r'skrot.*','pertains to child school payment'),\n",
    "            (r'uck skrót odnosi się do nazwy nadawcy uniwersyteckie centrum kliniczne nie ma związku z podmiotem z listy sdn', 'abbreviations do not match'),\n",
    "            (r'uck.*', 'abbreviations do not match'),\n",
    "            (r'zgoda departamentu zgodności', 'compliance approval'),\n",
    "            (r'hit jest zawarty w innym wyrazie nie ma powiązania z listą sdn','hit is contained in another word'),\n",
    "            (r'na   exclude hit', 'exculde hit'),\n",
    "            (r'mozna wykluczyć hit inne nazwy podmiotow i lokalizacje debtor name.*', 'no relation between entities'),\n",
    "            (r'bazując na stronach i detalach transakcj.*','based on transaction details and similar transactions'),\n",
    "            (r'(na podstawie danych w zleceniu inne nazwy firm inne formy działalności.*)','other company names from other forms have no relation'),\n",
    "            (r'kleint bhw','client bhw'),\n",
    "            ('.input and list entry are both financial institutions.*','input and list entry are both financial institutions and both the names of the financial institutions and first 4 characters of the swift bank identifier code (bic) do not match' ),\n",
    "            (r'uniwersytet adama mickiewicza.*','different full names and locations'),\n",
    "            (r'components that are matching are considered common references.*', 'components that are matching are considered common references and the input contains additional material name components that are not a reference to: place/location, corporate or legal identifiers'),\n",
    "            (r'input and list entry are both financial institutions and both the names of the financial institutions and first 4.*','input and list entry are both financial institutions and both the names of the financial institutions and first 4 characters of the swift bank identifier code (bic) do not match'),\n",
    "            (r'exculde hit','exclude hit'),\n",
    "            (r'bazując na wcześniejszej inwestygacji w analogicznych transakcjach beneficjentem jest client bh', 'previous investments similar transactions'),\n",
    "            (r'imię i lub nazwisko osoby fizycznej z listy sdn lub jej wariacja jest dopasowaniem low quality aka','low quality aka match'),\n",
    "            (r'profil firmy wyklucza udział podmiotu sankcjonowanego','the company profile excludes the participation of a sanctioned entity'),\n",
    "            (r'inne nazwy podmiotow i lokalizacje debtor name','other company names from other forms have no relation'),\n",
    "            (r'na podstawie danych w zleceniu inne nazwy firm inne formy działalności i inne lokalizacje.*', 'other company names from other forms'),\n",
    "            (r'dob mismatch[:,]? input entry dob \\d{1,2}-\\d{1,2}-\\d{2,4} star case id .*', 'dob mismatch'),\n",
    "            (r'Input name and list entry dates of birth do not match dob: .*', 'input name and list entry dates of birth do not match'),\n",
    "            (r'transaction processed as it meets all criteria of policy waiver id #\\d*', 'transaction processed as it meets all criteria of policy waiver'),\n",
    "            (r'input name and list entry \\[.*?\\] do not match', 'input name and list entry do not match'),\n",
    "            (r'input name and list entry dates of birth do not match.*', 'input name and list entry dates of birth do not match'),\n",
    "            (r'cit[0-9]{6}-[0-9]{6} dob as per input entry::\\d{2}-\\d{2}-\\d{2}', 'dob mismatch'),\n",
    "            (r'cit[0-9]{6}-[0-9]{6} dob as per input entry::[0-9]{2}-[0-9]{2}-[0-9]{2}', 'dob mismatch'),\n",
    "            (r'wykluczam na podstawie caloksztalcie informacji nazwisko i inicjal imienia take samo jak w polu nadawcy', 'cannot exclude based on the totality of information'),\n",
    "            (r'transaction processed under policy waiver id #[0-9]{3}', 'transaction processed under policy waiver'),\n",
    "            (r'iso truncation issue in payment details', 'iso truncation issue'),\n",
    "            (r'escalated for further review', 'escalated for further review'),\n",
    "            (r'linebreak hit is contained in another word has no relation to the sdn listn', 'linebreak hit is contained in another word'),\n",
    "            (r'invoice svk-[0-9]{2}-[0-9]{8}-[0-9]-[0-9]', 'invoice svk'),\n",
    "            (r'(bazujac na caloksztalcie informacji.*)', 'pertains to child school payment'),\n",
    "            (r'input is another known input type provided in a known unstructured format (free form field) that clearly indicates it i not the same type of information as the list entry ', 'input is a different type of information than list entry'),\n",
    "            (r'wyjasnienie skrotu abbreviations do not match', 'abbreviation explanation does not match'),\n",
    "            (r'wykluczam dopasowanie wykluczam pertains to child school payment', 'exclude match exclude pertains to child school payment'),\n",
    "            (r'wykluczam hit dopasowane komponenty nie wykazuja zadnego potencjalnego zwiazku z wpisem na liscie', 'exclude hit matching components show no potential relation to the list entry'),\n",
    "            (r'inna petna nazwa', 'other full name'),\n",
    "            (r'wykluczam na podstawie całokształtu informacji nazwisko i inicjał imienia takie samo jak w polu nadawcy','surname and name initial name same as senders'),\n",
    "            (r'(as per gcd verified stars id - cit[0-9]{6}-[0-9]{6} dob as per input entry::[0-9]{2}-[0-9]{2}-[0-9]{2})','date of birth mismatch'),\n",
    "            (r'(as per gcd verified stars id - cit[0-9]{6}-[0-9]{6} dob as per input entry:: [0-9]{2}-[0-9]{2}-[0-9]{2})','date of birth mismatch'),\n",
    "            (r'bazując na całokształcie informacji.*', 'pertains to child school payment'),\n",
    "            (r'ykluczam hit dopasowane komponenty nie wykazują żadnego potencjalnego związku z wpisem na liście','components not matching entry list'),\n",
    "            (r'typ wpisu wyklucza udział podmiotu sankcjonowanego', 'the type of entry excludes the participation of a sanctioned entity'),\n",
    "            (r'ykluczam hit dopasowane komponenty nie wykazują żadnego potencjalnego związku z wpisem na liście', 'no potnetial relation to entry list'),\n",
    "            (r'psb pagaz - exclude hit', 'exclude hit'),\n",
    "            (r'wyjaśnienie skrótu abbreviations do not match', 'abreviations do not match'),\n",
    "            (r'usc skrót odnosi się do nazwy odbiorcy urząd stanu cywilnego nie ma związku z podmiotem z listy sdn', 'abreviations do not match'),\n",
    "            (r'\\\\compliance approval', 'compliance approval'),\n",
    "            (r'167 fl confirmed by client|167fl confirmed by client', 'confirmed by client'),\n",
    "        (r'inna pełna nazwa', 'another full name'),\n",
    "            (r'bazując na stronach i tytule dot', 'based on pages and dot title'),\n",
    "            (r'\"input is another known input type', 'input is another known input type')\n",
    "        ]\n",
    "\n",
    "        \n",
    "        # Apply the replacements\n",
    "        for regex, replacement in regex_replacements:\n",
    "            self.df[\"guideline_reason\"] = self.df[\"guideline_reason\"].str.replace(regex, replacement, regex=True)\n",
    "    \n",
    "        return self\n",
    "\n",
    "    def remove_irrelevant_information(self):\n",
    "        self.df[\"guideline_reason\"] = self.df[\"guideline_reason\"].str.rstrip().replace(r'star case id \\w*', '', regex=True)\n",
    "        return self\n",
    "    \n",
    "    def remove_trailing_spaces(self):\n",
    "        self.df[\"guideline_reason\"] = self.df[\"guideline_reason\"].str.strip()\n",
    "        return self\n",
    "\n",
    "    def get_clean_df(self):\n",
    "        return self.df, pd.DataFrame(self.record_drops, columns=[\"Data Transformation\", \"Records Dropped\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analysis.ipynb  cleaner.py\r\n"
     ]
    }
   ],
   "source": [
    "## Source files pathcd /var/app/pctlness/tensorflow/song/python/data/prod/llm_data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting config.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile config.py\n",
    "# %load config.py\n",
    "class Config:\n",
    "# List of keywords\n",
    "    KEYWORDS = ['pvt', 'guidance', 'dates of birth', 'date of birth', 'date and place', 'date de naissance', 'birth date', 'date for birth', 'dob/location', ' dob', 'dob ', 'dob:', 'dob-', 'd o b', '\\ndob\\n', '\\ndob.', '\\tdob.', '\\ndob,', 'year of birth', 'different dob', 'date(s) of birth', 'date/place of birth', 'date place of birth', 'date of the birth', 'date of \\nbirth', 'birthdate', 'date of berth', 'date of bitrh', 'date of brith', 'date \\nof birth', 'date\\n of birth', 'date of\\nbirth', 'date\\nof birth', 'different dob','exclude', 'client bhw', 'za obiady','gender']\n",
    "\n",
    "    \n",
    "    # Resting Utils scripts /var/app/cstxuat/tensorflow/song/individual/gb54464/train/external/source_data\n",
    "    FOLDER_PATH = \"/var/app/pctlness/tensorflow/song/python/data/prod/llm_data/\"\n",
    "    RESULTS_DIR = \"/var/app/pctlness/tensorflow/song/python/data/prod/llm_data/clean_data/\"\n",
    "    \n",
    "    # Dictionary for translating Polish phrases to English\n",
    "    POLISH_TO_ENGLISH = {\n",
    "        \"do wery\": \"to verify\",\n",
    "        \"brak możliwości wykluczenia bez przeprowadzenia dodatkowej inwestygacji\": \"cannot exclude without additional investigation\",\n",
    "        \"do weryfikacji\": \"to verify\",\n",
    "        \"na podstawie danych wejściowych wykluczam hit, klient bhw\": \"based on input data, I exclude the hit, bhw client\",\n",
    "        \"a wer\": \"to verify\",\n",
    "        \"klient bhw\": \"bhw client\",\n",
    "        \"łączenie wyrazów\": \"combining words\",\n",
    "        \"wykluczam hit, dopasowane komponenty nie wykazują żadnego potencjalnego związku z wpisem na liście\": \"I exclude the hit, the matching components do not show any potential relationship with the entry on the list\",\n",
    "        \"brak możliwości wykluczenia, do weryfikacji\": \"cannot exclude, to verify\",\n",
    "        \"wykluczam hit bazując na analogicznych płatnościach między tymi samymi stronami - tytuł płatności wskazuje na rozliczenie/opłatę/numer faktury, który jest generowany automatycznie jako przypadkowy ciąg alfanumeryczny, w którego formacie nie ma stałego miejsca na ciągi liter i/lub cyfr, można więc wykluczyć jakiekolwiek powiązanie tytułu z konkretnym sdn)\": \"I exclude the hit based on similar payments between the same parties - the payment title indicates the settlement/payment/invoice number, which is automatically generated as a random alphanumeric string, in whose format there is no fixed place for strings of letters and/or numbers, so any connection of the title with a specific SDN can be excluded\",\n",
    "        \"pełne dane wejściowe osoby fizycznej nie są dokładnym dopasowaniem\": \"the full input data of the physical person is not an exact match\",\n",
    "        \"na podstawie danych wejściowych wykluczam hit; dotyczy osoby fizycznej - nie ma związku z lokalizacją sdn\": \"based on the input data, I exclude the hit; it concerns an individual - there is no connection with the SDN location\",\n",
    "        \"do weryfikacji\": \"to verify\",\n",
    "        \"brak możliwości wykluczenia hitu bez dodatkowej inwestygacji\": \"cannot exclude the hit without additional investigation\",\n",
    "        \"wykluczam hit bazując na całości informacji (klient bhw, inne pełne nazwy + inne lokalizacje)\": \"I exclude the hit based on the overall information (bhw client, other full names + other locations)\",\n",
    "        \"wykluczam hit, dopasowane komponenty nie wykazują żadnego potencjalnego związku z wpisem na liście\": \"I exclude the hit, the matching components do not show any potential relationship with the entry on the list\",\n",
    "        \"łamanie linii - hit jest zawarty w innym wyrazie, nie ma związku z listą sdn\": \"breaking the line - the hit is contained in another word, there is no connection with the SDN list\",\n",
    "        \"dotyczy imienia/nazwiska płatności do szkoły / przedszkola za dziecko; brak związku z listą sdn\": \"it concerns the name / surname of the payment to school / kindergarten for the child; there is no connection with the SDN list\",\n",
    "        \"do weryfikacji\": \"to verify\",\n",
    "        \"na podstawie danych z przelewu wykluczam hit, klient bhw\": \"based on the data from the transfer, I exclude the hit, bhw client\",\n",
    "        \"bazując na stronach transakcji (serwis internetowy służący do rezerwacji zakwaterowania online) oraz na analogicznych płatnościach, tytuł płatności wskazuje na rozliczenie/opłatę /rezerwację , to ciąg znaków który jest generowany automatycznie jako przypadkowy ciąg alfanumeryczny, w którego formacie nie ma stałego miejsca na ciągi liter i/lub cyfr, można więc wykluczyć jakiekolwiek powiązanie tytułu, z sdn\": \"based on the pages. transaction (online accommodation booking service) and similar payments, the payment title indicates settlement / payment / reservation, this is a string of characters that is generated automatically as a random alphanumeric string, in whose format there is no fixed place for strings of letters and/or numbers, so any connection of the title with the SDN can be excluded\",\n",
    "        \"wykluczam hit, bazując na stronach transakcji i detalach płatności, skrót 'spp' oznacza strefę płatnego parkowania, nie d\": \"I exclude the hit, based on the pages of the transaction and the details of the payment, the abbreviation 'spp' means the paid parking zone, no d\",\n",
    "        \"wykluczam hit dopasowane komponenty nie wykazuja żadnego potencjalnego związku z wpisem na liście\": \"I exclude the hit, matched components do not show any potential relationship with the entry on the list\",\n",
    "        \"bazując na całości informacji zlecenia - stronach oraz tytule, który jest związany z wynagrodzeniem dla nauczycieli, brak związku z sdn\": \"Based on the entirety of the order information - pages and title, which is related to the remuneration for teachers, there is no connection with SDN\",\n",
    "        \"dotyczy imienia, nazwisko takie samo jak w polu nadawcy: płatność do szkoły / przedszkola za dziecko; brak związku z listą sdn\": \"it concerns the name, surname the same as in the sender's field: payment to school / kindergarten for the child; there is no connection with the SDN list\",\n",
    "        \"maker - transakcje w ofac procesują się prawidłowo w dniu 27/04/2023\": \"maker - OFAC transactions are processed correctly on 27/04/2023\",\n",
    "        \"typ firmy wyklucza udział podmiotu sankcjonowanego\": \"the type of company excludes the participation of a sanctioned entity\",\n",
    "        \"wykluczam hit, bazując na stronach transakcji i detalach płatności, skrót 'us' dotyczy urząd stanu cywilnego, odmienne pełne nazwy i lokalizacje, brak związku z podmiotem pjsc united shipbuilding corporation z listy sdn-mail z dnia 02/07/2023\": \"I exclude the hit, based on the pages of the transaction and the details of the payment, the abbreviation 'us' refers to the civil status office, different full names and locations, no connection with the entity PJSC United Shipbuilding Corporation from the SDN list-mail from 02/07/2023\",\n",
    "        \"wykluczam hit bazując na stronach transakcji i detalach płatności; tytuł płatności wskazuje na opłatę za kurs jezykowy, brak związku z listą sdn\": \"I exclude the hit based on the pages of the transaction and the details of the payment; the payment title indicates a payment for a language course, no connection with the SDN list\",\n",
    "        \"kierownik ds. kontroli\": \"manager for control\",\n",
    "        \"do wer\": \"to verify\",\n",
    "        \"brak mozliwosci wykluczenia bez przeprowadzenia dodatkowei inwestygacji\": \"Potential Match: Escalated for further review\",\n",
    "        \"do weryfikacii\": \"to verify\",\n",
    "        \"na podstawie danych wejsciowych wykluczam hit, klient bhw\": \"based on input data, I exclude the hit, bhw client\",\n",
    "        \"a wer\": \"to verify\",\n",
    "        \"klient bhw\": \"bhw client\",\n",
    "        'kleitn bhw':\"bhw client\", 'klien tbhw': 'bhw client', 'klient bnhw': 'bhw client', 'klienty bhw': 'bhw client',\n",
    "        \"laczenie wyrazow\": \"combining words\",\n",
    "        \"wykluczam hit dopasowane komponenty nie, wykazuja Zadnego potencjalnego zwiazku z wisem na liscie\": \"I exclude the hit, the matching components do not show any potential relationship with the entry on the list\",\n",
    "        \"wykluczam hit dopasowane komponenty nie wykazują żadnego potencjalnego związku z wpisem na liście\": \"I exclude the hit, the matching components do not show any potential relationship with the entry on the list\",\n",
    "        \"brak mozliwosci wykluczenia bez przeprowadzenia dodatkowei inwestygacji\": \"Potential Match: Escalated for further review\",\n",
    "        \"brak mozliwosci wykluczenia, do weryfikacji\": \"cannot exclude, to verify\",\n",
    "        \"wvkluczam hit bazujac na analogicznych platnosciach miedzy tymi samymi stronami - tytul platnosci wskazuje na rozliczen ie/oplate/numer faktury, ktory jest generowany automatycznie jako przypadkowy ciag alfanumeryczny, w którego formacie nie ma stalego miejsca na ciagi liter i/lub cyfr, moana wiec wykluczy jakiekolwiek powiazanie tytulu z konkretnym sdn)\": \"I exclude the hit based on similar payments between the same parties - the payment title indicates the settlement/payment/invoice number, which is automatically generated as a random alphanumeric string, in whose format there is no fixed place for strings of letters and/or numbers, so any connection of the title with a specific SDN can be excluded\",\n",
    "        \"pelne dane weisciowe osoby fizycznei nie sa dokladnym dopasowaniem\": \"the full input data of the physical person is not an exact match\",\n",
    "        \"na podstawie danych wejsciowych wykluczam hit; dotyczy osoby fizycznej - nie ma zwiazku z lokalizacja sdn\": \"based on the input data, I exclude the hit; it concerns an individual - there is no connection with the SDN location\",\n",
    "        \"dow ep\": \"to verify\",\n",
    "        \"brak mozliwosci wykluczenia hit bez dodatkowei inwestygacji\": \"cannot exclude the hit without additional investigation\",\n",
    "        \"wykluczam hit bazujac na caloksztalcie informacii (klient bhw, inne pelne nazwy + inne lokalizacje)\": \"I exclude the hit based on the overall information (bhw client, other full names + other locations)\",\n",
    "        \"input is another known input type provided in a known unstructured format (free form field) that clearly indicates it i s not the same type of information as the list entry (e\": \"input is another known input type provided in a known unstructured format (free form field) that clearly indicates it is not the same type of information as the list entry\",\n",
    "        \"wykluczam hit dopasowane komponenty nie wykazuja zadnego potencjalnego zwiazku z wpisem na liscie\": \"I exclude the hit, the matching components do not show any potential relationship with the entry on the list\",\n",
    "        \"brak mozliwosci wykluczenia bez przeprowadzenia dodatkowej inwestygacji\": \"cannot exclude without additional investigation\",\n",
    "        \"Iamanie linii -hit jest zawarty w innym wyrazie, nie ma zwiazku z lista sdn\": \"breaking the line - the hit is contained in another word, there is no connection with the SDN list\",\n",
    "        \"na podstawie danych weisciowych wykluczam hit; dotyczy osoby fizycznej - nie ma zwiazku z lokalizacja sdn\": \"based on the input data, I exclude the hit; it concerns an individual - there is no connection with the SDN location\",\n",
    "        \"brak mozliwosci wykluczenia bez przeprowadzenia dodatkowei inwestygacii\": \"cannot exclude without additional investigation\",\n",
    "        \"dotyczy imienia/nazwiska platnose do szkoly / przedszkola za dziecko; brak zwiazku z lista sdn\": \"it concerns the name / surname of the payment to school / kindergarten for the child; there is no connection with the SDN list\",\n",
    "        \"dower\": \"to verify\",\n",
    "        \"na podstawie danych z przelewu wykluczam hit, klient bhw\": \"based on the data from the transfer, I exclude the hit, bhw client\",\n",
    "        \"na podstawie danych wejsciowych wykluczam hit, klient. bhw\": \"based on the input data, I exclude the hit, bhw client\",\n",
    "        \"na podstawie danych wejściowych wykluczam hit bhw client\": \"based on the input data, I exclude the hit, bhw client\",\n",
    "        \"bazuiac na stronach. transakcji (serwis internetowy stuzacy do rezerwacii zakwaterowania online) oraz a analogicznych platnosciach, tytul platnosci wskazuje na rozliczenie/oplate /rezerwacje , to ciag nakow ktory jest generowany automatyeznie jako przypadkowy ciag alfanumeryczny, w kórego formacie nie ma stalego miejsca na ciagi liter i/lub cyfr, mo¿na wiec wykluczy\": \"based on the pages. transaction (online accommodation booking service) and similar payments, the payment title indicates settlement / payment / reservation, this is a string of characters that is generated automatically as a random alphanumeric string, in whose format there is no fixed place for strings of letters and/or numbers, so any connection of the title with the SDN can be excluded\",\n",
    "        \"bazuiac na stronach transakcji (serwis internetowy stuzacy do rezerwacii zakwaterowania online) oraz na analogicznych platosciach, tytul platnosci wskazuje na rozliczenie/oplate /rezerwacie , to ciag znakow ktory jest generowany. automatycznie jako przypadkowy ciag alfanumeryczny, w kórego formacie nie ma stalego miejsca na ciagi liter i/lub cyfr, mozna wiec wykluczy é lakiekolwiek powiazanie tytulu, z sdn\": \"based on the pages. transaction (online accommodation booking service) and similar payments, the payment title indicates settlement / payment / reservation, this is a string of characters that is generated automatically as a random alphanumeric string, in whose format there is no fixed place for strings of letters and/or numbers, so any connection of the title with the SDN can be excluded\",\n",
    "        \"wykluczam hit bazujac na analogicznych platnosciach miedzy tymi samymi stronami - tytul platnosci wskazuje na rozlicze nie/oplate/numer faktury, który jest generowany automatycznie jako przypadkowy ciag alfanumeryczny, w którego formacie nie ma stalego miejsca na ciagi liter i/lub cyfr, mozna wiec wykluczy& jakiekolwiek powiazanie tytulu z konkretnym sdn)\": \"I exclude the hit based on similar payments between the same parties - the payment title indicates the settlement/payment/invoice number, which is automatically generated as a random alphanumeric string, in whose format there is no fixed place for strings of letters and/or numbers, so any connection of the title with a specific SDN can be excluded\",\n",
    "        \"wykluczam hit, bazujac na stronach transakcji i detalach platnosci, skrot 'spp oznacza strefa platnego parkowania, nie d\": \"I exclude the hit, based on the pages of the transaction and the details of the payment, the abbreviation 'spp' means the paid parking zone, no d\",\n",
    "        \"wykluczam bazując na całokształcie informacji\":\"exclude hit\" ,\n",
    "        \"dane wejściowe zawierają jeden element, wówczas dopasowanie należy uznać za fałszywe\" : \"the input contains one element, then the match should be considered false\",\n",
    "        \"wykluczam hit bazując na całokształcie informacji \": \"exclude hit\",\n",
    "        \"wykluczam hit bazując na całokształcie informacji\": \"exclude hit\",\n",
    "        \"dane wejściowe zawierają jeden element, wówczas dopasowanie należy uznać za fałszywe\": \"input contains one element\",\n",
    "        \"do wer\": \"to verify\",\n",
    "        \"brak mozliwosci wykluczenia bez przeprowadzenia dodatkowei inwestygacji\": \"Potential Match: Escalated for further review\",\n",
    "        \"do weryfikacii\": \"to verify\",\n",
    "        \"na podstawie danych wejsciowych wykluczam hit, klient bhw\": \"bhw client\",\n",
    "        \"a wer\": \"to verify\",\n",
    "        \"klient bhw\": \"bhw client\",\n",
    "        'kleitn bhw':\"bhw client\", 'klien tbhw': 'bhw client', 'klient bnhw': 'bhw client', 'klienty bhw': 'bhw client',\n",
    "        \"laczenie wyrazow\": \"combining words\",\n",
    "        \"wykluczam hit dopasowane komponenty nie, wykazuja Zadnego potencjalnego zwiazku z wisem na liscie\": \"I exclude the hit, the matching components do not show any potential relationship with the entry on the list\",\n",
    "        \"wykluczam hit dopasowane komponenty nie wykazują żadnego potencjalnego związku z wpisem na liście\": \"I exclude the hit, the matching components do not show any potential relationship with the entry on the list\",\n",
    "        \"brak mozliwosci wykluczenia bez przeprowadzenia dodatkowei inwestygacji\": \"cannot exclude without additional investigation\",\n",
    "        \"brak mozliwosci wykluczenia, do weryfikacji\": \"cannot exclude, to verify\",\n",
    "        \"wvkluczam hit bazujac na analogicznych platnosciach miedzy tymi samymi stronami - tytul platnosci wskazuje na rozliczen ie/oplate/numer faktury, ktory jest generowany automatycznie jako przypadkowy ciag alfanumeryczny, w którego formacie nie ma stalego miejsca na ciagi liter i/lub cyfr, moana wiec wykluczy jakiekolwiek powiazanie tytulu z konkretnym sdn)\": \"I exclude the hit based on similar payments between the same parties - the payment title indicates the settlement/payment/invoice number, which is automatically generated as a random alphanumeric string, in whose format there is no fixed place for strings of letters and/or numbers, so any connection of the title with a specific SDN can be excluded\",\n",
    "        \"pelne dane weisciowe osoby fizycznei nie sa dokladnym dopasowaniem\": \"the full input data of the physical person is not an exact match\",\n",
    "        \"na podstawie danych wejsciowych wykluczam hit; dotyczy osoby fizycznej - nie ma zwiazku z lokalizacja sdn\": \"based on the input data, I exclude the hit; it concerns an individual - there is no connection with the SDN location\",\n",
    "        \"dow ep\": \"to verify\",\n",
    "        \"brak mozliwosci wykluczenia hit bez dodatkowei inwestygacji\": \"cannot exclude the hit without additional investigation\",\n",
    "        \"wykluczam hit bazujac na caloksztalcie informacii (klient bhw, inne pelne nazwy + inne lokalizacje)\": \"I exclude the hit based on the overall information (bhw client, other full names + other locations)\",\n",
    "        \"input is another known input type provided in a known unstructured format (free form field) that clearly indicates it i s not the same type of information as the list entry (e\": \"input is another known input type provided in a known unstructured format (free form field) that clearly indicates it is not the same type of information as the list entry\",\n",
    "        \"wykluczam hit dopasowane komponenty nie wykazuja zadnego potencjalnego zwiazku z wpisem na liscie\": \"the matching components do not show any potential relationship with the entry on the list\",\n",
    "        \"brak mozliwosci wykluczenia bez przeprowadzenia dodatkowej inwestygacji\": \"cannot exclude without additional investigation\",\n",
    "        \"Iamanie linii -hit jest zawarty w innym wyrazie, nie ma zwiazku z lista sdn\": \"breaking the line - the hit is contained in another word, there is no connection with the SDN list\",\n",
    "        \"na podstawie danych weisciowych wykluczam hit; dotyczy osoby fizycznej - nie ma zwiazku z lokalizacja sdn\": \"based on the input data,  it concerns an individual - there is no connection with the SDN location\",\n",
    "        \"brak mozliwosci wykluczenia bez przeprowadzenia dodatkowei inwestygacii\": \"Potential Match: Escalated for further review\",\n",
    "        \"dotyczy imienia/nazwiska platnose do szkoly / przedszkola za dziecko; brak zwiazku z lista sdn\": \"it concerns the name / surname of the payment to school / kindergarten for the child; there is no connection with the SDN list\",\n",
    "        \"dower\": \"to verify\",\n",
    "        \"na podstawie danych z przelewu wykluczam hit, klient bhw\": \"bhw client\",\n",
    "        \"na podstawie danych wejsciowych wykluczam hit, klient. bhw\": \"bhw client\",\n",
    "        \"na podstawie danych wejściowych wykluczam hit bhw client\": \"bhw client\",\n",
    "        \"bazuiac na stronach. transakcji (serwis internetowy stuzacy do rezerwacii zakwaterowania online) oraz a analogicznych platnosciach, tytul platnosci wskazuje na rozliczenie/oplate /rezerwacje , to ciag nakow ktory jest generowany automatyeznie jako przypadkowy ciag alfanumeryczny, w kórego formacie nie ma stalego miejsca na ciagi liter i/lub cyfr, mo¿na wiec wykluczy\": \"based on the pages. transaction (online accommodation booking service) and similar payments, the payment title indicates settlement / payment / reservation, this is a string of characters that is generated automatically as a random alphanumeric string, in whose format there is no fixed place for strings of letters and/or numbers, so any connection of the title with the SDN can be excluded\",\n",
    "        \"bazuiac na stronach transakcji (serwis internetowy stuzacy do rezerwacii zakwaterowania online) oraz na analogicznych platosciach, tytul platnosci wskazuje na rozliczenie/oplate /rezerwacie , to ciag znakow ktory jest generowany. automatycznie jako przypadkowy ciag alfanumeryczny, w kórego formacie nie ma stalego miejsca na ciagi liter i/lub cyfr, mozna wiec wykluczy é lakiekolwiek powiazanie tytulu, z sdn\": \"based on the pages. transaction (online accommodation booking service) and similar payments, the payment title indicates settlement / payment / reservation, this is a string of characters that is generated automatically as a random alphanumeric string, in whose format there is no fixed place for strings of letters and/or numbers, so any connection of the title with the SDN can be excluded\",\n",
    "        \"wykluczam hit bazujac na analogicznych platnosciach miedzy tymi samymi stronami - tytul platnosci wskazuje na rozlicze nie/oplate/numer faktury, który jest generowany automatycznie jako przypadkowy ciag alfanumeryczny, w którego formacie nie ma stalego miejsca na ciagi liter i/lub cyfr, mozna wiec wykluczy& jakiekolwiek powiazanie tytulu z konkretnym sdn)\": \"I exclude the hit based on similar payments between the same parties - the payment title indicates the settlement/payment/invoice number, which is automatically generated as a random alphanumeric string, in whose format there is no fixed place for strings of letters and/or numbers, so any connection of the title with a specific SDN can be excluded\",\n",
    "        \"wykluczam hit, bazujac na stronach transakcji i detalach platnosci, skrot 'spp oznacza strefa platnego parkowania, nie d\": \"I exclude the hit, based on the pages of the transaction and the details of the payment, the abbreviation 'spp' means the paid parking zone, no d\",\n",
    "        \"łamanie linii hit jest zawarty w innym wyrazie nie ma związku z listą sd\":\"linebreak hit is contained in another word has no relation to the sdn list\"}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting dataPrepUtils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile dataPrepUtils.py\n",
    "import warnings\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import cx_Oracle\n",
    "import logging\n",
    "import time\n",
    "import argparse\n",
    "import datetime\n",
    "\n",
    "from config import Config\n",
    "from cleaner import Cleaner\n",
    "\n",
    "\n",
    "\n",
    "def setup_logging(log_level, log_format):\n",
    "    logging.basicConfig(level=log_level, format=log_format, handlers=[\n",
    "        logging.FileHandler(\"debug.log\"),  # Save logs to this file\n",
    "        logging.StreamHandler()  # And print logs to terminal\n",
    "    ])\n",
    "    logging.info('Logging setup complete.')\n",
    "    \n",
    "def clean_data(data):\n",
    "    initial_records = data.shape[0]\n",
    "    initial_columns = data.shape[1]\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Initialize and run the cleaner.filter_II_combinations()\\\n",
    "    cleaner = Cleaner(data)\n",
    "    clean_df, table1_df = cleaner.select_columns()\\\n",
    "                                  .get_alias()\\\n",
    "                                  .get_list_name()\\\n",
    "                                  .remove_newlines()\\\n",
    "                                  .remove_urls()\\\n",
    "                                  .to_lowercase()\\\n",
    "                                  .standardize_phrases()\\\n",
    "                                  .remove_examples()\\\n",
    "                                  .remove_rows_with_keywords()\\\n",
    "                                  .standardize_potential_match_notes()\\\n",
    "                                  .split_notes()\\\n",
    "                                  .detect_language()\\\n",
    "                                  .standardize_phrases()\\\n",
    "                                  .clean_l1_maker_note()\\\n",
    "                                  .clean_l2_maker_note()\\\n",
    "                                  .apply_guidelines()\\\n",
    "                                  .remove_irrelevant_information()\\\n",
    "                                  .unify_string_formats()\\\n",
    "                                  .preprocess_polish_notes()\\\n",
    "                                  .remove_trailing_spaces()\\\n",
    "                                  .unify_string_formats()\\\n",
    "                                  .get_clean_df()\n",
    "    \n",
    "    final_records = clean_df.shape[0]\n",
    "    final_columns = clean_df.shape[1]\n",
    "    dropped_records = initial_records - final_records\n",
    "    end_time = time.time()\n",
    "    processing_time = end_time - start_time\n",
    "    logging.info(f\"The cleaning operations dropped {dropped_records} records\")\n",
    "    return clean_df, (initial_records, initial_columns, final_records, final_columns, processing_time)\n",
    "\n",
    "def get_primary_values(df):\n",
    "    group_columns = ['business_unit', 'match_tag', 'match_value', 'offset', 'match_text', 'match_pattern_value', 'list_name']\n",
    "\n",
    "    # Generic logic to determine primary value for a column based on its frequency and max hit_id\n",
    "    def get_primary_for_column(column):\n",
    "        counts = df.groupby(group_columns + [column]).agg({\n",
    "            column: 'count',\n",
    "            'hit_id': 'max'\n",
    "        }).rename(columns={column: 'count'}).reset_index()\n",
    "\n",
    "        counts = counts.sort_values(by=['count', 'hit_id'], ascending=[False, False])\n",
    "\n",
    "        primary_values = counts.drop_duplicates(subset=group_columns, keep='first')\n",
    "\n",
    "        return primary_values[group_columns + [column]]\n",
    "\n",
    "    # Determine primary values\n",
    "    primary_input_types = get_primary_for_column('l1_maker_inputtype')\n",
    "    primary_actions = get_primary_for_column('guideline_action')\n",
    "    primary_reasons = get_primary_for_column('guideline_reason')\n",
    "\n",
    "    # Merge primary values back to the original DataFrame\n",
    "    df = df.merge(primary_input_types, on=group_columns, how='left', suffixes=('', '_primary'))\n",
    "    df = df.merge(primary_actions, on=group_columns, how='left', suffixes=('', '_action_primary'))\n",
    "    df = df.merge(primary_reasons, on=group_columns, how='left', suffixes=('', '_reason_primary'))\n",
    "\n",
    "    return df\n",
    "\n",
    "def filter_to_highest_hitid_records(df):\n",
    "    # Sort the DataFrame based on hit_id in descending order\n",
    "    df = df.sort_values(by='hit_id', ascending=False)\n",
    "\n",
    "    # Define grouping columns\n",
    "    group_columns = ['business_unit', 'match_tag', 'match_value', 'offset', 'match_text', 'match_pattern_value', 'list_name']\n",
    "\n",
    "    # Replace the columns with their primary values\n",
    "    df['l1_maker_inputtype'] = df['l1_maker_inputtype_primary']\n",
    "    df['guideline_action'] = df['guideline_action_action_primary']\n",
    "    df['guideline_reason'] = df['guideline_reason_reason_primary']\n",
    "\n",
    "\n",
    "    # Drop the additional columns used for computations\n",
    "    columns_to_drop = ['l1_maker_inputtype_primary', 'guideline_action_action_primary', 'guideline_reason_reason_primary']\n",
    "    df = df.drop(columns=columns_to_drop, errors='ignore')  # using errors='ignore' to ensure it doesn't fail if columns aren't there\n",
    "\n",
    "    # Drop duplicate rows based on grouping columns\n",
    "    df = df.drop_duplicates(subset=group_columns, keep='first')\n",
    "\n",
    "    # Retain only desired columns\n",
    "    desired_columns = group_columns + ['l1_maker_listtype', 'l1_maker_inputtype', 'guideline_action', 'guideline_reason', 'hit_id']\n",
    "    df = df[desired_columns]\n",
    "\n",
    "    return df\n",
    "\n",
    "def post_process(clean_df):\n",
    "    # Initial cleaning steps\n",
    "    clean_df.drop(columns=['matchable_description'], inplace=True)\n",
    "    clean_df.loc[clean_df[\"guideline_action\"].str.contains(\"nan\", case=False, na=False), \"guideline_reason\"] = \"potential match\"\n",
    "    clean_df.loc[clean_df['guideline_reason'].astype(str).apply(lambda x: x.strip().lower()) == 'nan', 'guideline_reason'] = 'potential match'\n",
    "    actions_to_drop = [\"Potential Inconclusive Match\",\"Out of Scope\"]\n",
    "    clean_df = clean_df[~clean_df.guideline_action.isin(actions_to_drop)]\n",
    "    clean_df = clean_df[clean_df[\"note_in_english\"]]\n",
    "    clean_df = clean_df[clean_df['match_pattern_value'].apply(is_english)]\n",
    "    text_columns = ['match_tag', 'match_value', 'match_text', 'list_name', 'aka', 'match_pattern_value', 'guideline_action', 'guideline_reason']\n",
    "    clean_df['l1_maker_inputtype'] = clean_df['l1_maker_inputtype'].str.replace('-', ' ', regex=True)\n",
    "    clean_df[text_columns] = clean_df[text_columns].applymap(lambda x: x.lower() if isinstance(x, str) else x)\n",
    "\n",
    "    # Flag data to get the primary input type, action, and reason\n",
    "    flagged_df = get_primary_values(clean_df)\n",
    "\n",
    "    # Filter to highest hit_id records and replace columns with their primary values\n",
    "    final_df = filter_to_highest_hitid_records(flagged_df)\n",
    "\n",
    "    return final_df    \n",
    "\n",
    "def is_english(text):\n",
    "    try:\n",
    "        str(text).encode(encoding='utf-8').decode('ascii')\n",
    "    except UnicodeDecodeError:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "    \n",
    "def process_folder(folder_path, file_extension='.xlsx'):\n",
    "    data_frames = []\n",
    "    metrics = []\n",
    "    total_initial_records = 0\n",
    "    total_final_records = 0\n",
    "    total_processing_time = 0\n",
    "    \n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(file_extension):\n",
    "            start_time = time.time()\n",
    "            data = pd.read_excel(os.path.join(folder_path, filename))\n",
    "            logging.info(f\"The data file: {filename} took {time.time() - start_time} seconds to load\")\n",
    "            start_time = time.time()\n",
    "            cleaned_data, data_metrics = clean_data(data)\n",
    "            metrics.append((filename,) + data_metrics)\n",
    "            logging.info(f\"The data file: {filename} took {time.time() - start_time} seconds to be cleaned\")\n",
    "            data_frames.append(cleaned_data)\n",
    "            total_initial_records += data_metrics[0]\n",
    "            total_final_records += data_metrics[2]\n",
    "            total_processing_time += data_metrics[4]\n",
    "    all_data = pd.concat(data_frames)\n",
    "    return all_data, metrics, (total_initial_records, total_final_records, total_processing_time)    \n",
    "\n",
    "def save_excel(df, filename, sheet_name_prefix='Sheet'):\n",
    "    \"\"\"Save a DataFrame to an Excel file, handling large data by saving to multiple sheets.\"\"\"\n",
    "    with pd.ExcelWriter(filename, engine='openpyxl') as writer:\n",
    "        for i, chunk in enumerate(np.array_split(df, max(1, len(df) // 500000))):\n",
    "            chunk.to_excel(writer, sheet_name=f'{sheet_name_prefix}{i + 1}', index=True)\n",
    "\n",
    "def save_statistics_to_rst(timestamp, *dfs):\n",
    "    \"\"\"Save statistics data to an .rst file.\"\"\"\n",
    "    with open(f'docs/source/statistics_{timestamp}.rst', 'w') as file:\n",
    "        for df in dfs:\n",
    "            file.write(df_to_rst(df))\n",
    "            file.write(\"\\n\\n\")\n",
    "\n",
    "def process_args():\n",
    "    \"\"\"Parse and process command line arguments.\"\"\"\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--folder_path\", help=\"The path to the folder containing the data files\", default=Config.FOLDER_PATH)\n",
    "    parser.add_argument(\"--log_level\", help=\"The log level\", type=int, default=logging.INFO)\n",
    "    parser.add_argument(\"--log_format\", help=\"The log format\", default='%(levelname)s: %(message)s')\n",
    "    return parser.parse_args()\n",
    "\n",
    "def extract_location(messy_string):\n",
    "    cleaned= re.sub(r'\\bINPUT_DATE\\b.+','.', messy_string)\n",
    "    cleaned=re.findall(r'[^#!@,~;.-]+', cleaned)\n",
    "    cleaned= ' '.join(cleaned)\n",
    "    #cleaned=re.findall(r'[^-]+',cleaned)\n",
    "    #cleaned=' '.join(cleaned)\n",
    "    loc= re.findall(r'LOCATION(.*?)POB',cleaned)\n",
    "    loc=' '.join(loc)\n",
    "    return loc.strip()\n",
    "\n",
    "def extract_alias(messy_string):\n",
    "    cleaned= re.sub(r'\\bINPUT_DATE\\b.+','.', messy_string)\n",
    "    cleaned=re.findall(r'[^#!@,~;.-]+', cleaned)\n",
    "    cleaned= ' '.join(cleaned)\n",
    "    #cleaned=re.findall(r'[^-]+',cleaned)\n",
    "    #cleaned=' '.join(cleaned)\n",
    "    aka=re.findall(r'AKA (.*?)TYPE', cleaned)\n",
    "    aka=' '.join(aka)\n",
    "    return aka.strip()\n",
    "\n",
    "\n",
    "def extract_fullname(messy_string):\n",
    "    cleaned= re.sub(r'\\bINPUT_DATE\\b.+','.', messy_string)\n",
    "    cleaned=re.findall(r'[^''#!@,]+', cleaned)\n",
    "    cleaned= ' '.join(cleaned)\n",
    "    #cleaned=re.findall(r'[^-]+',cleaned)\n",
    "    #cleaned=' '.join(cleaned)\n",
    "    hit=re.findall(r'(.*?)RISK_ID', cleaned)\n",
    "    hit=' '.join(hit)\n",
    "    hit= hit.title()\n",
    "    return hit.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_flow.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_flow.py\n",
    "import warnings\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import cx_Oracle\n",
    "import logging\n",
    "import time\n",
    "import argparse\n",
    "import datetime\n",
    "from cleaner import Cleaner\n",
    "from config import Config\n",
    "\n",
    "\n",
    "from dataPrepUtils import *\n",
    "\n",
    "\n",
    "def main():\n",
    "    args = process_args()\n",
    "    setup_logging(args.log_level, args.log_format)\n",
    "\n",
    "    # Process the folder\n",
    "    start_time = time.time()\n",
    "    all_data, metrics, total_metrics = process_folder(args.folder_path)\n",
    "    logging.info(f\"Processed folder in {time.time() - start_time} seconds\")\n",
    "    logging.info(f\"All data size is {len(all_data)} records\")\n",
    "    \n",
    "    # Post process the data\n",
    "    start_time = time.time()\n",
    "    all_data_grouped = post_process(all_data)\n",
    "    logging.info(f\"All data grouped size is {len(all_data_grouped)} records\")\n",
    "    logging.info(f\"The available columns are {all_data_grouped.columns}\")\n",
    "    logging.info(f\"Post-processed data in {time.time() - start_time} seconds\")\n",
    "\n",
    "    # Save results to Excel\n",
    "    results_dir = Config.RESULTS_DIR\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    timestamp = datetime.datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "\n",
    "    input_types = ['Place/Location', 'Vessel/Aircraft', 'Other Input Type', 'Unknown', 'Individual', 'Entity']\n",
    "    for input_type in input_types:\n",
    "        filtered_df = all_data_grouped[all_data_grouped['l1_maker_inputtype'] == input_type]\n",
    "        logging.info(f\"Filter the input type for {input_type} size: {len(filtered_df)} records\")\n",
    "        save_excel(filtered_df, os.path.join(results_dir, f'{input_type.replace(\"/\", \"_\")}_vsAll_{timestamp}.xlsx'))\n",
    "        logging.info(f\"The cleaned data for {input_type} was saved at: {os.path.join(results_dir, f'{input_type.replace(\"/\", \"_\")}_vsAll_{timestamp}.xlsx')}\")\n",
    "\n",
    "\n",
    "    logging.info(f\"The grouping and counting has fiished!\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Logging setup complete.\n",
      "INFO: The data file: nam_may.xlsx took 275.59404373168945 seconds to load\n",
      "/opt/middleware/anaconda_python/3.6.10/envs/tf/lib/python3.6/site-packages/pandas/core/strings.py:1843: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  return func(self, *args, **kwargs)\n",
      "INFO: The cleaning operations dropped 84870 records\n",
      "INFO: The data file: nam_may.xlsx took 672.7815017700195 seconds to be cleaned\n",
      "INFO: The data file: nam_march.xlsx took 267.24010729789734 seconds to load\n",
      "INFO: The cleaning operations dropped 93698 records\n",
      "INFO: The data file: nam_march.xlsx took 629.4010400772095 seconds to be cleaned\n",
      "INFO: The data file: nam_jan.xlsx took 238.10758876800537 seconds to load\n",
      "INFO: The cleaning operations dropped 74362 records\n",
      "INFO: The data file: nam_jan.xlsx took 592.4338600635529 seconds to be cleaned\n",
      "INFO: The data file: nam_jul.xlsx took 257.8052306175232 seconds to load\n",
      "INFO: The cleaning operations dropped 80870 records\n",
      "INFO: The data file: nam_jul.xlsx took 657.1269538402557 seconds to be cleaned\n",
      "INFO: The data file: nam_apr.xlsx took 232.32833909988403 seconds to load\n",
      "INFO: The cleaning operations dropped 68853 records\n",
      "INFO: The data file: nam_apr.xlsx took 546.1515438556671 seconds to be cleaned\n",
      "INFO: The data file: nam_jun.xlsx took 266.70523738861084 seconds to load\n",
      "INFO: The cleaning operations dropped 82004 records\n",
      "INFO: The data file: nam_jun.xlsx took 669.3855419158936 seconds to be cleaned\n",
      "INFO: The data file: nam_feb.xlsx took 227.21483898162842 seconds to load\n",
      "INFO: The cleaning operations dropped 71142 records\n",
      "INFO: The data file: nam_feb.xlsx took 529.9861710071564 seconds to be cleaned\n",
      "INFO: The data file: nam_aug.xlsx took 286.7643337249756 seconds to load\n",
      "INFO: The cleaning operations dropped 88315 records\n",
      "INFO: The data file: nam_aug.xlsx took 665.5676000118256 seconds to be cleaned\n",
      "INFO: Processed folder in 7018.714554071426 seconds\n",
      "INFO: All data size is 4063177 records\n",
      "INFO: All data grouped size is 1633839 records\n",
      "INFO: The available columns are Index(['business_unit', 'match_tag', 'match_value', 'offset', 'match_text',\n",
      "       'match_pattern_value', 'list_name', 'l1_maker_listtype',\n",
      "       'l1_maker_inputtype', 'guideline_action', 'guideline_reason', 'hit_id'],\n",
      "      dtype='object')\n",
      "INFO: Post-processed data in 137.95455479621887 seconds\n",
      "INFO: Filter the input type for Place/Location size: 364363 records\n",
      "INFO: Filter the input type for Vessel/Aircraft size: 12074 records\n",
      "INFO: Filter the input type for Other Input Type size: 103619 records\n",
      "INFO: Filter the input type for Unknown size: 34175 records\n",
      "INFO: Filter the input type for Individual size: 854312 records\n",
      "INFO: Filter the input type for Entity size: 265296 records\n",
      "INFO: The grouping and counting has fiished!\n"
     ]
    }
   ],
   "source": [
    "%run test_flow.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Once cleaned and group apply the song logic to pick primary input and list type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import cx_Oracle\n",
    "class DB:\n",
    "    def __init__(self):\n",
    "        self.hostname=hostname\n",
    "        self.port=port\n",
    "        self.service_name=service_name\n",
    "        self.user_name=user_name\n",
    "        self.user_pass=passwd\n",
    "        self.con_str = self.user_name+'/'+self.user_pass+'@'+self.hostname+':'+ self.port+'/'+self.service_name\n",
    "        self.conn = cx_Oracle.connect(self.con_str )\n",
    "        \n",
    "    def connect(self):\n",
    "         self.conn = cx_Oracle.connect(self.con_str)\n",
    "        \n",
    "    def get_connection(self):\n",
    "        return self.conn\n",
    "    \n",
    "    def close_connection(self):\n",
    "        self.conn.close()\n",
    "        print('Connection closed successfully')\n",
    "        \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analysis.ipynb  config.py         debug.log     test_flow.py\r\n",
      "cleaner.py      dataPrepUtils.py  \u001b[0m\u001b[01;34m__pycache__\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing config_dedup.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile config_dedup.py\n",
    "class Config:\n",
    "# List of keywords\n",
    "    KEYWORDS = ['pvt', 'guidance', 'dates of birth', 'date of birth', 'date and place', 'date de naissance', 'birth date', 'date for birth', 'dob/location', ' dob', 'dob ', 'dob:', 'dob-', 'd o b', '\\ndob\\n', '\\ndob.', '\\tdob.', '\\ndob,', 'year of birth', 'different dob', 'date(s) of birth', 'date/place of birth', 'date place of birth', 'date of the birth', 'date of \\nbirth', 'birthdate', 'date of berth', 'date of bitrh', 'date of brith', 'date \\nof birth', 'date\\n of birth', 'date of\\nbirth', 'date\\nof birth', 'different dob','exclude', 'client bhw', 'za obiady','gender']\n",
    "\n",
    "    \n",
    "    # Default folder path\n",
    "    #FOLDER_PATH = \"/var/app/cstxuat/tensorflow/gbrWorkspace/Code/refactored/sample_data/\"\n",
    "   # FOLDER_PATH = \"/var/app/cstxuat/tensorflow/gbrWorkspace/Code/data_emea/\"\n",
    "    #RESULTS_DIR = \"/var/app/cstxuat/tensorflow/gbrWorkspace/Code/data_emea/results\"\n",
    "    #RESULTS_DIR = \"/var/app/cstxuat/tensorflow/gbrWorkspace/Code/refactored/sample_data2/results\"\n",
    "    #FOLDER_PATH = \"/var/app/cstxuat/tensorflow/gbrWorkspace/Code/refactored/sample_data2/\"\n",
    "    # Input Type Disambiguation logic results and source files\n",
    "    #RESULTS_DIR = \"/var/app/cstxuat/tensorflow/gbrWorkspace/Code/data_emea/results/excel_results_grouping/\"\n",
    "    # Validation logic for individual matches only\n",
    "    #FOLDER_PATH = \"/var/app/cstxuat/tensorflow/gbrWorkspace/Code/data_emea/results/excel_results_grouping/validation_individual_results/\"\n",
    "    #RESULTS_DIR = \"/var/app/cstxuat/tensorflow/gbrWorkspace/Code/data_emea/results/excel_results_grouping/validation_individual_results/\"\n",
    "     # Validation logic for individual matches only\n",
    "    #FOLDER_PATH = \"/var/app/cstxuat/tensorflow/gbrWorkspace/Code/data_emea/results/excel_results_grouping/validation_entity_results/\"\n",
    "    #RESULTS_DIR = \"/var/app/cstxuat/tensorflow/gbrWorkspace/Code/data_emea/results/excel_results_grouping/validation_entity_results/\"\n",
    "    \n",
    "    # Join Labeling and modeling prediction files \n",
    "    #FOLDER_PATH = \"/var/app/cstxuat/tensorflow/gbrWorkspace/Code/data_emea/results/excel_results_grouping/validation_individual_results/results/\"\n",
    "    #RESULTS_DIR = \"/var/app/cstxuat/tensorflow/gbrWorkspace/Code/data_emea/results/excel_results_grouping/validation_individual_results/results/\"\n",
    "    \n",
    "    # Test one month\n",
    "   # FOLDER_PATH = \"/var/app/cstxuat/tensorflow/gbrWorkspace/Code/data_emea/results/excel_results_grouping/validation_individual_results/test_one_month/\"\n",
    "    #RESULTS_DIR = \"/var/app/cstxuat/tensorflow/gbrWorkspace/Code/data_emea/results/excel_results_grouping/validation_individual_results/test_one_month/\"\n",
    "    \n",
    "    # INDIVIDUAL\n",
    "    #FOLDER_PATH = \"/var/app/cstxuat/tensorflow/gbrWorkspace/Code/refactored/individual\"\n",
    "    #RESULTS_DIR = \"/var/app/cstxuat/tensorflow/gbrWorkspace/Code/refactored/individual\"\n",
    "    \n",
    "    # Individual ALL Months /tensorflow/gbrWorkspace/Code/refactored/individual_all_months\n",
    "    #FOLDER_PATH = \"/var/app/cstxuat/tensorflow/gbrWorkspace/Code/refactored/individual_all_months\"\n",
    "    #RESULTS_DIR = \"/var/app/cstxuat/tensorflow/gbrWorkspace/Code/refactored/individual_all_months\"\n",
    "    \n",
    "    # IEPLVA\n",
    "    #FOLDER_PATH = \"/var/app/cstxuat/tensorflow/gbrWorkspace/Code/refactored/all_entities\"\n",
    "    #RESULTS_DIR = \"/var/app/cstxuat/tensorflow/gbrWorkspace/Code/refactored/all_entities\"\n",
    "    \n",
    "    # E2EProfiling\n",
    "    #FOLDER_PATH = \"/var/app/cstxuat/tensorflow/gbrWorkspace/Code/refactored/end2end_profiling\"\n",
    "    #RESULTS_DIR = \"/var/app/cstxuat/tensorflow/gbrWorkspace/Code/refactored/end2end_profiling\"\n",
    "    \n",
    "    # remove external research\n",
    "    #FOLDER_PATH = \"'/var/app/cstxuat/tensorflow/LLM/song/train/dataprep/source_data\"\n",
    "    #RESULTS_DIR = \"/var/app/cstxuat/tensorflow/gbrWorkspace/Code/data_emea/clean_data\"\n",
    "    \n",
    "    # dedup external research\n",
    "    #FOLDER_PATH = \"/var/app/cstxuat/tensorflow/gbrWorkspace/Code/data_emea/external\"\n",
    "    #RESULTS_DIR = \"/var/app/cstxuat/tensorflow/gbrWorkspace/Code/data_emea/external\"\n",
    "    \n",
    "    # Other input type /var/app/cstxuat/tensorflow/song/individual/gb54464/train/external\n",
    "    #OLDER_PATH = \"/var/app/cstxuat/tensorflow/song/individual/gb54464/train/external\"\n",
    "    #RESULTS_DIR = \"/var/app/cstxuat/tensorflow/song/individual/gb54464/train/external\"\n",
    "    \n",
    "    # Resting Utils scripts /var/app/cstxuat/tensorflow/song/individual/gb54464/train/external/source_data\n",
    "    #FOLDER_PATH = \"/var/app/cstxuat/tensorflow/song/individual/gb54464/train/external/source_data\"\n",
    "    #RESULTS_DIR = \"/var/app/cstxuat/tensorflow/song/individual/gb54464/train/external/source_data\"\n",
    "    \n",
    "    # Song Logic scripts /var/app/cstxuat/tensorflow/song/individual/gb54464/train/external/source_data\n",
    "    FOLDER_PATH = \"/var/app/pctlness/tensorflow/song/python/data/prod/llm_data/clean_data\"\n",
    "    RESULTS_DIR = \"/var/app/pctlness/tensorflow/song/python/data/prod/llm_data/clean_data/\"\n",
    "    \n",
    "    # Dictionary for translating Polish phrases to English\n",
    "    POLISH_TO_ENGLISH = {\n",
    "        \"do wery\": \"to verify\",\n",
    "        \"brak możliwości wykluczenia bez przeprowadzenia dodatkowej inwestygacji\": \"cannot exclude without additional investigation\",\n",
    "        \"do weryfikacji\": \"to verify\",\n",
    "        \"na podstawie danych wejściowych wykluczam hit, klient bhw\": \"based on input data, I exclude the hit, bhw client\",\n",
    "        \"a wer\": \"to verify\",\n",
    "        \"klient bhw\": \"bhw client\",\n",
    "        \"łączenie wyrazów\": \"combining words\",\n",
    "        \"wykluczam hit, dopasowane komponenty nie wykazują żadnego potencjalnego związku z wpisem na liście\": \"I exclude the hit, the matching components do not show any potential relationship with the entry on the list\",\n",
    "        \"brak możliwości wykluczenia, do weryfikacji\": \"cannot exclude, to verify\",\n",
    "        \"wykluczam hit bazując na analogicznych płatnościach między tymi samymi stronami - tytuł płatności wskazuje na rozliczenie/opłatę/numer faktury, który jest generowany automatycznie jako przypadkowy ciąg alfanumeryczny, w którego formacie nie ma stałego miejsca na ciągi liter i/lub cyfr, można więc wykluczyć jakiekolwiek powiązanie tytułu z konkretnym sdn)\": \"I exclude the hit based on similar payments between the same parties - the payment title indicates the settlement/payment/invoice number, which is automatically generated as a random alphanumeric string, in whose format there is no fixed place for strings of letters and/or numbers, so any connection of the title with a specific SDN can be excluded\",\n",
    "        \"pełne dane wejściowe osoby fizycznej nie są dokładnym dopasowaniem\": \"the full input data of the physical person is not an exact match\",\n",
    "        \"na podstawie danych wejściowych wykluczam hit; dotyczy osoby fizycznej - nie ma związku z lokalizacją sdn\": \"based on the input data, I exclude the hit; it concerns an individual - there is no connection with the SDN location\",\n",
    "        \"do weryfikacji\": \"to verify\",\n",
    "        \"brak możliwości wykluczenia hitu bez dodatkowej inwestygacji\": \"cannot exclude the hit without additional investigation\",\n",
    "        \"wykluczam hit bazując na całości informacji (klient bhw, inne pełne nazwy + inne lokalizacje)\": \"I exclude the hit based on the overall information (bhw client, other full names + other locations)\",\n",
    "        \"wykluczam hit, dopasowane komponenty nie wykazują żadnego potencjalnego związku z wpisem na liście\": \"I exclude the hit, the matching components do not show any potential relationship with the entry on the list\",\n",
    "        \"łamanie linii - hit jest zawarty w innym wyrazie, nie ma związku z listą sdn\": \"breaking the line - the hit is contained in another word, there is no connection with the SDN list\",\n",
    "        \"dotyczy imienia/nazwiska płatności do szkoły / przedszkola za dziecko; brak związku z listą sdn\": \"it concerns the name / surname of the payment to school / kindergarten for the child; there is no connection with the SDN list\",\n",
    "        \"do weryfikacji\": \"to verify\",\n",
    "        \"na podstawie danych z przelewu wykluczam hit, klient bhw\": \"based on the data from the transfer, I exclude the hit, bhw client\",\n",
    "        \"bazując na stronach transakcji (serwis internetowy służący do rezerwacji zakwaterowania online) oraz na analogicznych płatnościach, tytuł płatności wskazuje na rozliczenie/opłatę /rezerwację , to ciąg znaków który jest generowany automatycznie jako przypadkowy ciąg alfanumeryczny, w którego formacie nie ma stałego miejsca na ciągi liter i/lub cyfr, można więc wykluczyć jakiekolwiek powiązanie tytułu, z sdn\": \"based on the pages. transaction (online accommodation booking service) and similar payments, the payment title indicates settlement / payment / reservation, this is a string of characters that is generated automatically as a random alphanumeric string, in whose format there is no fixed place for strings of letters and/or numbers, so any connection of the title with the SDN can be excluded\",\n",
    "        \"wykluczam hit, bazując na stronach transakcji i detalach płatności, skrót 'spp' oznacza strefę płatnego parkowania, nie d\": \"I exclude the hit, based on the pages of the transaction and the details of the payment, the abbreviation 'spp' means the paid parking zone, no d\",\n",
    "        \"wykluczam hit dopasowane komponenty nie wykazuja żadnego potencjalnego związku z wpisem na liście\": \"I exclude the hit, matched components do not show any potential relationship with the entry on the list\",\n",
    "        \"bazując na całości informacji zlecenia - stronach oraz tytule, który jest związany z wynagrodzeniem dla nauczycieli, brak związku z sdn\": \"Based on the entirety of the order information - pages and title, which is related to the remuneration for teachers, there is no connection with SDN\",\n",
    "        \"dotyczy imienia, nazwisko takie samo jak w polu nadawcy: płatność do szkoły / przedszkola za dziecko; brak związku z listą sdn\": \"it concerns the name, surname the same as in the sender's field: payment to school / kindergarten for the child; there is no connection with the SDN list\",\n",
    "        \"maker - transakcje w ofac procesują się prawidłowo w dniu 27/04/2023\": \"maker - OFAC transactions are processed correctly on 27/04/2023\",\n",
    "        \"typ firmy wyklucza udział podmiotu sankcjonowanego\": \"the type of company excludes the participation of a sanctioned entity\",\n",
    "        \"wykluczam hit, bazując na stronach transakcji i detalach płatności, skrót 'us' dotyczy urząd stanu cywilnego, odmienne pełne nazwy i lokalizacje, brak związku z podmiotem pjsc united shipbuilding corporation z listy sdn-mail z dnia 02/07/2023\": \"I exclude the hit, based on the pages of the transaction and the details of the payment, the abbreviation 'us' refers to the civil status office, different full names and locations, no connection with the entity PJSC United Shipbuilding Corporation from the SDN list-mail from 02/07/2023\",\n",
    "        \"wykluczam hit bazując na stronach transakcji i detalach płatności; tytuł płatności wskazuje na opłatę za kurs jezykowy, brak związku z listą sdn\": \"I exclude the hit based on the pages of the transaction and the details of the payment; the payment title indicates a payment for a language course, no connection with the SDN list\",\n",
    "        \"kierownik ds. kontroli\": \"manager for control\",\n",
    "        \"do wer\": \"to verify\",\n",
    "        \"brak mozliwosci wykluczenia bez przeprowadzenia dodatkowei inwestygacji\": \"Potential Match: Escalated for further review\",\n",
    "        \"do weryfikacii\": \"to verify\",\n",
    "        \"na podstawie danych wejsciowych wykluczam hit, klient bhw\": \"based on input data, I exclude the hit, bhw client\",\n",
    "        \"a wer\": \"to verify\",\n",
    "        \"klient bhw\": \"bhw client\",\n",
    "        'kleitn bhw':\"bhw client\", 'klien tbhw': 'bhw client', 'klient bnhw': 'bhw client', 'klienty bhw': 'bhw client',\n",
    "        \"laczenie wyrazow\": \"combining words\",\n",
    "        \"wykluczam hit dopasowane komponenty nie, wykazuja Zadnego potencjalnego zwiazku z wisem na liscie\": \"I exclude the hit, the matching components do not show any potential relationship with the entry on the list\",\n",
    "        \"wykluczam hit dopasowane komponenty nie wykazują żadnego potencjalnego związku z wpisem na liście\": \"I exclude the hit, the matching components do not show any potential relationship with the entry on the list\",\n",
    "        \"brak mozliwosci wykluczenia bez przeprowadzenia dodatkowei inwestygacji\": \"Potential Match: Escalated for further review\",\n",
    "        \"brak mozliwosci wykluczenia, do weryfikacji\": \"cannot exclude, to verify\",\n",
    "        \"wvkluczam hit bazujac na analogicznych platnosciach miedzy tymi samymi stronami - tytul platnosci wskazuje na rozliczen ie/oplate/numer faktury, ktory jest generowany automatycznie jako przypadkowy ciag alfanumeryczny, w którego formacie nie ma stalego miejsca na ciagi liter i/lub cyfr, moana wiec wykluczy jakiekolwiek powiazanie tytulu z konkretnym sdn)\": \"I exclude the hit based on similar payments between the same parties - the payment title indicates the settlement/payment/invoice number, which is automatically generated as a random alphanumeric string, in whose format there is no fixed place for strings of letters and/or numbers, so any connection of the title with a specific SDN can be excluded\",\n",
    "        \"pelne dane weisciowe osoby fizycznei nie sa dokladnym dopasowaniem\": \"the full input data of the physical person is not an exact match\",\n",
    "        \"na podstawie danych wejsciowych wykluczam hit; dotyczy osoby fizycznej - nie ma zwiazku z lokalizacja sdn\": \"based on the input data, I exclude the hit; it concerns an individual - there is no connection with the SDN location\",\n",
    "        \"dow ep\": \"to verify\",\n",
    "        \"brak mozliwosci wykluczenia hit bez dodatkowei inwestygacji\": \"cannot exclude the hit without additional investigation\",\n",
    "        \"wykluczam hit bazujac na caloksztalcie informacii (klient bhw, inne pelne nazwy + inne lokalizacje)\": \"I exclude the hit based on the overall information (bhw client, other full names + other locations)\",\n",
    "        \"input is another known input type provided in a known unstructured format (free form field) that clearly indicates it i s not the same type of information as the list entry (e\": \"input is another known input type provided in a known unstructured format (free form field) that clearly indicates it is not the same type of information as the list entry\",\n",
    "        \"wykluczam hit dopasowane komponenty nie wykazuja zadnego potencjalnego zwiazku z wpisem na liscie\": \"I exclude the hit, the matching components do not show any potential relationship with the entry on the list\",\n",
    "        \"brak mozliwosci wykluczenia bez przeprowadzenia dodatkowej inwestygacji\": \"cannot exclude without additional investigation\",\n",
    "        \"Iamanie linii -hit jest zawarty w innym wyrazie, nie ma zwiazku z lista sdn\": \"breaking the line - the hit is contained in another word, there is no connection with the SDN list\",\n",
    "        \"na podstawie danych weisciowych wykluczam hit; dotyczy osoby fizycznej - nie ma zwiazku z lokalizacja sdn\": \"based on the input data, I exclude the hit; it concerns an individual - there is no connection with the SDN location\",\n",
    "        \"brak mozliwosci wykluczenia bez przeprowadzenia dodatkowei inwestygacii\": \"cannot exclude without additional investigation\",\n",
    "        \"dotyczy imienia/nazwiska platnose do szkoly / przedszkola za dziecko; brak zwiazku z lista sdn\": \"it concerns the name / surname of the payment to school / kindergarten for the child; there is no connection with the SDN list\",\n",
    "        \"dower\": \"to verify\",\n",
    "        \"na podstawie danych z przelewu wykluczam hit, klient bhw\": \"based on the data from the transfer, I exclude the hit, bhw client\",\n",
    "        \"na podstawie danych wejsciowych wykluczam hit, klient. bhw\": \"based on the input data, I exclude the hit, bhw client\",\n",
    "        \"na podstawie danych wejściowych wykluczam hit bhw client\": \"based on the input data, I exclude the hit, bhw client\",\n",
    "        \"bazuiac na stronach. transakcji (serwis internetowy stuzacy do rezerwacii zakwaterowania online) oraz a analogicznych platnosciach, tytul platnosci wskazuje na rozliczenie/oplate /rezerwacje , to ciag nakow ktory jest generowany automatyeznie jako przypadkowy ciag alfanumeryczny, w kórego formacie nie ma stalego miejsca na ciagi liter i/lub cyfr, mo¿na wiec wykluczy\": \"based on the pages. transaction (online accommodation booking service) and similar payments, the payment title indicates settlement / payment / reservation, this is a string of characters that is generated automatically as a random alphanumeric string, in whose format there is no fixed place for strings of letters and/or numbers, so any connection of the title with the SDN can be excluded\",\n",
    "        \"bazuiac na stronach transakcji (serwis internetowy stuzacy do rezerwacii zakwaterowania online) oraz na analogicznych platosciach, tytul platnosci wskazuje na rozliczenie/oplate /rezerwacie , to ciag znakow ktory jest generowany. automatycznie jako przypadkowy ciag alfanumeryczny, w kórego formacie nie ma stalego miejsca na ciagi liter i/lub cyfr, mozna wiec wykluczy é lakiekolwiek powiazanie tytulu, z sdn\": \"based on the pages. transaction (online accommodation booking service) and similar payments, the payment title indicates settlement / payment / reservation, this is a string of characters that is generated automatically as a random alphanumeric string, in whose format there is no fixed place for strings of letters and/or numbers, so any connection of the title with the SDN can be excluded\",\n",
    "        \"wykluczam hit bazujac na analogicznych platnosciach miedzy tymi samymi stronami - tytul platnosci wskazuje na rozlicze nie/oplate/numer faktury, który jest generowany automatycznie jako przypadkowy ciag alfanumeryczny, w którego formacie nie ma stalego miejsca na ciagi liter i/lub cyfr, mozna wiec wykluczy& jakiekolwiek powiazanie tytulu z konkretnym sdn)\": \"I exclude the hit based on similar payments between the same parties - the payment title indicates the settlement/payment/invoice number, which is automatically generated as a random alphanumeric string, in whose format there is no fixed place for strings of letters and/or numbers, so any connection of the title with a specific SDN can be excluded\",\n",
    "        \"wykluczam hit, bazujac na stronach transakcji i detalach platnosci, skrot 'spp oznacza strefa platnego parkowania, nie d\": \"I exclude the hit, based on the pages of the transaction and the details of the payment, the abbreviation 'spp' means the paid parking zone, no d\",\n",
    "        \"wykluczam bazując na całokształcie informacji\":\"exclude hit\" ,\n",
    "        \"dane wejściowe zawierają jeden element, wówczas dopasowanie należy uznać za fałszywe\" : \"the input contains one element, then the match should be considered false\",\n",
    "        \"wykluczam hit bazując na całokształcie informacji \": \"exclude hit\",\n",
    "        \"wykluczam hit bazując na całokształcie informacji\": \"exclude hit\",\n",
    "        \"dane wejściowe zawierają jeden element, wówczas dopasowanie należy uznać za fałszywe\": \"input contains one element\",\n",
    "        \"do wer\": \"to verify\",\n",
    "        \"brak mozliwosci wykluczenia bez przeprowadzenia dodatkowei inwestygacji\": \"Potential Match: Escalated for further review\",\n",
    "        \"do weryfikacii\": \"to verify\",\n",
    "        \"na podstawie danych wejsciowych wykluczam hit, klient bhw\": \"bhw client\",\n",
    "        \"a wer\": \"to verify\",\n",
    "        \"klient bhw\": \"bhw client\",\n",
    "        'kleitn bhw':\"bhw client\", 'klien tbhw': 'bhw client', 'klient bnhw': 'bhw client', 'klienty bhw': 'bhw client',\n",
    "        \"laczenie wyrazow\": \"combining words\",\n",
    "        \"wykluczam hit dopasowane komponenty nie, wykazuja Zadnego potencjalnego zwiazku z wisem na liscie\": \"I exclude the hit, the matching components do not show any potential relationship with the entry on the list\",\n",
    "        \"wykluczam hit dopasowane komponenty nie wykazują żadnego potencjalnego związku z wpisem na liście\": \"I exclude the hit, the matching components do not show any potential relationship with the entry on the list\",\n",
    "        \"brak mozliwosci wykluczenia bez przeprowadzenia dodatkowei inwestygacji\": \"cannot exclude without additional investigation\",\n",
    "        \"brak mozliwosci wykluczenia, do weryfikacji\": \"cannot exclude, to verify\",\n",
    "        \"wvkluczam hit bazujac na analogicznych platnosciach miedzy tymi samymi stronami - tytul platnosci wskazuje na rozliczen ie/oplate/numer faktury, ktory jest generowany automatycznie jako przypadkowy ciag alfanumeryczny, w którego formacie nie ma stalego miejsca na ciagi liter i/lub cyfr, moana wiec wykluczy jakiekolwiek powiazanie tytulu z konkretnym sdn)\": \"I exclude the hit based on similar payments between the same parties - the payment title indicates the settlement/payment/invoice number, which is automatically generated as a random alphanumeric string, in whose format there is no fixed place for strings of letters and/or numbers, so any connection of the title with a specific SDN can be excluded\",\n",
    "        \"pelne dane weisciowe osoby fizycznei nie sa dokladnym dopasowaniem\": \"the full input data of the physical person is not an exact match\",\n",
    "        \"na podstawie danych wejsciowych wykluczam hit; dotyczy osoby fizycznej - nie ma zwiazku z lokalizacja sdn\": \"based on the input data, I exclude the hit; it concerns an individual - there is no connection with the SDN location\",\n",
    "        \"dow ep\": \"to verify\",\n",
    "        \"brak mozliwosci wykluczenia hit bez dodatkowei inwestygacji\": \"cannot exclude the hit without additional investigation\",\n",
    "        \"wykluczam hit bazujac na caloksztalcie informacii (klient bhw, inne pelne nazwy + inne lokalizacje)\": \"I exclude the hit based on the overall information (bhw client, other full names + other locations)\",\n",
    "        \"input is another known input type provided in a known unstructured format (free form field) that clearly indicates it i s not the same type of information as the list entry (e\": \"input is another known input type provided in a known unstructured format (free form field) that clearly indicates it is not the same type of information as the list entry\",\n",
    "        \"wykluczam hit dopasowane komponenty nie wykazuja zadnego potencjalnego zwiazku z wpisem na liscie\": \"the matching components do not show any potential relationship with the entry on the list\",\n",
    "        \"brak mozliwosci wykluczenia bez przeprowadzenia dodatkowej inwestygacji\": \"cannot exclude without additional investigation\",\n",
    "        \"Iamanie linii -hit jest zawarty w innym wyrazie, nie ma zwiazku z lista sdn\": \"breaking the line - the hit is contained in another word, there is no connection with the SDN list\",\n",
    "        \"na podstawie danych weisciowych wykluczam hit; dotyczy osoby fizycznej - nie ma zwiazku z lokalizacja sdn\": \"based on the input data,  it concerns an individual - there is no connection with the SDN location\",\n",
    "        \"brak mozliwosci wykluczenia bez przeprowadzenia dodatkowei inwestygacii\": \"Potential Match: Escalated for further review\",\n",
    "        \"dotyczy imienia/nazwiska platnose do szkoly / przedszkola za dziecko; brak zwiazku z lista sdn\": \"it concerns the name / surname of the payment to school / kindergarten for the child; there is no connection with the SDN list\",\n",
    "        \"dower\": \"to verify\",\n",
    "        \"na podstawie danych z przelewu wykluczam hit, klient bhw\": \"bhw client\",\n",
    "        \"na podstawie danych wejsciowych wykluczam hit, klient. bhw\": \"bhw client\",\n",
    "        \"na podstawie danych wejściowych wykluczam hit bhw client\": \"bhw client\",\n",
    "        \"bazuiac na stronach. transakcji (serwis internetowy stuzacy do rezerwacii zakwaterowania online) oraz a analogicznych platnosciach, tytul platnosci wskazuje na rozliczenie/oplate /rezerwacje , to ciag nakow ktory jest generowany automatyeznie jako przypadkowy ciag alfanumeryczny, w kórego formacie nie ma stalego miejsca na ciagi liter i/lub cyfr, mo¿na wiec wykluczy\": \"based on the pages. transaction (online accommodation booking service) and similar payments, the payment title indicates settlement / payment / reservation, this is a string of characters that is generated automatically as a random alphanumeric string, in whose format there is no fixed place for strings of letters and/or numbers, so any connection of the title with the SDN can be excluded\",\n",
    "        \"bazuiac na stronach transakcji (serwis internetowy stuzacy do rezerwacii zakwaterowania online) oraz na analogicznych platosciach, tytul platnosci wskazuje na rozliczenie/oplate /rezerwacie , to ciag znakow ktory jest generowany. automatycznie jako przypadkowy ciag alfanumeryczny, w kórego formacie nie ma stalego miejsca na ciagi liter i/lub cyfr, mozna wiec wykluczy é lakiekolwiek powiazanie tytulu, z sdn\": \"based on the pages. transaction (online accommodation booking service) and similar payments, the payment title indicates settlement / payment / reservation, this is a string of characters that is generated automatically as a random alphanumeric string, in whose format there is no fixed place for strings of letters and/or numbers, so any connection of the title with the SDN can be excluded\",\n",
    "        \"wykluczam hit bazujac na analogicznych platnosciach miedzy tymi samymi stronami - tytul platnosci wskazuje na rozlicze nie/oplate/numer faktury, który jest generowany automatycznie jako przypadkowy ciag alfanumeryczny, w którego formacie nie ma stalego miejsca na ciagi liter i/lub cyfr, mozna wiec wykluczy& jakiekolwiek powiazanie tytulu z konkretnym sdn)\": \"I exclude the hit based on similar payments between the same parties - the payment title indicates the settlement/payment/invoice number, which is automatically generated as a random alphanumeric string, in whose format there is no fixed place for strings of letters and/or numbers, so any connection of the title with a specific SDN can be excluded\",\n",
    "        \"wykluczam hit, bazujac na stronach transakcji i detalach platnosci, skrot 'spp oznacza strefa platnego parkowania, nie d\": \"I exclude the hit, based on the pages of the transaction and the details of the payment, the abbreviation 'spp' means the paid parking zone, no d\",\n",
    "        \"łamanie linii hit jest zawarty w innym wyrazie nie ma związku z listą sd\":\"linebreak hit is contained in another word has no relation to the sdn list\"}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting nam_end2endProfiling_dedup_step_monthly_adding_word.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile nam_end2endProfiling_dedup_step_monthly_adding_word.py\n",
    "import warnings\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import cx_Oracle\n",
    "import logging\n",
    "import time\n",
    "from config_dedup import Config\n",
    "\n",
    "# Suppress warnings and set seeds\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ[\"KMP_WARNINGS\"] = \"FALSE\"\n",
    "random_seed = 23\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "\n",
    "# Paths for CS Constants\n",
    "sys.path.append(\"/var/app/pctlness/tensorflow/song/python/process/common/\")\n",
    "sys.path.append(\"/var/app/pctlness/tensorflow/song/unix/bin/\")\n",
    "\n",
    "from CSConstants import * \n",
    "sys.path.append(CONTEXT_INPUT_METADATA)\n",
    "sys.path.append(CONTEXT_INPUT_MODEL)\n",
    "sys.path.append(CONTEXT_INPUT_PROCESS)\n",
    "from TXInputTypeProcess import *\n",
    "\n",
    "import pandas as pd\n",
    "import csv\n",
    "import cx_Oracle\n",
    "\n",
    "class DB:\n",
    "    def __init__(self):\n",
    "        self.hostname=hostname\n",
    "        self.port=port\n",
    "        self.service_name=service_name\n",
    "        self.user_name=user_name\n",
    "        self.user_pass=passwd\n",
    "        self.con_str = self.user_name+'/'+self.user_pass+'@'+self.hostname+':'+ self.port+'/'+self.service_name\n",
    "        self.conn = cx_Oracle.connect(self.con_str )\n",
    "        \n",
    "    def connect(self):\n",
    "         self.conn = cx_Oracle.connect(self.con_str)\n",
    "        \n",
    "    def get_connection(self):\n",
    "        return self.conn\n",
    "    \n",
    "    def close_connection(self):\n",
    "        self.conn.close()\n",
    "        print('Connection closed successfully')\n",
    "        \n",
    "        \n",
    "##con = cx_Oracle.connect('SONG/sm294HfD@clfdsngdbu-scan.eur.nsroot.net:8889/ESNGFDU')\n",
    "con = cx_Oracle.connect('SONG/rA2ZDtEa@clswsngdbu-scan.nam.nsroot.net:7001/NSNGSWU')\n",
    "\n",
    "\n",
    "# Save a large DataFrame to multiple sheets in an Excel file\n",
    "def save_to_multiple_sheets(df, output_filename, split_size=1_000_000):\n",
    "    with pd.ExcelWriter(output_filename, engine='openpyxl') as writer:\n",
    "        for i, start in enumerate(range(0, len(df), split_size)):\n",
    "            df.iloc[start:start+split_size].to_excel(writer, sheet_name=f\"Sheet_{i+1}\", index=False)\n",
    "\n",
    "# Data Extraction for a given month\n",
    "def extract_data_for_month(month_start, month_end):\n",
    "    query = f\"\"\"\n",
    "        SELECT m.HIT_ID,\n",
    "               MATCH_LENGTH,\n",
    "               ALGORITHIM3_CLASSIFICATION song_list_type,\n",
    "               ALGORITHIM4_CLASSIFICATION song_input_type,\n",
    "               L2_MAKER_INPUTTYPE,\n",
    "               L2_MAKER_LISTTYPE,\n",
    "               r.INPUT_NAME,\n",
    "               ALGORITHIM2_ACTION song_action, \n",
    "               ALGORITHIM2_NOTE song_note,\n",
    "               r.HMM_TOKENS,\n",
    "               L2_MAKER_ACTION,\n",
    "               L2_MAKER_NOTE\n",
    "        FROM match_det m\n",
    "        LEFT JOIN run_result r ON r.hit_id = m.HIT_ID\n",
    "        WHERE m.L1_MAKER_DATE BETWEEN '{month_start}' AND '{month_end}'\n",
    "        \n",
    "    \"\"\"\n",
    "    return pd.read_sql(query, con)\n",
    "\n",
    "def main():\n",
    "    start_time = time.time()\n",
    "    \n",
    "    logging.basicConfig(filename=\"debug.log\", level=logging.INFO, format=\"%(levelname)s: %(message)s\")\n",
    "    logging.info(\"Starting the script!\")\n",
    "    \n",
    "    file_input_types =  ['Individual', 'Entity', 'Place_Location', 'Vessel_Aircraft', 'Other_Input_Type']\n",
    "    #file_input_types =  ['Other_Input_Type']\n",
    "\n",
    "\n",
    "    excel_paths = {input_type: os.path.join(Config.RESULTS_DIR, f\"{input_type}_vsAll_20231005160139.xlsx\") for input_type in file_input_types}\n",
    "    print(excel_paths)\n",
    "    all_data = {input_type.replace(\"Other_Input_Type\", \"Other Input Type\").replace(\"Place_Location\", \"Place/Location\").replace(\"Vessel_Aircraft\",\"Vessel/Aircraft\"): pd.read_excel(path, engine='openpyxl') for input_type, path in excel_paths.items()}\n",
    "    print(excel_paths)\n",
    "\n",
    "    all_db_data = []\n",
    "    months_to_process = [\n",
    "        ('01-JAN-2023', '31-JAN-2023'),\n",
    "        ('01-FEB-2023', '28-FEB-2023'),\n",
    "    ]    \n",
    "    for month_start, month_end in months_to_process:\n",
    "        monthly_data_db = extract_data_for_month(month_start, month_end)        \n",
    "        logging.info(f\"Extracted Data from DB Columns are {list(monthly_data_db.columns)} starting in {month_start}.\")\n",
    "        logging.info(f\"Extracted {len(monthly_data_db)} records from database for the specified month range.\")\n",
    "        monthly_data_db.rename(columns={'HIT_ID':'hit_id', 'SONG_INPUT_TYPE':'song_input_type','SONG_LIST_TYPE':'song_list_type', 'HMM_TOKENS':'TRIGRAM_TOKENS'}, inplace=True)\n",
    "        logging.info(f\"Extracted Data from DB Columns after renaming are {list(monthly_data_db.columns)}.\")\n",
    "        monthly_data_db[\"start_date\"] = month_start\n",
    "        monthly_data_db[\"end_date\"] = month_end\n",
    "        all_db_data.append(monthly_data_db)\n",
    "    \n",
    "    all_db_data_df = pd.concat(all_db_data)\n",
    "    print(f\"The extracted data for all 6 months contained: {len(all_db_data_df)} records\")\n",
    "    logging.info(f\"The extracted data for all 6 months contained: {len(all_db_data_df)} records\")\n",
    "\n",
    "    all_db_data_df.drop_duplicates(inplace=True)\n",
    "    logging.info(f\"The Joined data for all 6 months contained removing duplicates: {len(all_db_data_df)} records\")\n",
    "    print(f\"The Joined data for all 6 months contained removing duplicates: {len(all_db_data_df)} records\")\n",
    "\n",
    "\n",
    "    \n",
    "    excel_columns_to_keep = ['business_unit',\n",
    "                           'match_tag', \n",
    "                           'match_value', \n",
    "                           'offset', \n",
    "                           'match_text', \n",
    "                           'match_pattern_value', \n",
    "                           'list_name', \n",
    "                           'l1_maker_listtype', \n",
    "                           'l1_maker_inputtype', \n",
    "                           'guideline_action', \n",
    "                           'guideline_reason', \n",
    "                           'hit_id']\n",
    "    \n",
    "    for input_type in ['Individual', 'Entity', 'Place/Location', 'Vessel/Aircraft', 'Other Input Type']:\n",
    "    #for input_type in ['Other Input Type']:\n",
    "\n",
    "        logging.info(f\"Processing data for input type: {input_type}\")\n",
    "        \n",
    "        # Merge Excel data with DB data\n",
    "        merged_data = pd.merge(all_data[input_type][excel_columns_to_keep], all_db_data_df, on='hit_id', how='left')\n",
    "        logging.info(f\"Merged Data Columns after renaming are {list(merged_data.columns)}.\")\n",
    "\n",
    "        \n",
    "        # Drop duplicates based on 'hit_id'\n",
    "        merged_data.drop_duplicates(subset=['hit_id'], inplace=True)\n",
    "\n",
    "        # Logic for Input Type Selection\n",
    "        condition_song_l1_agreement = (merged_data['song_input_type'] == input_type) & (merged_data['l1_maker_inputtype'] == input_type)\n",
    "        condition_song_l2_agreement = (merged_data['song_input_type'] == input_type) & (merged_data['L2_MAKER_INPUTTYPE'] == input_type)\n",
    "        condition_l2_blank_song_l1_agreement = (pd.isna(merged_data['L2_MAKER_INPUTTYPE']) & condition_song_l1_agreement)\n",
    "\n",
    "        merged_data['final_input_type'] = np.where(condition_song_l1_agreement, input_type, \n",
    "                                                  np.where(condition_song_l2_agreement, input_type, np.nan))\n",
    "        merged_data['status_input_type'] = np.where(condition_l2_blank_song_l1_agreement | condition_song_l2_agreement, 100, 0)\n",
    "\n",
    "        # Logic for List Type \n",
    "        condition_song_l1_listtype_agreement = (merged_data['song_list_type'] == merged_data['l1_maker_listtype'])\n",
    "        condition_song_l2_listtype_agreement = (merged_data['song_list_type'] == merged_data['L2_MAKER_LISTTYPE'])\n",
    "        condition_use_l2_as_default = ~condition_song_l1_listtype_agreement & ~condition_song_l2_listtype_agreement & merged_data['L2_MAKER_LISTTYPE'].notna()\n",
    "\n",
    "        # Apply conditions\n",
    "        merged_data['status_list_type'] = np.where(condition_song_l1_listtype_agreement, 100, \n",
    "                                                   np.where(condition_song_l2_listtype_agreement, 100, 0))\n",
    "\n",
    "        merged_data['final_list_type'] = np.where(condition_song_l1_listtype_agreement, merged_data['song_list_type'],\n",
    "                                                  np.where(condition_song_l2_listtype_agreement, merged_data['song_list_type'],\n",
    "                                                           np.where(condition_use_l2_as_default, merged_data['L2_MAKER_LISTTYPE'], np.nan)))\n",
    "        # Drop duplicates\n",
    "        merged_data.drop_duplicates(inplace=True)\n",
    "        \n",
    "        print(f\"the total number of records so far: {len(merged_data)}\")\n",
    "        \n",
    "        \n",
    "        input_type_cleaned = input_type.replace(\"/\",\"_\")\n",
    "        \n",
    "        merged_data = merged_data[merged_data['MATCH_LENGTH'].notna()]\n",
    "        print(f\"the total number of records so far after dropping non floats in MATCH_LENGTH: {len(merged_data)}\")\n",
    "\n",
    "        \n",
    "        \n",
    "        # Get the Word \n",
    "        merged_data['MATCH_WORD'] =  merged_data.apply(lambda x: fetchMatchText( x['match_value'],x['offset'],x['MATCH_LENGTH']), axis=1) \n",
    "        \n",
    "        # Save the processed data\n",
    "        save_to_multiple_sheets(merged_data, os.path.join(Config.RESULTS_DIR, f\"final_data_deduplicated_monthly_{input_type.replace('/', '_')}.xlsx\"))\n",
    "\n",
    "    end_time = time.time()\n",
    "    duration = (end_time - start_time) / 60\n",
    "    logging.info(f\"Script run duration: {duration:.2f} minutes\")\n",
    "    logging.info(\"Script completed successfully!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Starting the script!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Individual': '/var/app/pctlness/tensorflow/song/python/data/prod/llm_data/clean_data/Individual_vsAll_20231005160139.xlsx', 'Entity': '/var/app/pctlness/tensorflow/song/python/data/prod/llm_data/clean_data/Entity_vsAll_20231005160139.xlsx', 'Place_Location': '/var/app/pctlness/tensorflow/song/python/data/prod/llm_data/clean_data/Place_Location_vsAll_20231005160139.xlsx', 'Vessel_Aircraft': '/var/app/pctlness/tensorflow/song/python/data/prod/llm_data/clean_data/Vessel_Aircraft_vsAll_20231005160139.xlsx', 'Other_Input_Type': '/var/app/pctlness/tensorflow/song/python/data/prod/llm_data/clean_data/Other_Input_Type_vsAll_20231005160139.xlsx'}\n",
      "{'Individual': '/var/app/pctlness/tensorflow/song/python/data/prod/llm_data/clean_data/Individual_vsAll_20231005160139.xlsx', 'Entity': '/var/app/pctlness/tensorflow/song/python/data/prod/llm_data/clean_data/Entity_vsAll_20231005160139.xlsx', 'Place_Location': '/var/app/pctlness/tensorflow/song/python/data/prod/llm_data/clean_data/Place_Location_vsAll_20231005160139.xlsx', 'Vessel_Aircraft': '/var/app/pctlness/tensorflow/song/python/data/prod/llm_data/clean_data/Vessel_Aircraft_vsAll_20231005160139.xlsx', 'Other_Input_Type': '/var/app/pctlness/tensorflow/song/python/data/prod/llm_data/clean_data/Other_Input_Type_vsAll_20231005160139.xlsx'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Extracted Data from DB Columns are ['HIT_ID', 'MATCH_LENGTH', 'SONG_LIST_TYPE', 'SONG_INPUT_TYPE', 'L2_MAKER_INPUTTYPE', 'L2_MAKER_LISTTYPE', 'INPUT_NAME', 'SONG_ACTION', 'SONG_NOTE', 'HMM_TOKENS', 'L2_MAKER_ACTION', 'L2_MAKER_NOTE'] starting in 01-JAN-2023.\n",
      "INFO: Extracted 551615 records from database for the specified month range.\n",
      "INFO: Extracted Data from DB Columns after renaming are ['hit_id', 'MATCH_LENGTH', 'song_list_type', 'song_input_type', 'L2_MAKER_INPUTTYPE', 'L2_MAKER_LISTTYPE', 'INPUT_NAME', 'SONG_ACTION', 'SONG_NOTE', 'TRIGRAM_TOKENS', 'L2_MAKER_ACTION', 'L2_MAKER_NOTE'].\n",
      "INFO: Extracted Data from DB Columns are ['HIT_ID', 'MATCH_LENGTH', 'SONG_LIST_TYPE', 'SONG_INPUT_TYPE', 'L2_MAKER_INPUTTYPE', 'L2_MAKER_LISTTYPE', 'INPUT_NAME', 'SONG_ACTION', 'SONG_NOTE', 'HMM_TOKENS', 'L2_MAKER_ACTION', 'L2_MAKER_NOTE'] starting in 01-FEB-2023.\n",
      "INFO: Extracted 522965 records from database for the specified month range.\n",
      "INFO: Extracted Data from DB Columns after renaming are ['hit_id', 'MATCH_LENGTH', 'song_list_type', 'song_input_type', 'L2_MAKER_INPUTTYPE', 'L2_MAKER_LISTTYPE', 'INPUT_NAME', 'SONG_ACTION', 'SONG_NOTE', 'TRIGRAM_TOKENS', 'L2_MAKER_ACTION', 'L2_MAKER_NOTE'].\n",
      "INFO: The extracted data for all 6 months contained: 1074580 records\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The extracted data for all 6 months contained: 1074580 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: The Joined data for all 6 months contained removing duplicates: 1074580 records\n",
      "INFO: Processing data for input type: Individual\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Joined data for all 6 months contained removing duplicates: 1074580 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Merged Data Columns after renaming are ['business_unit', 'match_tag', 'match_value', 'offset', 'match_text', 'match_pattern_value', 'list_name', 'l1_maker_listtype', 'l1_maker_inputtype', 'guideline_action', 'guideline_reason', 'hit_id', 'MATCH_LENGTH', 'song_list_type', 'song_input_type', 'L2_MAKER_INPUTTYPE', 'L2_MAKER_LISTTYPE', 'INPUT_NAME', 'SONG_ACTION', 'SONG_NOTE', 'TRIGRAM_TOKENS', 'L2_MAKER_ACTION', 'L2_MAKER_NOTE', 'start_date', 'end_date'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the total number of records so far: 854312\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'get_compression_method'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m~/tensorflow/song/individual/gb54464/nam_end2endProfiling_dedup_step_monthly_adding_word.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/tensorflow/song/individual/gb54464/nam_end2endProfiling_dedup_step_monthly_adding_word.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0minput_type_cleaned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"_\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m         \u001b[0mno_match_length\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"problematic_match_length_{input_type_cleaned}.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0mmerged_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmerged_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmerged_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'MATCH_LENGTH'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/middleware/anaconda_python/3.6.10/envs/tf/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal)\u001b[0m\n\u001b[1;32m   3202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_is_cached\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool_t\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3203\u001b[0m         \u001b[0;34m\"\"\"Return boolean indicating if self is cached or not.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3204\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_cacher\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_cacher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/middleware/anaconda_python/3.6.10/envs/tf/lib/python3.6/site-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnotna\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m from pandas.io.common import (\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mget_compression_method\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mget_filepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'get_compression_method'"
     ]
    }
   ],
   "source": [
    "%run nam_end2endProfiling_dedup_step_monthly_adding_word.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sys' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-f896d61b0168>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/var/app/pctlness/tensorflow/song/python/process/common/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mCSConstants\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCONTEXT_INPUT_METADATA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCONTEXT_INPUT_MODEL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCONTEXT_INPUT_PROCESS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sys' is not defined"
     ]
    }
   ],
   "source": [
    "sys.path.append(\"/var/app/pctlness/tensorflow/song/python/process/common/\")\n",
    "from CSConstants import * \n",
    "sys.path.append(CONTEXT_INPUT_METADATA)\n",
    "sys.path.append(CONTEXT_INPUT_MODEL)\n",
    "sys.path.append(CONTEXT_INPUT_PROCESS)\n",
    "from TXInputTypeProcess import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Starting the script!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Individual': '/var/app/pctlness/tensorflow/song/python/data/prod/llm_data/clean_data/Individual_vsAll_20231005160139.xlsx', 'Entity': '/var/app/pctlness/tensorflow/song/python/data/prod/llm_data/clean_data/Entity_vsAll_20231005160139.xlsx', 'Place_Location': '/var/app/pctlness/tensorflow/song/python/data/prod/llm_data/clean_data/Place_Location_vsAll_20231005160139.xlsx', 'Vessel_Aircraft': '/var/app/pctlness/tensorflow/song/python/data/prod/llm_data/clean_data/Vessel_Aircraft_vsAll_20231005160139.xlsx', 'Other_Input_Type': '/var/app/pctlness/tensorflow/song/python/data/prod/llm_data/clean_data/Other_Input_Type_vsAll_20231005160139.xlsx'}\n",
      "{'Individual': '/var/app/pctlness/tensorflow/song/python/data/prod/llm_data/clean_data/Individual_vsAll_20231005160139.xlsx', 'Entity': '/var/app/pctlness/tensorflow/song/python/data/prod/llm_data/clean_data/Entity_vsAll_20231005160139.xlsx', 'Place_Location': '/var/app/pctlness/tensorflow/song/python/data/prod/llm_data/clean_data/Place_Location_vsAll_20231005160139.xlsx', 'Vessel_Aircraft': '/var/app/pctlness/tensorflow/song/python/data/prod/llm_data/clean_data/Vessel_Aircraft_vsAll_20231005160139.xlsx', 'Other_Input_Type': '/var/app/pctlness/tensorflow/song/python/data/prod/llm_data/clean_data/Other_Input_Type_vsAll_20231005160139.xlsx'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Extracted Data from DB Columns are ['HIT_ID', 'MATCH_LENGTH', 'SONG_LIST_TYPE', 'SONG_INPUT_TYPE', 'L2_MAKER_INPUTTYPE', 'L2_MAKER_LISTTYPE', 'INPUT_NAME', 'SONG_ACTION', 'SONG_NOTE', 'HMM_TOKENS', 'L2_MAKER_ACTION', 'L2_MAKER_NOTE'] starting in 01-JAN-2023.\n",
      "INFO: Extracted 551615 records from database for the specified month range.\n",
      "INFO: Extracted Data from DB Columns after renaming are ['hit_id', 'MATCH_LENGTH', 'song_list_type', 'song_input_type', 'L2_MAKER_INPUTTYPE', 'L2_MAKER_LISTTYPE', 'INPUT_NAME', 'SONG_ACTION', 'SONG_NOTE', 'TRIGRAM_TOKENS', 'L2_MAKER_ACTION', 'L2_MAKER_NOTE'].\n",
      "INFO: Extracted Data from DB Columns are ['HIT_ID', 'MATCH_LENGTH', 'SONG_LIST_TYPE', 'SONG_INPUT_TYPE', 'L2_MAKER_INPUTTYPE', 'L2_MAKER_LISTTYPE', 'INPUT_NAME', 'SONG_ACTION', 'SONG_NOTE', 'HMM_TOKENS', 'L2_MAKER_ACTION', 'L2_MAKER_NOTE'] starting in 01-FEB-2023.\n",
      "INFO: Extracted 522965 records from database for the specified month range.\n",
      "INFO: Extracted Data from DB Columns after renaming are ['hit_id', 'MATCH_LENGTH', 'song_list_type', 'song_input_type', 'L2_MAKER_INPUTTYPE', 'L2_MAKER_LISTTYPE', 'INPUT_NAME', 'SONG_ACTION', 'SONG_NOTE', 'TRIGRAM_TOKENS', 'L2_MAKER_ACTION', 'L2_MAKER_NOTE'].\n",
      "INFO: The extracted data for all 6 months contained: 1074580 records\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The extracted data for all 6 months contained: 1074580 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: The Joined data for all 6 months contained removing duplicates: 1074580 records\n",
      "INFO: Processing data for input type: Individual\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Joined data for all 6 months contained removing duplicates: 1074580 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Merged Data Columns after renaming are ['business_unit', 'match_tag', 'match_value', 'offset', 'match_text', 'match_pattern_value', 'list_name', 'l1_maker_listtype', 'l1_maker_inputtype', 'guideline_action', 'guideline_reason', 'hit_id', 'MATCH_LENGTH', 'song_list_type', 'song_input_type', 'L2_MAKER_INPUTTYPE', 'L2_MAKER_LISTTYPE', 'INPUT_NAME', 'SONG_ACTION', 'SONG_NOTE', 'TRIGRAM_TOKENS', 'L2_MAKER_ACTION', 'L2_MAKER_NOTE', 'start_date', 'end_date'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the total number of records so far: 854312\n",
      "the total number of records so far after dropping non floats in MATCH_LENGTH: 153337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Processing data for input type: Entity\n",
      "INFO: Merged Data Columns after renaming are ['business_unit', 'match_tag', 'match_value', 'offset', 'match_text', 'match_pattern_value', 'list_name', 'l1_maker_listtype', 'l1_maker_inputtype', 'guideline_action', 'guideline_reason', 'hit_id', 'MATCH_LENGTH', 'song_list_type', 'song_input_type', 'L2_MAKER_INPUTTYPE', 'L2_MAKER_LISTTYPE', 'INPUT_NAME', 'SONG_ACTION', 'SONG_NOTE', 'TRIGRAM_TOKENS', 'L2_MAKER_ACTION', 'L2_MAKER_NOTE', 'start_date', 'end_date'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the total number of records so far: 265296\n",
      "the total number of records so far after dropping non floats in MATCH_LENGTH: 49646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Processing data for input type: Place/Location\n",
      "INFO: Merged Data Columns after renaming are ['business_unit', 'match_tag', 'match_value', 'offset', 'match_text', 'match_pattern_value', 'list_name', 'l1_maker_listtype', 'l1_maker_inputtype', 'guideline_action', 'guideline_reason', 'hit_id', 'MATCH_LENGTH', 'song_list_type', 'song_input_type', 'L2_MAKER_INPUTTYPE', 'L2_MAKER_LISTTYPE', 'INPUT_NAME', 'SONG_ACTION', 'SONG_NOTE', 'TRIGRAM_TOKENS', 'L2_MAKER_ACTION', 'L2_MAKER_NOTE', 'start_date', 'end_date'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the total number of records so far: 364363\n",
      "the total number of records so far after dropping non floats in MATCH_LENGTH: 67588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Processing data for input type: Vessel/Aircraft\n",
      "INFO: Merged Data Columns after renaming are ['business_unit', 'match_tag', 'match_value', 'offset', 'match_text', 'match_pattern_value', 'list_name', 'l1_maker_listtype', 'l1_maker_inputtype', 'guideline_action', 'guideline_reason', 'hit_id', 'MATCH_LENGTH', 'song_list_type', 'song_input_type', 'L2_MAKER_INPUTTYPE', 'L2_MAKER_LISTTYPE', 'INPUT_NAME', 'SONG_ACTION', 'SONG_NOTE', 'TRIGRAM_TOKENS', 'L2_MAKER_ACTION', 'L2_MAKER_NOTE', 'start_date', 'end_date'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the total number of records so far: 12074\n",
      "the total number of records so far after dropping non floats in MATCH_LENGTH: 3048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Processing data for input type: Other Input Type\n",
      "INFO: Merged Data Columns after renaming are ['business_unit', 'match_tag', 'match_value', 'offset', 'match_text', 'match_pattern_value', 'list_name', 'l1_maker_listtype', 'l1_maker_inputtype', 'guideline_action', 'guideline_reason', 'hit_id', 'MATCH_LENGTH', 'song_list_type', 'song_input_type', 'L2_MAKER_INPUTTYPE', 'L2_MAKER_LISTTYPE', 'INPUT_NAME', 'SONG_ACTION', 'SONG_NOTE', 'TRIGRAM_TOKENS', 'L2_MAKER_ACTION', 'L2_MAKER_NOTE', 'start_date', 'end_date'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the total number of records so far: 103619\n",
      "the total number of records so far after dropping non floats in MATCH_LENGTH: 26652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Script run duration: 22.70 minutes\n",
      "INFO: Script completed successfully!\n"
     ]
    }
   ],
   "source": [
    "%run nam_end2endProfiling_dedup_step_monthly_adding_word.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save labeling utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting nam_label_functions_all_entities.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile nam_label_functions_all_entities.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import pandas as pd\n",
    "import jellyfish\n",
    "import numpy as np\n",
    "import warnings\n",
    "import os\n",
    "import logging\n",
    "from config import Config\n",
    "import time\n",
    "\n",
    "# Constants\n",
    "FOLDER_PATH = Config.FOLDER_PATH\n",
    "INPUT_FILE = os.path.join(FOLDER_PATH, 'final_data_deduplicated_Individual.xlsx')\n",
    "OUTPUT_FILE = os.path.join(FOLDER_PATH, 'processed_data_with_multiclass_Individual.xlsx')\n",
    "salutations = ['mr', 'miss', 'dr', 'md', 'mohd', 'sheikh', 'ms', 'shaikh', 'shaik', 'late', 'shri', 'sri', 'merchant', 'mrs', 'master', 'junior']\n",
    "commonname = [\"mohammad\", \"mohammed\", \"muhammad\", \"muhammed\", \"mohd\", \"mhmd\"]\n",
    "\n",
    "# Logging Setup\n",
    "\n",
    "def setup_logging(log_level, log_format):\n",
    "    logging.basicConfig(level=log_level, format=log_format, handlers=[\n",
    "        logging.FileHandler(\"debug.log\"),  # Save logs to this file\n",
    "        logging.StreamHandler()  # And print logs to terminal\n",
    "    ])\n",
    "    logging.info('Logging setup complete.')\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "\n",
    "def strip_salutation_from_text(text):\n",
    "    text_split=text.split()\n",
    "    for val in salutations:\n",
    "        if val in text_split:\n",
    "            text_split.remove(val)\n",
    "    output=\" \".join(x for x in text_split)\n",
    "    output=output.strip()\n",
    "    return output\n",
    "\n",
    "def strip_commoname_from_text(text):\n",
    "    text_split=text.split()\n",
    "    for val in text_split:\n",
    "        if val in commonname:\n",
    "            text_split.remove(val)\n",
    "    text=\" \".join(x for x in text_split)\n",
    "    text=text.strip()\n",
    "    return text\n",
    "\n",
    "def strip_singleletter_from_text(text):\n",
    "    text_split=text.split()\n",
    "    single_letters=[]\n",
    "    for val in text_split:\n",
    "        if len(val) ==1:\n",
    "            single_letters.append(val)\n",
    "    cleaned_name=\" \".join(set(text_split)-set(single_letters))\n",
    "    cleaned_name=cleaned_name.strip()\n",
    "    return cleaned_name\n",
    "\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "\n",
    "def get_multi_class(inputname, listname):\n",
    "    try:\n",
    "        inputname=str(inputname).lower()\n",
    "        listname=str(listname).lower()\n",
    "        inputname=strip_salutation_from_text(inputname)\n",
    "        f1=Monge_Levenshtein(inputname,listname)\n",
    "        f2=phonetics_compute(inputname,listname)\n",
    "        input_length=len(inputname.split())\n",
    "        list_length=len(listname.split())\n",
    "        # name cleaning for additional name component scenario\n",
    "        add_input_name=add_name_cleaning(inputname,listname)\n",
    "        add_input_length=len(add_input_name.split())\n",
    "        #print(add_input_name)\n",
    "        #print(f1,f2)\n",
    "        \n",
    "        # POTENTIAL CASES\n",
    "\n",
    "        #1 Exact Match\n",
    "        if f1 ==1:\n",
    "            return \"Potential Match: Exact Name Match Scenario\"\n",
    "\n",
    "        #2 Phonetic Match\n",
    "        elif f2==1:\n",
    "            #print(True)\n",
    "            return \"Potential Match: Phonetic Name Match Scenario\"\n",
    "\n",
    "        #3 Initial Case\n",
    "        elif is_initials_feature(inputname, listname):\n",
    "            return \"Potential Match: Initial Name Match Scenario\"\n",
    "            \n",
    "        #4 Special Additional Name Case Handling\n",
    "        elif (add_input_length==list_length) and input_length!=list_length:\n",
    "            f1=Monge_Levenshtein(add_input_name,listname)\n",
    "            f2=phonetics_compute(add_input_name,listname)\n",
    "            \n",
    "            if f1==1:\n",
    "                return \"Potential Match: Exact Name Match Scenario\"\n",
    "            elif f2==1:\n",
    "                #print(True)\n",
    "                return \"Potential Match: Phonetic Name Match Scenario\"\n",
    "            else:\n",
    "                return \"False Match: Full Name Mismatch Scenario\"\n",
    "        \n",
    "        #5 single letter case handling\n",
    "        elif (input_length==list_length) and find_single_letter(inputname):\n",
    "            input_name_split=inputname.split()\n",
    "            single_letter_count=[x for x in input_name_split if len(x)==1]\n",
    "            if len(single_letter_count)==1:\n",
    "                cleaned_name=[x for x in input_name_split if len(x)!=1]\n",
    "            else:\n",
    "                cleaned_name=inputname\n",
    "            #print(cleaned_name)\n",
    "            cleaned_name_value=\" \".join(x for x in cleaned_name)\n",
    "            cleaned_name_value=cleaned_name_value.strip()\n",
    "            f1=Monge_Levenshtein(cleaned_name_value, listname)\n",
    "            f2=phonetics_compute(cleaned_name_value, listname)\n",
    "            if f1==1:\n",
    "                return \"Potential Match: Exact Name Match Scenario\"\n",
    "            elif f2==1:\n",
    "                return \"Potential Match: Phonetic Name Match Scenario\"\n",
    "            else:\n",
    "                return \"False Match: Full Name Mismatch Scenario\"\n",
    "            \n",
    "        #6 commonname case handling\n",
    "        elif (input_length > list_length):\n",
    "            cleaned_name=strip_commoname_from_text(inputname)\n",
    "            cleaned_name_length=len(cleaned_name.split())\n",
    "            f1=Monge_Levenshtein(cleaned_name,listname)\n",
    "            f2=phonetics_compute(cleaned_name,listname)\n",
    "            \n",
    "            if f1==1:\n",
    "                return \"Potential Match: Exact Name Match Scenario\"\n",
    "            elif f2==1:\n",
    "                return \"Potential Match: Phonetic Name Match Scenario\"\n",
    "            else:\n",
    "                if cleaned_name_length > list_length:\n",
    "                    return \"False Match: Additional Name Component Scenario\"\n",
    "                else:\n",
    "                    return \"False Match: Full Name Mismatch Scenario\"                   \n",
    "                \n",
    "        \n",
    "        #FALSE CASES\n",
    "\n",
    "        #1 different name scenarios\n",
    "        elif (input_length==list_length) and (f1!=1 and f2!=1):\n",
    "            return \"False Match: Full Name Mismatch Scenario\"\n",
    "        elif (list_length>input_length) and (f1!=1 and f2!=1):\n",
    "            return \"False Match: Full Name Mismatch Scenario\"\n",
    "\n",
    "        #2 Additional name component\n",
    "        \n",
    "        elif (add_input_length> list_length) and (f1!=1 and f2!=1):\n",
    "            return \"False Match: Additional Name Component Scenario\"\n",
    "        \n",
    "    except:\n",
    "        raise\n",
    "        #return \"Error\"\n",
    "\n",
    "def phonetics_compute(inputname, listname):\n",
    "    input_soundex=[]\n",
    "    list_soundex=[]    \n",
    "    input_metaphone=[]\n",
    "    list_metaphone=[]\n",
    "    input_nyiis=[]\n",
    "    list_nyiis=[]\n",
    "    inputname=strip_singleletter_from_text(inputname)\n",
    "    input_split=inputname.split()\n",
    "    list_split=listname.split()\n",
    "    \n",
    "    for val in input_split:\n",
    "        input_soundex.append(jellyfish.soundex(val))\n",
    "        input_metaphone.append(jellyfish.metaphone(val))\n",
    "        input_nyiis.append(jellyfish.nysiis(val))\n",
    "    for val in list_split:\n",
    "        list_soundex.append(jellyfish.soundex(val))\n",
    "        list_metaphone.append(jellyfish.metaphone(val))\n",
    "        list_nyiis.append(jellyfish.nysiis(val))\n",
    "        \n",
    "    input_soundex_value=\" \".join(x for x in input_soundex)\n",
    "    list_soundex_value=\" \".join(x for x in list_soundex)\n",
    "    input_metaphone_value=\" \".join(x for x in input_metaphone)\n",
    "    list_metaphone_value=\" \".join(x for x in list_metaphone)\n",
    "    input_nyiis_value=\" \".join(x for x in input_nyiis)\n",
    "    list_nyiis_value=\" \".join(x for x in list_nyiis)\n",
    "    \n",
    "   # print(input_soundex_value, \":\", list_soundex_value, \"\\n\",input_metaphone_value,\":\", list_metaphone_value)\n",
    "    soundex=Monge_Levenshtein(input_soundex_value,list_soundex_value)\n",
    "    metaphone=Monge_Levenshtein(input_metaphone_value,list_metaphone_value)\n",
    "    nyiis=Monge_Levenshtein(input_nyiis_value,list_nyiis_value)\n",
    "    #print(soundex, metaphone,nyiis)\n",
    "    #vote=[]\n",
    "    #if soundex ==1:\n",
    "        #vote.append(True)\n",
    "    #if metaphone==1:\n",
    "        #vote.append(True)\n",
    "    #if nyiis==1:\n",
    "        #vote.append(True)\n",
    "    \n",
    "    #print(vote)\n",
    "    \n",
    "    #vote_score=len(vote)/3\n",
    "    #print(vote_score)\n",
    "    #if vote_score>0.5:\n",
    "        #return 1\n",
    "        \n",
    "    if soundex==1 and metaphone==1 and nyiis==1:\n",
    "        return 1\n",
    "    else:\n",
    "        return min(soundex, metaphone,nyiis)\n",
    "\n",
    "    \n",
    "def add_name_cleaning(inputname, listname):   \n",
    "    inputname=str(inputname).lower()\n",
    "    listname=str(listname).lower()\n",
    "    inputname=strip_salutation_from_text(inputname)\n",
    "    inputname_split=inputname.split()\n",
    "    listname_split=listname.split()\n",
    "    \n",
    "    refined_text=[]\n",
    "    single_token_count=[x for x in inputname_split if len(x)==1]\n",
    "\n",
    "    if len(single_token_count)==1:\n",
    "        for val in inputname_split:\n",
    "            if len(val)!=1:\n",
    "                refined_text.append(val)\n",
    "\n",
    "        if len(refined_text)==len(listname_split):\n",
    "            output=\" \".join(x for x in refined_text)\n",
    "        else:\n",
    "            output=inputname\n",
    "        return output\n",
    "    \n",
    "    return inputname\n",
    "\n",
    "\n",
    "\n",
    "# Initials Function\n",
    "def is_initials_feature(inputname, listname):\n",
    "    inputname=str(inputname).lower()\n",
    "    listname=str(listname).lower()\n",
    "    input_split=inputname.split()\n",
    "    list_split=listname.split()\n",
    "    if find_single_letter(inputname):\n",
    "        #print(True)\n",
    "        output=[]  \n",
    "        \n",
    "        for val in input_split:\n",
    "            for i in list_split:\n",
    "                if len(val)==1:\n",
    "                    if i[0] ==val:\n",
    "                        #print(val)\n",
    "                        output.append(True)\n",
    "                        break\n",
    "                elif len(val)>1:\n",
    "                    if dice_bi_coefficient(val,i)==1:\n",
    "                        #print('dice',val)\n",
    "                        output.append(True)\n",
    "                        break\n",
    "        #print(output)\n",
    "        if len(output)==len(input_split):\n",
    "            return True\n",
    "\n",
    "def levenshtein(seq1, seq2):\n",
    "    size_x = len(seq1) + 1\n",
    "    size_y = len(seq2) + 1\n",
    "    matrix = np.zeros ((size_x, size_y))\n",
    "    for x in range(size_x):\n",
    "        matrix [x, 0] = x\n",
    "    for y in range(size_y):\n",
    "        matrix [0, y] = y\n",
    "\n",
    "    for x in range(1, size_x):\n",
    "        for y in range(1, size_y):\n",
    "            if seq1[x-1] == seq2[y-1]:\n",
    "                matrix [x,y] = min(\n",
    "                        matrix[x-1, y] + 1,\n",
    "                        matrix[x-1, y-1],\n",
    "                        matrix[x, y-1] + 1\n",
    "                    )\n",
    "            else:\n",
    "                matrix [x,y] = min(\n",
    "                        matrix[x-1,y] + 1,\n",
    "                        matrix[x-1,y-1] + 1,\n",
    "                        matrix[x,y-1] + 1\n",
    "                    )\n",
    "\n",
    "    return 1-(matrix[size_x - 1, size_y - 1]/len(seq1))\n",
    "    \n",
    "\n",
    "#Hybrid Feature\n",
    "def Monge_Levenshtein(s1,s2):\n",
    "    s1=strip_singleletter_from_text(s1)\n",
    "    s1=s1.strip()\n",
    "    s1_split=s1.split()\n",
    "    s2_split=s2.split()\n",
    "    #if len(s1_split)==1 and len(s2_split)!=1:\n",
    "        #return 0\n",
    "    #else:    \n",
    "    cummax=0\n",
    "    for ws in s1.split(\" \"):\n",
    "        maxscore=0\n",
    "        for wt in s2.split(\" \"):\n",
    "            maxscore = max(maxscore,levenshtein(ws,wt))\n",
    "        cummax += maxscore\n",
    "\n",
    "    return cummax/len(s1.split(\" \"))\n",
    "        \n",
    "        \n",
    "def single_named_component_inference(inputname,listname):\n",
    "    x1=inputname.split()\n",
    "    x2=listname.split()\n",
    "    #input is single, list is more than 1\n",
    "    if len(x1)==1 and len(x2)!=1:\n",
    "        return 'False Match: Single Input Component: False Scenario'\n",
    "    #both input & list are single components\n",
    "    elif len(x2)==1 and len(x1)>1:\n",
    "        f1=Monge_Levenshtein(listname,inputname)\n",
    "        f2=spelling_sound(listname,inputname)\n",
    "        if f1==1 or f2==1:\n",
    "            return 'Single List Component: Likely Escalation'\n",
    "# Spelling & Sound Feature\n",
    "def spelling_sound(inputname, listname):\n",
    "\n",
    "    try:\n",
    "        #salutations=['mr','miss','dr','md','ms','shaikh','shaik']\n",
    "        for val in salutations:\n",
    "            inputname=inputname.replace(val,'')\n",
    "\n",
    "        x1=inputname.strip()\n",
    "        #print(x1)\n",
    "        x1=x1.split()\n",
    "        x2=listname.split()\n",
    "\n",
    "\n",
    "        #if len(x1)==1 and len(x2)>1:\n",
    "            #return 0\n",
    "        #elif len(x1)==1 and len(x2)==1:\n",
    "            #return 1    \n",
    "        #else:    \n",
    "        output=[]\n",
    "        for i in x1:\n",
    "            for j in x2:\n",
    "                try:\n",
    "                    match_soundex=jellyfish.soundex(i)\n",
    "                    list_soundex=jellyfish.soundex(j)\n",
    "                except:\n",
    "                    match_soundex=''\n",
    "                    list_soundex=''\n",
    "\n",
    "            match_metaphone=jellyfish.metaphone(i)\n",
    "            list_metaphone=jellyfish.metaphone(j)\n",
    "\n",
    "            if Monge_Levenshtein(match_metaphone,list_metaphone)==1 or Monge_Levenshtein(match_soundex,list_soundex)==1:\n",
    "                output.append(True)\n",
    "                break\n",
    "            else:\n",
    "                output.append(False)\n",
    "        true_output=[x for x in output if x==True]\n",
    "        if len(true_output)>=len(x1):\n",
    "            result=1\n",
    "        else:\n",
    "            result=phoentic_similarity(inputname, listname)\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "    return result\n",
    "        \n",
    "        \n",
    "#Phoetics Function\n",
    "def phoentic_similarity(inputname,listname):\n",
    "    x1=inputname.split()\n",
    "    x2=listname.split()\n",
    "\n",
    "    phonetics_input_soundex_code=''\n",
    "    phoentics_list_soundex_code=''\n",
    "    phonetics_input_metaphone_code=''\n",
    "    phoentics_list_metaphone_code=''\n",
    "\n",
    "    for i in x1:\n",
    "        try:       \n",
    "            phonetics_input_soundex_code+=' '+ jellyfish.soundex(i)\n",
    "        except: \n",
    "            phonetics_input_soundex_code=''\n",
    "        if i not in ['w','y']:\n",
    "            phonetics_input_metaphone_code+=' '+ jellyfish.metaphone(i)\n",
    "        else:\n",
    "            phonetics_input_metaphone_code+=' '+ i \n",
    "\n",
    "    for j in x2:\n",
    "        try:       \n",
    "            phoentics_list_soundex_code+=' '+ jellyfish.soundex(i)\n",
    "        except: \n",
    "            phoentics_list_soundex_code=''\n",
    "        if j not in ['w','y']:\n",
    "            phoentics_list_metaphone_code+=' '+ jellyfish.metaphone(j)\n",
    "        else:\n",
    "            phoentics_list_metaphone_code+=' '+ j \n",
    "\n",
    "    phonetics_input_soundex_code=phonetics_input_soundex_code.strip()\n",
    "    phoentics_list_soundex_code=phoentics_list_soundex_code.strip()\n",
    "    phonetics_input_metaphone_code=phonetics_input_metaphone_code.strip()\n",
    "    phoentics_list_metaphone_code=phoentics_list_metaphone_code.strip()\n",
    "\n",
    "    phonetics_input_soundex_code=' '.join(phonetics_input_soundex_code.split())\n",
    "    phoentics_list_soundex_code=' '.join(phoentics_list_soundex_code.split())\n",
    "    phonetics_input_metaphone_code=' '.join(phonetics_input_metaphone_code.split())\n",
    "    phoentics_list_metaphone_code=' '.join(phoentics_list_metaphone_code.split())\n",
    "\n",
    "    soundex_similarity= Monge_Levenshtein(phonetics_input_soundex_code,phoentics_list_soundex_code)\n",
    "    metaphone_similarity= Monge_Levenshtein(phonetics_input_metaphone_code,phoentics_list_metaphone_code)\n",
    "\n",
    "    return max(soundex_similarity,metaphone_similarity)\n",
    "\n",
    "# Gram Feature\n",
    "def dice_bi_coefficient(a, b):\n",
    "    a=a.lower()\n",
    "    a=a.strip()\n",
    "    a=' '.join(a.split())\n",
    "    b=b.lower()\n",
    "    b=b.strip()\n",
    "    b=' '.join(b.split())\n",
    "    \"\"\"dice coefficient 2nt/(na + nb).\"\"\"\n",
    "    if not len(a) or not len(b): return 0.0\n",
    "    if len(a) == 1:  a=a+u'.'\n",
    "    if len(b) == 1:  b=b+u'.'\n",
    "\n",
    "    a_bigram_list=[]\n",
    "    a_bigram_list.append(' '+a[0])\n",
    "    j=len(a)-1\n",
    "    a_bigram_list.append(a[j]+' ')\n",
    "\n",
    "    b_bigram_list=[]\n",
    "    b_bigram_list.append(' '+b[0])\n",
    "    k=len(b)-1\n",
    "    b_bigram_list.append(a[k:]+' ')\n",
    "\n",
    "    for i in range(len(a)-1):\n",
    "        a_bigram_list.append(a[i:i+2])\n",
    "\n",
    "    for i in range(len(b)-1):\n",
    "        b_bigram_list.append(b[i:i+2])\n",
    "\n",
    "    a_bigrams = set(a_bigram_list)\n",
    "    b_bigrams = set(b_bigram_list)\n",
    "    overlap = len(a_bigrams & b_bigrams)\n",
    "    dice_coeff = overlap * 2.0/(len(a_bigrams) + len(b_bigrams))\n",
    "    return dice_coeff\n",
    "\n",
    "# Initials Function\n",
    "def is_initials_feature(inputname, listname):\n",
    "    inputname=str(inputname).lower()\n",
    "    listname=str(listname).lower()\n",
    "    input_split=inputname.split()\n",
    "    list_split=listname.split()\n",
    "    if find_single_letter(inputname):\n",
    "        #print(True)\n",
    "        output=[]  \n",
    "        \n",
    "        for val in input_split:\n",
    "            for i in list_split:\n",
    "                if len(val)==1:\n",
    "                    if i[0] ==val:\n",
    "                        output.append(True)\n",
    "                        break\n",
    "                elif len(val)>1:\n",
    "                    if dice_bi_coefficient(val,i)==1:\n",
    "                        output.append(True)\n",
    "        #print(output)\n",
    "        if len(output)==len(input_split):\n",
    "            return True\n",
    "    \n",
    "def find_single_letter(text):\n",
    "    text_split=text.split()\n",
    "    \n",
    "    for val in text_split:\n",
    "        if len(val)==1:\n",
    "            return True\n",
    "\n",
    "\n",
    "\n",
    "def matches_words_sounds(list_name, input_name):\n",
    "    listns=sorted(list_name.lower().split())\n",
    "    inputns=sorted(input_name.lower().split())\n",
    "    list_len=len(listns)\n",
    "    input_len=len(inputns)\n",
    "    if list_len==input_len:\n",
    "        if listns==inputns:\n",
    "            return 'Escalated Match','Same'\n",
    "        elif sorted([jf.soundex(x1) for x1 in listns])==sorted([jf.soundex(x2) for x2 in inputns]):\n",
    "            return 'Escalated Match','Soundex'\n",
    "        elif sorted([jf.metaphone(x1) for x1 in listns])==sorted([jf.metaphone(x2) for x2 in inputns]):\n",
    "            return 'Escalated Match','Metaphone'\n",
    "    return 'False Match','Else'\n",
    "\n",
    "def l2_action(list_name, input_name, maker_action, note):\n",
    "    reasons=['DOB','Google','External','Full Name','Additional']\n",
    "    try:\n",
    "        if maker_action=='':\n",
    "            return '',''\n",
    "        elif maker_action!='False Match':\n",
    "            return 'Escalated Match','Maker'\n",
    "        note=str(note).lower()\n",
    "        \n",
    "        if any(val in note for val in dobs):\n",
    "            #res=matches_words_sounds(list_name, input_name)\n",
    "            return 'Escalated Match','DOB'+'_'+res[1]\n",
    "        elif 'google' in note:\n",
    "            res=matches_words_sounds(list_name, input_name)\n",
    "            return 'Escalated Match','Google'+'_'+res[1]\n",
    "        elif 'external' in note:\n",
    "            res=matches_words_sounds(list_name, input_name)\n",
    "            return 'Escalated Match','External'+'_'+res[1]\n",
    "        elif 'https' in note:\n",
    "            res=matches_words_sounds(list_name, input_name)\n",
    "            return 'Escalated Match','External'+'_'+res[1]\n",
    "        elif 'full name mismatch' in note:\n",
    "            res=matches_words_sounds(list_name, input_name)\n",
    "            return res[0],'Full Name'+'_'+res[1]\n",
    "        elif 'not an exact match' in note:\n",
    "            res=matches_words_sounds(list_name, input_name)\n",
    "            return res[0],'Full Name'+'_'+res[1]\n",
    "        elif 'input name has additional name components' in note:\n",
    "            res=matches_words_sounds(list_name, input_name)\n",
    "            return res[0],'Additional'+'_'+res[1]\n",
    "        \n",
    "        else:\n",
    "            return matches_words_sounds(list_name, input_name)\n",
    "    except:\n",
    "        return maker_action,'Exception'\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "skipped_records = 0\n",
    "\n",
    "def apply_labeling_function(df):\n",
    "    start_time = time.time()  # Starting the timer\n",
    "\n",
    "    df.rename(columns={'INPUT_NAME': 'InputName', 'list_name': 'ListName'}, inplace=True)\n",
    "    \n",
    "\n",
    "    global skipped_records\n",
    "\n",
    "    def apply_function(row):\n",
    "        try:\n",
    "            return get_multi_class(row['InputName'], row['ListName'])\n",
    "        except UnicodeDecodeError:\n",
    "            global skipped_records\n",
    "            skipped_records += 1\n",
    "            return \"Error\"\n",
    "\n",
    "    df['label_function_output'] = df.apply(apply_function, axis=1)\n",
    "\n",
    "    df.drop_duplicates(inplace=True)\n",
    "\n",
    "    elapsed_time = time.time() - start_time  # Calculating the elapsed time\n",
    "\n",
    "    logger.info(f\"Processed {len(df)} records in {elapsed_time} seconds.\")\n",
    "    logger.info(\"Process completed.\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create labeling column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from config_dedup import Config\n",
    "from nam_label_functions_all_entities import apply_labeling_function\n",
    "import warnings\n",
    "import random\n",
    "import numpy as np\n",
    "import sys\n",
    "import cx_Oracle\n",
    "import logging\n",
    "import time\n",
    "from config_dedup import Config\n",
    "# Suppress warnings and set seeds\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ[\"KMP_WARNINGS\"] = \"FALSE\"\n",
    "random_seed = 23\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "\n",
    "# Paths for CS Constants\n",
    "sys.path.append(\"/var/app/pctlness/tensorflow/song/python/process/common/\")\n",
    "sys.path.append(\"/var/app/pctlness/tensorflow/song/unix/bin/\")\n",
    "\n",
    "from CSConstants import * \n",
    "sys.path.append(CONTEXT_INPUT_METADATA)\n",
    "sys.path.append(CONTEXT_INPUT_MODEL)\n",
    "sys.path.append(CONTEXT_INPUT_PROCESS)\n",
    "from TXInputTypeProcess import *\n",
    "\n",
    "import pandas as pd\n",
    "import csv\n",
    "import cx_Oracle\n",
    "\n",
    "# Save a large DataFrame to multiple sheets in an Excel file (ADD ME TO HELPER FUNCTIONS)\n",
    "def save_to_multiple_sheets(df, output_filename, split_size=1_000_000):\n",
    "    with pd.ExcelWriter(output_filename, engine='openpyxl') as writer:\n",
    "        for i, start in enumerate(range(0, len(df), split_size)):\n",
    "            df.iloc[start:start+split_size].to_excel(writer, sheet_name=f\"Sheet_{i+1}\", index=False)\n",
    "\n",
    "#input_types = ['Individual', 'Entity', 'Place_Location', 'Vessel_Aircraft', 'Other_Input_Type']\n",
    "input_types = ['Other_Input_Type']\n",
    "\n",
    "for input_type in input_types:\n",
    "    file_path = os.path.join(Config.RESULTS_DIR, f\"final_data_deduplicated_monthly_{input_type}.xlsx\")\n",
    "    df = pd.read_excel(file_path, engine='openpyxl')\n",
    "    \n",
    "\n",
    "    \n",
    "    # Apply the labeling function    \n",
    "    df = apply_labeling_function(df)\n",
    "    df['TRIGRAM_TOKENS'] = df['match_value'].apply(lambda value: trigramAddrTokens(value,True)[0])\n",
    "\n",
    "    # Save the labeled DataFrame\n",
    "    # df.to_excel(file_path, index=False, engine='openpyxl')\n",
    "    # Save the processed data\n",
    "    save_to_multiple_sheets(df, os.path.join(Config.RESULTS_DIR, f\"final_data_labeled_{input_type.replace('/', '_')}.xlsx\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/var/app/pctlness/tensorflow/song/individual/gb54464'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data = '/var/app/pctlness/tensorflow/song/python/data/prod/llm_data/clean_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "oit = pd.read_excel(cleaned_data + \"final_data_deduplicated_monthly_Other_Input_Type.xlsx\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mm desk internal sweeping;mm desk internal 2;mm desk internal 2;internal only                                                   21\n",
       "/ref/ 2000025526 icari153605 icari153554 icari153513 icari153348 icari153139 icari152812 icari151707                            14\n",
       "/ref/ 2000026095 icarc112325 icari156538 icari156450 icari156239 icari155909 icari155860 icari155690                            13\n",
       "ihu2209812,ihu2209813,ihu2209814,ih;u2209815,ihu2209816,ihu2209879,ihu2;209880,ihu2209881,ihu2209938,ihu220;9939,ihu2209940,    11\n",
       "ihu2203385,ihu2209167,ihu2300043,ih;u2300045,ihu2300100,ihu2300115,ihu2;300116,ihu2300117,ihu2300118,ihu230;0141,ihu2300142,    11\n",
       "                                                                                                                                ..\n",
       "electric motion sensors import goods                                                                                             1\n",
       "name arvindpreet singh pp;u931447 5 dob 30 05 2003 ref no;fan369774 783 st id 1107817;acc/swift ad2tt23012300827                 1\n",
       "entac and artacil invoice neon01952223 branch code 4732                                                                          1\n",
       "/ins/eburgb2l lazar s trip agent margaret william s                                                                              1\n",
       "para acreditar en la cuenta no. 100 210026339cesar edgardo ravelo bueno pymt reason:personal expenses                            1\n",
       "Name: match_value, Length: 24301, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oit.match_value.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "oit['TRIGRAM_TOKENS'] = df['match_value'].apply(lambda value: trigramAddrTokens(value,True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PartyIdentifier OtherInfo OtherInfo PartyIdentifier                                                                                                                                                                                                        1146\n",
       "PartyIdentifier START                                                                                                                                                                                                                                       638\n",
       "OtherInfo OtherInfo Number OtherInfo OtherInfo Number Number OtherInfo OtherInfo OtherInfo Number PartyIdentifier CompanyName OtherInfo Number                                                                                                              497\n",
       "OtherInfo PartyIdentifier                                                                                                                                                                                                                                   226\n",
       "Unknown                                                                                                                                                                                                                                                     199\n",
       "                                                                                                                                                                                                                                                           ... \n",
       "LastName LastName OtherInfo OtherInfo Unknown Number OtherInfo Number OtherInfo OtherInfo PartyIdentifier Unknown OtherInfo Unknown PartyIdentifier OtherInfo OtherInfo OtherInfo                                                                             1\n",
       "OtherInfo OtherInfo LastName OtherInfo OtherInfo CompanyName CompanyName CompanyName CompanyName CompanyContext CompanyName CompanyName CompanyName OtherInfo Number                                                                                          1\n",
       "OtherInfo CompanyName CompanyContext OtherInfo OtherInfo Number CompanyName Number LastName LastName LastName OtherInfo OtherInfo Unknown Number OtherInfo OtherInfo OtherInfo OtherInfo OtherInfo                                                            1\n",
       "OtherInfo OtherInfo OtherInfo CompanyName CompanyName CompanyName CompanyName CompanyName CompanyName CompanyName CompanyName CompanySuffix OtherInfo OtherInfo OtherInfo OtherInfo OtherInfo OtherInfo CompanyName CompanyName CompanyName CompanyName       1\n",
       "CompanyName CompanyName Unknown OtherInfo CompanyName CompanyContext OtherInfo Number LastName LastName OtherInfo OtherInfo OtherInfo OtherInfo OtherInfo OtherInfo Unknown OtherInfo                                                                         1\n",
       "Name: TRIGRAM_TOKENS, Length: 14374, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oit.TRIGRAM_TOKENS.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create bucketization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "from config import Config\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "# Save a large DataFrame to multiple sheets in an Excel file\n",
    "def save_to_multiple_sheets(df, output_filename, split_size=1_000_000):\n",
    "    with pd.ExcelWriter(output_filename, engine='openpyxl') as writer:\n",
    "        for i, start in enumerate(range(0, len(df), split_size)):\n",
    "            df.iloc[start:start+split_size].to_excel(writer, sheet_name=f\"Sheet_{i+1}\", index=False)\n",
    "\n",
    "all_tokens_allowed = ['LastName', 'Number', 'Postal' ,'City', 'State', 'CompanyName', 'Unknown',\n",
    " 'StreetName', 'Street', 'POBOX' ,'AddressInfo' ,'Country','CountryCode',\n",
    " 'OtherInfo', 'PartyIdentifier', 'CompanySuffix','CompanyContext',\n",
    " 'HouseInfo', 'OfRelation', 'AddressWords' ,'StateCode', 'Salutation',\n",
    " 'Direction']\n",
    "\n",
    "\n",
    "def contains_address_token(trigram_string):\n",
    "    address_tokens = [\n",
    "        'AddressInfo', 'Addresswords', 'City', 'Country', 'CountryCode',\n",
    "        'Direction', 'HouseInfo', 'POBOX', 'State', 'StateCode', 'StreetName', 'Postal'\n",
    "    ]\n",
    "    return any(token in trigram_string for token in address_tokens)\n",
    "\n",
    "\n",
    "def contains_name_tag(trigram_tokens):\n",
    "    name_tags = ['FirstName', 'LastName', 'FullName', 'MiddleName', 'Alias', 'Salutation', 'PartyIdentifier']\n",
    "    return any(tag in trigram_tokens for tag in name_tags)\n",
    "\n",
    "def apply_final_mapping_individual(df):\n",
    "    df['guideline_reason'] = df['guideline_reason'].astype(str)\n",
    "    df['guideline_action'] = df['guideline_action'].astype(str)\n",
    "    df['final_input_type'] = df['final_input_type'].astype(str)\n",
    "    df['final_list_type'] = df['final_list_type'].astype(str)\n",
    "    # Initialize new columns\n",
    "    df['final_action'] = 'Review Needed'\n",
    "    df['final_reason'] = 'Review Needed'\n",
    "    \n",
    "    # Create a new column 'has_name'\n",
    "    df['has_name'] = df['TRIGRAM_TOKENS'].apply(contains_name_tag)\n",
    "    \n",
    "    # Initialize individual buckets\n",
    "    individual_buckets = {\n",
    "        'Entity': ('False Match', 'Cross Match between Input (Individual) and List (Entity)'),\n",
    "        'Vessel/Aircraft': ('False Match', 'Cross Match between Input (Individual) and List (Vessel/Aircraft)'),\n",
    "        'Place/Location': {\n",
    "            'input name is in a defined individual field and the list entry is a place or location': ('False Match', 'Input Name in a defined individual field and list entry is a place/location'),\n",
    "            'input name in field contains additional identifiers and has no relation with list name': ('False Match', 'Input Name contains additional identifiers and has no relation with list name'),\n",
    "            'input name is contained in a field that contains additional identifiers and text in the individual\\'s name is not an exact match to the sanctioned location referenced in the list entry': ('False Match', 'Name is contained in a field with additional identifiers, not an exact match to sanctioned location'),\n",
    "        },\n",
    "        'Individual': {\n",
    "            'input is a single name component and list entry is multi name component': ('False Match', 'Single Name component and list entry is multi name component'),\n",
    "            'input name has additional components not matching list entry': ('False Match', 'Input Name has additional components not matching list entry'),\n",
    "            'matching components are different and not possible variations': ('False Match', 'Full Name Mismatch'),\n",
    "            'potential match: escalate for further review': ('Potential Positive Match', 'Potential match: Escalate for further review')\n",
    "        },\n",
    "    }\n",
    "    \n",
    "    # Loop through each row in the DataFrame\n",
    "    for index, row in df.iterrows():\n",
    "        final_list_type = row['final_list_type']\n",
    "        guideline_reason = row['guideline_reason']\n",
    "        has_address = row.get('has_address', False)\n",
    "        \n",
    "        # Check if the list type has specific rules defined\n",
    "        if final_list_type in individual_buckets:\n",
    "            if final_list_type in ['Entity', 'Vessel/Aircraft']:\n",
    "                action, reason = individual_buckets[final_list_type]\n",
    "            else:\n",
    "                best_score = 0\n",
    "                best_action = None\n",
    "                best_reason = None\n",
    "                \n",
    "                for bucket, (action, reason) in individual_buckets[final_list_type].items():\n",
    "                    score = fuzz.partial_ratio(guideline_reason.lower(), bucket.lower())\n",
    "                    \n",
    "                    if score > best_score:\n",
    "                        best_score = score\n",
    "                        best_action = action\n",
    "                        best_reason = reason\n",
    "                \n",
    "                # Assuming a threshold score of 80 for a match\n",
    "                if best_score >= 80:\n",
    "                    action, reason = best_action, best_reason\n",
    "                else:\n",
    "                    action, reason = 'Review Needed', 'Review Needed'\n",
    "                \n",
    "                # Extra conditions for Place/Location type\n",
    "                if final_list_type == 'Place/Location':\n",
    "                    if best_reason == 'Name is contained in a field with additional identifiers, not an exact match to sanctioned location' and not has_address:\n",
    "                        action, reason = best_action, best_reason\n",
    "                    elif best_reason == 'Name in a defined individual field and list entry is a place/location' and row['has_name']:\n",
    "                        action, reason = best_action, best_reason\n",
    "                \n",
    "            df.at[index, 'final_action'] = action\n",
    "            df.at[index, 'final_reason'] = reason\n",
    "    \n",
    "    return df\n",
    "\n",
    "def keyword_match(text, keywords=['potential', 'escalate', 'escalated', 'further']):\n",
    "    return any(keyword in text.lower() for keyword in keywords)\n",
    "\n",
    "\n",
    "def apply_final_mapping_entity(df):\n",
    "    df['guideline_reason'] = df['guideline_reason'].astype(str)\n",
    "    df['guideline_action'] = df['guideline_action'].astype(str)\n",
    "    df['final_input_type'] = df['final_input_type'].astype(str)\n",
    "    df['final_list_type'] = df['final_list_type'].astype(str)\n",
    "\n",
    "    # Initialize new columns\n",
    "\n",
    "    df['final_action'] = 'Review Needed'\n",
    "    df['final_reason'] = 'Review Needed'\n",
    "\n",
    "    # Define the buckets for entity input type\n",
    "\n",
    "    entity_buckets = {\n",
    "\n",
    "        'Place/Location': ('Escalate', 'Place/Location requires escalation'),\n",
    "        'Vessel/Aircraft': ('False Match', 'Cross Match between Input (Entity) and List (Vessel/Aircraft)'),\n",
    "\n",
    "        'Entity': {\n",
    "\n",
    "            'matching components are different and not possible variations': ('False Match', 'Matching components are different'),\n",
    "            'components that are matching are considered common references and the input contains additional material name components that are not a reference to: place/location, corporate or legal identifiers': ('False Match', 'Common references but additional material name components'),\n",
    "            'input and list entry are both financial institutions and both the names of the financial institutions and first 4 characters of the swift bank identifier code (bic) do not match': ('False Match', 'Financial institutions with non-matching BIC'),\n",
    "            'input is not a financial institution and is matching against a list entry for a swift bank identifier code (bic) of a financial institution (list entry is not the name a financial institution)': ('False Match', 'Non-financial institution matching against financial institution BIC'),\n",
    "            'potential match: escalate for further review': ('Escalate', 'Potential match for further review')\n",
    "\n",
    "        },\n",
    "\n",
    "        'Individual': {\n",
    "            'input name (entity) is not a full name match to the list entry (individual), there is no nexus or connection between them and there are no other sanctions concerns': ('False Match', 'No nexus or connection between entity and individual'),\n",
    "            'potential match: escalate for further review': ('Escalate', 'Potential match for further review')\n",
    "        }\n",
    "\n",
    "    }\n",
    "    # Loop through each row in the DataFrame\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "\n",
    "        final_list_type = row['final_list_type']\n",
    "        guideline_reason = row['guideline_reason'].lower()\n",
    "\n",
    "        # Check if the list type has specific rules defined\n",
    "        if final_list_type in entity_buckets:\n",
    "            if final_list_type in ['Place/Location', 'Vessel/Aircraft']:\n",
    "                action, reason = entity_buckets[final_list_type]\n",
    "            else:\n",
    "                best_score = 0\n",
    "                best_action = None\n",
    "                best_reason = None\n",
    "\n",
    "                for bucket, (action, reason) in entity_buckets[final_list_type].items():\n",
    "                    score = fuzz.partial_ratio(guideline_reason, bucket.lower())\n",
    "                    if score > best_score:\n",
    "                        best_score = score\n",
    "                        best_action = action\n",
    "                        best_reason = reason\n",
    "\n",
    "                # Direct keyword match for \"Potential Positive Match\" scenarios\n",
    "\n",
    "                if keyword_match(guideline_reason):\n",
    "                    action, reason = 'Escalate', 'Potential match for further review'\n",
    "\n",
    "                # If the best_score is above a threshold (say, 80), then use the corresponding bucket\n",
    "                elif best_score >= 20:\n",
    "                    action, reason = best_action, best_reason\n",
    "                else:\n",
    "                    action, reason = 'Review Needed', 'Review Needed'\n",
    "\n",
    "            df.at[index, 'final_action'] = action\n",
    "            df.at[index, 'final_reason'] = reason\n",
    "    return df\n",
    "\n",
    "def apply_final_mapping_place_location(df):\n",
    "    \n",
    "    df['guideline_reason'] = df['guideline_reason'].astype(str)\n",
    "    df['guideline_action'] = df['guideline_action'].astype(str)\n",
    "    df['final_input_type'] = df['final_input_type'].astype(str)\n",
    "    df['final_list_type'] = df['final_list_type'].astype(str)\n",
    "    \n",
    "    # Initialize final_action and final_reason columns\n",
    "    df['final_action'] = 'Review Needed'\n",
    "    df['final_reason'] = 'Review Needed'\n",
    "    \n",
    "    # Cross Match scenarios\n",
    "    condition_entity_cross_match = (df['final_list_type'] == 'Entity')\n",
    "    condition_individual_cross_match = (df['final_list_type'] == 'Individual')\n",
    "    condition_vessel_aircraft_cross_match = (df['final_list_type'] == 'Vessel/Aircraft')\n",
    "    \n",
    "    df.loc[condition_entity_cross_match, 'final_reason'] = 'Cross Match between Input (Place/Location) and List (Entity)'\n",
    "    df.loc[condition_entity_cross_match, 'final_action'] = 'False Match'\n",
    "    \n",
    "    df.loc[condition_individual_cross_match, 'final_reason'] = 'Cross Match between Input (Place/Location) and List (Individual)'\n",
    "    df.loc[condition_individual_cross_match, 'final_action'] = 'False Match'\n",
    "    \n",
    "    df.loc[condition_vessel_aircraft_cross_match, 'final_reason'] = 'Cross Match between Input (Place/Location) and List (Vessel/Aircraft)'\n",
    "    df.loc[condition_vessel_aircraft_cross_match, 'final_action'] = 'False Match'\n",
    "\n",
    "    # Place/Location specific logic\n",
    "    for idx, row in df[df['final_list_type'] == 'Place/Location'].iterrows():\n",
    "        guideline_reason = row['guideline_reason']\n",
    "        \n",
    "        if fuzz.partial_ratio('valid location that is not the same location as referenced in the list entry nor otherwise in a sanctioned jurisdiction', guideline_reason) >= 80:\n",
    "            df.at[idx, 'final_action'] = 'False Match'\n",
    "            df.at[idx, 'final_reason'] = 'input address is a valid location that is not the same location as referenced in the list entry nor otherwise in a sanctioned jurisdiction'\n",
    "        \n",
    "    # Potential Positive Match scenarios\n",
    "    potential_keywords = ['potential', 'escalated', 'verify', 'verification', 'further', 'escalate']\n",
    "    condition_potential = df['guideline_reason'].str.lower().apply(lambda x: any(keyword in x for keyword in potential_keywords))\n",
    "\n",
    "    df.loc[condition_potential, 'final_action'] = 'Potential Positive Match'\n",
    "    df.loc[condition_potential, 'final_reason'] = 'Potential Match: Escalated for further review'\n",
    "            \n",
    "    return df\n",
    "\n",
    "def apply_final_mapping_vessel_aircraft(df):\n",
    "    \n",
    "    df['guideline_reason'] = df['guideline_reason'].astype(str)\n",
    "    df['guideline_action'] = df['guideline_action'].astype(str)\n",
    "    df['final_input_type'] = df['final_input_type'].astype(str)\n",
    "    df['final_list_type'] = df['final_list_type'].astype(str)\n",
    "    \n",
    "    # Initialize final_action and final_reason columns\n",
    "    df['final_action'] = 'Review Needed'\n",
    "    df['final_reason'] = 'Review Needed'\n",
    "    \n",
    "    # Cross Match with Individual\n",
    "    condition_individual_cross_match = (df['final_list_type'] == 'Individual')\n",
    "    df.loc[condition_individual_cross_match, 'final_reason'] = 'Cross Match between Input (Vessel/Aircraft) and List (Individual)'\n",
    "    df.loc[condition_individual_cross_match, 'final_action'] = 'False Match'\n",
    "    \n",
    "    # List Type: Place/Location - Escalate\n",
    "    condition_place_location = (df['final_list_type'] == 'Place/Location')\n",
    "    df.loc[condition_place_location, 'final_reason'] = 'Escalate for further review'\n",
    "    df.loc[condition_place_location, 'final_action'] = 'Potential Positive Match'\n",
    "    \n",
    "    # Placeholder for Phase 2 Entity and Vessel/Aircraft logic\n",
    "    # Uncomment the following lines when ready for Phase 2\n",
    "    \n",
    "    # condition_entity_phase2 = (df['final_list_type'] == 'Entity')\n",
    "    # df.loc[condition_entity_phase2, 'final_reason'] = 'Phase 2: Review Needed'\n",
    "    # df.loc[condition_entity_phase2, 'final_action'] = 'Review Needed'\n",
    "    \n",
    "    # condition_vessel_aircraft_phase2 = (df['final_list_type'] == 'Vessel/Aircraft')\n",
    "    # df.loc[condition_vessel_aircraft_phase2, 'final_reason'] = 'Phase 2: Review Needed'\n",
    "    # df.loc[condition_vessel_aircraft_phase2, 'final_action'] = 'Review Needed'\n",
    "    \n",
    "    return df\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "def keyword_match(text, keywords=['potential', 'escalate', 'escalated', 'further']):\n",
    "    return any(keyword in text.lower() for keyword in keywords)\n",
    "\n",
    "def apply_final_mapping_other_input_type(df):\n",
    "   # Type casting\n",
    "    df['guideline_reason'] = df['guideline_reason'].astype(str)\n",
    "    df['guideline_action'] = df['guideline_action'].astype(str)\n",
    "    df['final_input_type'] = df['final_input_type'].astype(str)\n",
    "    df['final_list_type'] = df['final_list_type'].astype(str)\n",
    "\n",
    "   # Initialize new columns\n",
    "    df['final_action'] = 'Review Needed'\n",
    "    df['final_reason'] = 'Review Needed'\n",
    "\n",
    "   # Create buckets for Other Input Type\n",
    "    other_input_type_buckets = {\n",
    "       'Individual': {},\n",
    "       'Entity': {},\n",
    "       'Place/Location': {},\n",
    "       'Vessel/Aircraft': {},\n",
    "   }\n",
    "\n",
    "   # Common rules\n",
    "    common_rules = {\n",
    "       'exclude hit': ('Need Review', 'Need Review'),\n",
    "       'pertains to child school payment': ('Need Review', 'Need Review'),\n",
    "       'client bhw': ('Need Review', 'Need Review'),\n",
    "       'cross match': ('Need Review', 'Need Review'),\n",
    "       'potential match': ('Potential Positive Match', 'Potential Match escalate for further review')\n",
    "   }\n",
    "\n",
    "    specific_rules = {\n",
    "       'input is a currency name provided in a known structured format that clearly indicates it is not the same type of information as the list entry': ('False Match', 'Currency Reference'),\n",
    "       'input is a date format provided in a known structured format that clearly indicates it is not the same type of information as the list entry': ('False Match', 'Date Reference'),\n",
    "       'input is a partial match against an account number provided in a known structured format that clearly indicates it is not the same type of information as the list entry': ('False Match', 'Partial Account Number'),\n",
    "       'input is an alphanumeric reference provided in a known structured format that clearly indicates it is not the same type of information as the list entry': ('False Match', 'Alphanumeric Reference'),\n",
    "       'input is another known input type': ('False Match', 'Known Input Type'),\n",
    "       'input type is part of a response to an open sanctions investigation and does not represent new information': ('False Match', 'Sanctions Investigation')\n",
    "   }\n",
    "\n",
    "    for list_type in other_input_type_buckets.keys():\n",
    "        other_input_type_buckets[list_type].update(common_rules)\n",
    "        other_input_type_buckets[list_type].update(specific_rules)\n",
    "\n",
    "   # Loop through each row in the DataFrame\n",
    "    for index, row in df.iterrows():\n",
    "        final_list_type = row['final_list_type']\n",
    "        guideline_reason = row['guideline_reason'].lower()\n",
    "\n",
    "        if final_list_type in other_input_type_buckets:\n",
    "            best_score = 0\n",
    "            best_action = None\n",
    "            best_reason = None\n",
    "\n",
    "            for bucket, (action, reason) in other_input_type_buckets[final_list_type].items():\n",
    "                score = fuzz.partial_ratio(guideline_reason, bucket.lower())\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_action = action\n",
    "                    best_reason = reason\n",
    "\n",
    "           # Direct keyword match for \"Potential Positive Match\" scenarios\n",
    "            if keyword_match(guideline_reason):\n",
    "                action, reason = 'Potential Positive Match', 'Potential Match escalate for further review'\n",
    "\n",
    "           # If the best_score is above a threshold (say, 80), then use the corresponding bucket\n",
    "            elif best_score >= 80:\n",
    "                action, reason = best_action, best_reason\n",
    "\n",
    "            else:\n",
    "                action, reason = 'Review Needed', 'Review Needed'\n",
    "\n",
    "            df.at[index, 'final_action'] = action\n",
    "            df.at[index, 'final_reason'] = reason\n",
    "\n",
    "    return df\n",
    "\n",
    "def merge_excel_files(input_type, labeling_file, modeling_file, output_file):\n",
    "    df_labeling = pd.read_excel(labeling_file)\n",
    "    print(f\"Len file loaded is: {len(df_labeling)}\")\n",
    "    \n",
    "    # Columns to keep\n",
    "    columns_to_keep_labeling= ['business_unit', \n",
    "                               'match_tag', \n",
    "                               'match_value', \n",
    "                               'offset', \n",
    "                               'match_text',\n",
    "                               'match_pattern_value', \n",
    "                               'ListName', \n",
    "                               'l1_maker_listtype',\n",
    "                               'l1_maker_inputtype', \n",
    "                               'guideline_action', \n",
    "                               'guideline_reason', \n",
    "                               'hit_id',\n",
    "                               'final_input_type', \n",
    "                               'song_list_type',\n",
    "                               'song_input_type', \n",
    "                               'L2_MAKER_INPUTTYPE', \n",
    "                               'L2_MAKER_LISTTYPE',\n",
    "                               'InputName', \n",
    "                               'SONG_ACTION', \n",
    "                               'SONG_NOTE', \n",
    "                               'L2_MAKER_ACTION',\n",
    "                               'L2_MAKER_NOTE',\n",
    "                               'TRIGRAM_TOKENS',\n",
    "                               'status_input_type',\n",
    "                               'status_list_type',\n",
    "                               'final_list_type',\n",
    "                               'label_function_output',\n",
    "                               'start_date',\n",
    "                               'MATCH_WORD']\n",
    "                            \n",
    "    # Select columns to keep labeling file\n",
    "    df_labeling = df_labeling[columns_to_keep_labeling]\n",
    "    print(f\"Len of the file loaded after columns selected: {len(df_labeling)}\")\n",
    "    \n",
    "    important_columns_k = ['hit_id',\n",
    "                           'business_unit', \n",
    "                           'match_tag', \n",
    "                           'match_value', \n",
    "                           'offset', \n",
    "                           'match_text',\n",
    "                           'match_pattern_value', \n",
    "                           'ListName', \n",
    "                           'l1_maker_listtype',\n",
    "                           'l1_maker_inputtype', \n",
    "                           'guideline_action', \n",
    "                           'guideline_reason',  \n",
    "                           'final_input_type', \n",
    "                           'song_list_type',\n",
    "                           'song_input_type', \n",
    "                           'L2_MAKER_INPUTTYPE', \n",
    "                           'L2_MAKER_LISTTYPE',\n",
    "                           'InputName', \n",
    "                           'SONG_ACTION', \n",
    "                           'SONG_NOTE', \n",
    "                           'L2_MAKER_ACTION',\n",
    "                           'L2_MAKER_NOTE', \n",
    "                           'status_input_type',\n",
    "                           'status_list_type',\n",
    "                           'final_list_type',\n",
    "                           'label_function_output',\n",
    "                           'TRIGRAM_TOKENS',\n",
    "                           'start_date',\n",
    "                           'MATCH_WORD']\n",
    "    \n",
    "    # Only merge with df_modeling when input_type is \"Individual\"\n",
    "    if input_type == \"Unknown\":\n",
    "        \n",
    "        # Columns to keep modeling file\n",
    "        columns_to_keep_modeling= ['hit_id', \n",
    "                                   'Model_Output', \n",
    "                                   'Model_Proba']                    \n",
    "\n",
    "        df_modeling = pd.read_excel(modeling_file)\n",
    "        # Select columns to keep\n",
    "        df_modeling = df_modeling[columns_to_keep_modeling]\n",
    "        merged_df = pd.merge(df_modeling, df_labeling, on='hit_id', how='inner')[important_columns_k + ['Model_Output', 'Model_Proba']]\n",
    "    else:\n",
    "        merged_df = df_labeling\n",
    "        #merged_df.dropna(subset='final_input_type', inplace=True)\n",
    "\n",
    "    merged_df.drop_duplicates(inplace=True)   \n",
    "    # Apply status filters\n",
    "    merged_df = merged_df[(merged_df['status_input_type'] == 100) & (merged_df['status_list_type'] == 100)] \n",
    "    print(len(merged_df))\n",
    "    merged_df.dropna(subset=['final_input_type', 'final_list_type'], inplace=True)\n",
    "    print(len(merged_df))\n",
    "\n",
    "\n",
    "    if input_type == \"Unknown\":\n",
    "        merged_df['has_address'] = False\n",
    "    #elif input_type == 'Other Input Type':\n",
    "    #    merged_df['has_address'] = \"is_other_input_type\"\n",
    "    else:   \n",
    "        # Tokenization of TRIGRAM_TOKENS values\n",
    "        #merged_df.dropna(subset=['TRIGRAM_TOKENS'], inplace=True)\n",
    "        #print(f\"size after dropping TRIGRAMS_TOKENS nulls: {len(merged_df)}\")\n",
    "        tokenized_values = merged_df['TRIGRAM_TOKENS'].str.split().explode()\n",
    "        unique_tokens = tokenized_values.unique()\n",
    "        if len(unique_tokens) == 0:\n",
    "            unique_tokens == all_tokens_allowed\n",
    "            print(unique_tokens)\n",
    "\n",
    "\n",
    "\n",
    "        # Create a new column 'has_address' which is True if the TRIGRAM_TOKENS column contains any address token\n",
    "        merged_df['has_address'] = merged_df['TRIGRAM_TOKENS'].apply(contains_address_token)\n",
    "        print(f\"size of df after has address: {len(merged_df)}\")\n",
    "        print(len(merged_df))\n",
    "    \n",
    "    \n",
    "    # Apply the mapping function depending on the 'final_input_type'\n",
    "    mapping_functions = {\n",
    "        'Individual': apply_final_mapping_individual,\n",
    "        'Entity': apply_final_mapping_entity,\n",
    "        'Place/Location': apply_final_mapping_place_location,\n",
    "        'Vessel/Aircraft': apply_final_mapping_vessel_aircraft,\n",
    "        'Other Input Type': apply_final_mapping_other_input_type\n",
    "    }\n",
    "    \n",
    "    valid_input_types = set(mapping_functions.keys())\n",
    "    print(\"Valid Input Types:\")\n",
    "    print(valid_input_types)\n",
    "    filtered_df = merged_df[merged_df['final_input_type'].isin(valid_input_types)]\n",
    "    \n",
    "    l2_escalation_notes=['external','google','https']\n",
    "    \n",
    "    def remove_rows_with_l2_escalation_notes(df):\n",
    "        for note in l2_escalation_notes:\n",
    "            old_len = len(df)\n",
    "            df = df[~df[\"L2_MAKER_NOTE\"].astype(str).str.contains(note)]\n",
    "            new_len = len(df)\n",
    "            print(f'Remove Rows with Keyword: {note}', old_len - new_len)\n",
    "        return df\n",
    "    \n",
    "    # Remove l2 escalation notes\n",
    "    remove_rows_with_l2_escalation_notes(filtered_df)\n",
    "    \n",
    "    # Define the function to apply the final mapping based on the input type\n",
    "    def apply_final_mapping(group):\n",
    "        input_type = group.name  # should give you the name of the group\n",
    "        print(\"The group name is:\")\n",
    "        print(input_type)\n",
    "        func = mapping_functions.get(input_type, lambda x: x)\n",
    "        return func(group)\n",
    "\n",
    "\n",
    "    print(f\"Number of records in dataframe: {len(filtered_df)}\")\n",
    "    grouped_df = filtered_df.groupby('final_input_type').apply(apply_final_mapping)\n",
    "\n",
    "    \n",
    "   # merged_df = merged_df.groupby('final_input_type').apply(lambda x: mapping_functions.get(x.name, lambda x: x)(x))\n",
    "    # Drop NAN for column ListName\n",
    "    grouped_df = grouped_df[grouped_df['ListName'].notna()]\n",
    "    grouped_df.drop_duplicates(inplace=True)\n",
    "    save_to_multiple_sheets(grouped_df, os.path.join(Config.RESULTS_DIR, f\"final_data_joined_{input_type.replace('/', '_')}.xlsx\"))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labeling_file_path is /var/app/pctlness/tensorflow/song/python/data/prod/llm_data/clean_data/final_data_labeled_Other_Input_Type.xlsx\n",
      "Len file loaded is: 26652\n",
      "Len of the file loaded after columns selected: 26652\n",
      "18554\n",
      "18554\n",
      "size of df after has address: 18554\n",
      "18554\n",
      "Valid Input Types:\n",
      "{'Entity', 'Place/Location', 'Individual', 'Other Input Type', 'Vessel/Aircraft'}\n",
      "Remove Rows with Keyword: external 0\n",
      "Remove Rows with Keyword: google 0\n",
      "Remove Rows with Keyword: https 0\n",
      "Number of records in dataframe: 18554\n",
      "The group name is:\n",
      "Other Input Type\n"
     ]
    }
   ],
   "source": [
    "from config_dedup import Config\n",
    "start_time = time.time()\n",
    "logging.basicConfig(filename=\"debug.log\", level=logging.INFO, format=\"%(levelname)s: %(message)s\")\n",
    "logging.info(\"Starting the script!\")\n",
    "\n",
    "#input_types = [  'Place/Location', 'Vessel/Aircraft']\n",
    "#input_types = ['Individual', 'Entity']\n",
    "#input_types = ['Entity']\n",
    "#input_types = ['Individual']\n",
    "\n",
    "\n",
    "\n",
    "#input_types = [ 'Vessel/Aircraft']\n",
    "input_types = [ 'Other Input Type']\n",
    "\n",
    "\n",
    "for input_type in input_types:\n",
    "    labeling_file_path = os.path.join(Config.FOLDER_PATH, f'final_data_labeled_{input_type.replace(\"/\",\"_\").replace(\" \", \"_\")}.xlsx')\n",
    "    print(f\"labeling_file_path is {labeling_file_path}\")\n",
    "    modeling_file_path = os.path.join(Config.FOLDER_PATH, f'final_data_deduplicated_Individual_result.xlsx')\n",
    "    output_file_path = os.path.join(Config.FOLDER_PATH, f'merged_results_{input_type}.xlsx')\n",
    "\n",
    "    merge_excel_files(input_type, labeling_file_path, modeling_file_path, output_file_path)\n",
    "\n",
    "elapsed_time = (time.time() - start_time) / 60\n",
    "logging.info(f\"Script run duration: {elapsed_time:.2f} minutes\")\n",
    "logging.info(\"Script completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labeling_file_path is /var/app/pctlness/tensorflow/song/python/data/prod/llm_data/clean_data/final_data_labeled_Other_Input_Type.xlsx\n",
      "Len file loaded is: 26652\n",
      "Len of the file loaded after columns selected: 26652\n",
      "18554\n",
      "18554\n",
      "Valid Input Types:\n",
      "{'Entity', 'Place/Location', 'Individual', 'Other Input Type', 'Vessel/Aircraft'}\n",
      "Remove Rows with Keyword: external 0\n",
      "Remove Rows with Keyword: google 0\n",
      "Remove Rows with Keyword: https 0\n",
      "Number of records in dataframe: 18554\n",
      "The group name is:\n",
      "Other Input Type\n"
     ]
    }
   ],
   "source": [
    "from config_dedup import Config\n",
    "start_time = time.time()\n",
    "logging.basicConfig(filename=\"debug.log\", level=logging.INFO, format=\"%(levelname)s: %(message)s\")\n",
    "logging.info(\"Starting the script!\")\n",
    "\n",
    "#input_types = [  'Place/Location', 'Vessel/Aircraft']\n",
    "#input_types = ['Individual', 'Entity']\n",
    "#input_types = ['Entity']\n",
    "#input_types = ['Individual']\n",
    "\n",
    "\n",
    "\n",
    "#input_types = [ 'Vessel/Aircraft']\n",
    "input_types = [ 'Other Input Type']\n",
    "\n",
    "\n",
    "for input_type in input_types:\n",
    "    labeling_file_path = os.path.join(Config.FOLDER_PATH, f'final_data_labeled_{input_type.replace(\"/\",\"_\").replace(\" \", \"_\")}.xlsx')\n",
    "    print(f\"labeling_file_path is {labeling_file_path}\")\n",
    "    modeling_file_path = os.path.join(Config.FOLDER_PATH, f'final_data_deduplicated_Individual_result.xlsx')\n",
    "    output_file_path = os.path.join(Config.FOLDER_PATH, f'merged_results_{input_type}.xlsx')\n",
    "\n",
    "    merge_excel_files(input_type, labeling_file_path, modeling_file_path, output_file_path)\n",
    "\n",
    "elapsed_time = (time.time() - start_time) / 60\n",
    "logging.info(f\"Script run duration: {elapsed_time:.2f} minutes\")\n",
    "logging.info(\"Script completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_specific_table(con, table_name):\n",
    "    cursor = con.cursor()\n",
    "    cursor.execute(f\"SELECT column_name, data_type FROM all_tab_columns WHERE table_name = '{table_name}' AND owner = 'SONG'\")\n",
    "    columns = cursor.fetchall()\n",
    "    print(f\"Table: {table_name}\")\n",
    "    print(\"Columns:\")\n",
    "    for column in columns:\n",
    "        print(column)\n",
    "        if column[1] in [\"DATE\", \"TIMESTAMP\"]: \n",
    "            cursor.execute(f\"SELECT MIN({column[0]}), MAX({column[0]}) FROM SONG.{table_name}\")\n",
    "            min_max_dates = cursor.fetchone()\n",
    "            print(f\"Date Range for {column[0]}: {min_max_dates[0]} to {min_max_dates[1]}\")\n",
    "    cursor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table: RUN_RESULT\n",
      "Columns:\n",
      "('REASON_CODE', 'VARCHAR2')\n",
      "('LIST_NAME', 'VARCHAR2')\n",
      "('INPUT_NAME', 'VARCHAR2')\n",
      "('OTHER_NAME', 'VARCHAR2')\n",
      "('HMM_TOKENS', 'VARCHAR2')\n",
      "('CDRTS$ROW', 'TIMESTAMP(6)')\n",
      "('RUN_ID', 'NUMBER')\n",
      "('HIT_ID', 'NUMBER')\n",
      "('ALGORITHIM1_CLASSIFICATION', 'VARCHAR2')\n",
      "('ALGORITHIM2_CLASSIFICATION', 'VARCHAR2')\n",
      "('ALGORITHIM3_CLASSIFICATION', 'VARCHAR2')\n",
      "('ALGORITHIM4_CLASSIFICATION', 'VARCHAR2')\n",
      "('ALGORITHIM_BIL_OUTPUT', 'VARCHAR2')\n",
      "('ALGORITHIM1_ACTION', 'VARCHAR2')\n",
      "('ALGORITHIM1_NOTE', 'VARCHAR2')\n",
      "('ALGORITHIM2_ACTION', 'VARCHAR2')\n",
      "('ALGORITHIM2_NOTE', 'VARCHAR2')\n",
      "('PARTITION_DATE', 'DATE')\n",
      "Date Range for PARTITION_DATE: 2023-10-06 00:00:00 to 2023-10-08 00:00:00\n"
     ]
    }
   ],
   "source": [
    "explore_specific_table(con, 'RUN_RESULT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_specific_table(con, 'RUN_RESULT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'explore_table_names' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-5d3f468522d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mcursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mexplore_table_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'explore_table_names' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def explore_tables(con):\n",
    "    cursor = con.cursor()\n",
    "    cursor.execute(\"SELECT table_name FROM all_tables WHERE owner = 'SONG'\")\n",
    "    tables = [table[0] for table in cursor.fetchall()]\n",
    "    cursor.close()\n",
    "\n",
    "    for table_name in tables:\n",
    "        print(f\"Table: {table_name}\")\n",
    "\n",
    "        cursor = con.cursor()\n",
    "        cursor.execute(f\"SELECT column_name, data_type FROM all_tab_columns WHERE table_name = '{table_name}' AND owner = 'SONG'\")\n",
    "        columns = cursor.fetchall()\n",
    "        print(\"Columns:\")\n",
    "        for column in columns:\n",
    "            print(column)\n",
    "\n",
    "        df = pd.read_sql(f\"SELECT * FROM SONG.{table_name} WHERE ROWNUM <= 10\", con)\n",
    "        print(df.head())  # Print first 5 rows of the dataframe\n",
    "\n",
    "        cursor.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def explore_table_names(con):\n",
    "    cursor = con.cursor()\n",
    "    cursor.execute(\"SELECT table_name FROM all_tables WHERE owner = 'SONG'\")\n",
    "    tables = [table[0] for table in cursor.fetchall()]\n",
    "    cursor.close()\n",
    "\n",
    "    for table_name in tables:\n",
    "        print(f\"Table: {table_name}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table: STOP_TIME\n",
      "Table: SONG_INPUT_LIST_TYPE_MAP\n",
      "Table: SONG_INPUT_HMM_TRAN_AUDIT\n",
      "Table: SONG_INPUT_RF_AUDIT\n",
      "Table: SONG_INPUT_RF_DATA\n",
      "Table: SONG_INPUT_HMM_EMISSION_AUDIT\n",
      "Table: SONG_LIST_MODEL_AUDIT\n",
      "Table: DATE_DETAILS\n",
      "Table: SONG_MODEL_MST\n",
      "Table: SONG_INPUT_OTHER_AUDIT\n",
      "Table: SONG_INPUT_OTHER_DATA\n",
      "Table: SONG_INPUT_HMM_EMISSION_DATA\n",
      "Table: SONG_INPUT_NAME_AUDIT\n",
      "Table: SONG_MODEL_AUDIT\n",
      "Table: SONG_INPUT_HMM_TRAN_DATA\n",
      "Table: SONG_INPUT_NAME_DATA\n",
      "Table: SONG_CONFIG_MAP\n",
      "Table: GG_LAG_UPDATE\n",
      "Table: SONG_LIST_MODEL_DATA\n",
      "Table: SONG_METADATA\n",
      "Table: SONG_METADATA_AUDIT\n",
      "Table: DT$_GG_LAG_UPDATE\n",
      "Table: DT$_SONG_INPUT_HMM_EMISSION_AUDIT\n",
      "Table: DT$_DATE_DETAILS\n",
      "Table: DT$_SONG_INPUT_LIST_TYPE_MAP\n",
      "Table: DT$_SONG_INPUT_HMM_TRAN_DATA\n",
      "Table: DT$_SONG_MODEL_AUDIT\n",
      "Table: DT$_SONG_METADATA_AUDIT\n",
      "Table: DT$_SONG_MODEL_MST\n",
      "Table: DT$_SONG_LIST_MODEL_DATA\n",
      "Table: DT$_SONG_INPUT_RF_DATA\n",
      "Table: DT$_SONG_INPUT_HMM_EMISSION_DATA\n",
      "Table: DT$_SONG_LIST_MODEL_AUDIT\n",
      "Table: DT$_SONG_INPUT_OTHER_AUDIT\n",
      "Table: DT$_SONG_INPUT_OTHER_DATA\n",
      "Table: DT$_SONG_INPUT_HMM_TRAN_AUDIT\n",
      "Table: DT$_SONG_INPUT_RF_AUDIT\n",
      "Table: DT$_SONG_CONFIG_MAP\n",
      "Table: DT$_SONG_INPUT_NAME_DATA\n",
      "Table: DT$_SONG_INPUT_NAME_AUDIT\n",
      "Table: DT$_SONG_METADATA\n",
      "Table: DT$_EXC_STEPS_LOG\n",
      "Table: DT$_TRANSACTION_DET\n",
      "Table: DT$_MATCH_DET\n",
      "Table: DT$_RUN_MST\n",
      "Table: DT$_RUN_RESULT\n",
      "Table: IAS_SCENARIO_MST\n",
      "Table: IAS_WORKER_TASK_EXECUTION\n",
      "Table: IAS_TASK_MST\n",
      "Table: IAS_WORKER_MST\n",
      "Table: IAS_SCENARIO_WORKER_TASK_MAP\n",
      "Table: IAS_SCENARIO_TASK_MAP\n",
      "Table: IAS_MST\n",
      "Table: IAS_SCENARIO_DECISION_MAP\n",
      "Table: IAS_SCENARIO_WORKER_ACTION_MAP\n",
      "Table: IAS_EXECUTION_FLOW_MST\n",
      "Table: IAS_SCENARIO_EXECUTION_PLAN\n",
      "Table: IAS_STEPS_LOG\n",
      "Table: EXC_STEPS_LOG\n",
      "Table: RUN_MST\n",
      "Table: RUN_RESULT\n",
      "Table: TRANSACTION_DET\n",
      "Table: MATCH_DET\n"
     ]
    }
   ],
   "source": [
    "explore_table_names(con)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create the dtaste for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LastName LastName LastName'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigramAddrTokens('Breto Rangel, Guillermo',True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "files = []\n",
    "input_types = ['Individual', 'Entity', 'Place_Location', 'Vessel_Aircraft', 'Other Input Type']\n",
    "\n",
    "# This dictionary will hold the concatenated dataframes for each input type\n",
    "concatenated_dataframes = {}\n",
    "\n",
    "for input_type in input_types:\n",
    "    # Get all the file paths for the given input type\n",
    "    files.append(os.path.join(Config.RESULTS_DIR, f\"final_data_joined_{input_type}.xlsx\"))\n",
    "\n",
    "dfs = [pd.read_excel(file, engine='openpyxl') for file in files]\n",
    "concatenated_dataframes[input_type] = pd.concat(dfs, ignore_index=True)\n",
    "combined_df = pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/var/app/pctlness/tensorflow/song/python/data/prod/llm_data/clean_data/final_data_joined_Individual.xlsx',\n",
       " '/var/app/pctlness/tensorflow/song/python/data/prod/llm_data/clean_data/final_data_joined_Entity.xlsx',\n",
       " '/var/app/pctlness/tensorflow/song/python/data/prod/llm_data/clean_data/final_data_joined_Place_Location.xlsx',\n",
       " '/var/app/pctlness/tensorflow/song/python/data/prod/llm_data/clean_data/final_data_joined_Vessel_Aircraft.xlsx',\n",
       " '/var/app/pctlness/tensorflow/song/python/data/prod/llm_data/clean_data/final_data_joined_Other Input Type.xlsx']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "# Reshuffle the combined dataframe\n",
    "reshuffled_df = combined_df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "# Split the reshuffled dataframe into training and test \n",
    "train_df = reshuffled_df.sample(frac=0.8, random_state=42)\n",
    "test_df = reshuffled_df.drop(train_df.index).reset_index(drop=True)\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "\n",
    "# Convert the train and test dataframes to HuggingFace datasets\n",
    "dataset_dict = DatasetDict({\n",
    "   'train': Dataset.from_pandas(train_df),\n",
    "   'test': Dataset.from_pandas(test_df)\n",
    "})\n",
    "\n",
    "# Specify the directory to save the datasets\n",
    "save_path = os.path.join(Config.RESULTS_DIR, \"reshuffled_dataset\")\n",
    "\n",
    "# Save the datasets to disk\n",
    "dataset_dict.save_to_disk(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['business_unit', 'match_tag', 'match_value', 'offset', 'match_text', 'match_pattern_value', 'ListName', 'l1_maker_listtype', 'l1_maker_inputtype', 'guideline_action', 'guideline_reason', 'hit_id', 'final_input_type', 'song_list_type', 'song_input_type', 'L2_MAKER_INPUTTYPE', 'L2_MAKER_LISTTYPE', 'InputName', 'SONG_ACTION', 'SONG_NOTE', 'L2_MAKER_ACTION', 'L2_MAKER_NOTE', 'TRIGRAM_TOKENS', 'status_input_type', 'status_list_type', 'final_list_type', 'label_function_output', 'start_date', 'MATCH_WORD', 'has_address', 'final_action', 'final_reason', 'has_name'],\n",
       "        num_rows: 204723\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['business_unit', 'match_tag', 'match_value', 'offset', 'match_text', 'match_pattern_value', 'ListName', 'l1_maker_listtype', 'l1_maker_inputtype', 'guideline_action', 'guideline_reason', 'hit_id', 'final_input_type', 'song_list_type', 'song_input_type', 'L2_MAKER_INPUTTYPE', 'L2_MAKER_LISTTYPE', 'InputName', 'SONG_ACTION', 'SONG_NOTE', 'L2_MAKER_ACTION', 'L2_MAKER_NOTE', 'TRIGRAM_TOKENS', 'status_input_type', 'status_list_type', 'final_list_type', 'label_function_output', 'start_date', 'MATCH_WORD', 'has_address', 'final_action', 'final_reason', 'has_name'],\n",
       "        num_rows: 51181\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add hitname , alias, place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'business_unit': 'ICG_PNR_NAM_US_CITIFT', 'match_tag': '/transactionmessage/origcdtradrline', 'match_value': 'mr. dmitry losevskiy', 'offset': 5, 'match_text': 'dmitry', 'match_pattern_value': 'dmitry', 'ListName': 'dmitry', 'l1_maker_listtype': 'Individual', 'l1_maker_inputtype': 'Individual', 'guideline_action': 'false match', 'guideline_reason': 'input name has additional components not matching list entry', 'hit_id': 96145331, 'final_input_type': 'Individual', 'song_list_type': 'Individual', 'song_input_type': 'Individual', 'L2_MAKER_INPUTTYPE': None, 'L2_MAKER_LISTTYPE': None, 'InputName': None, 'SONG_ACTION': None, 'SONG_NOTE': None, 'L2_MAKER_ACTION': None, 'L2_MAKER_NOTE': None, 'TRIGRAM_TOKENS': 'Salutation LastName LastName', 'status_input_type': 100, 'status_list_type': 100, 'final_list_type': 'Individual', 'label_function_output': 'False Match: Full Name Mismatch Scenario', 'start_date': '01-FEB-2023', 'MATCH_WORD': 'dmitry', 'has_address': False, 'final_action': 'False Match', 'final_reason': 'Input Name has additional components not matching list entry', 'has_name': True}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "files = []\n",
    "input_types = ['Individual', 'Entity', 'Place_Location', 'Vessel_Aircraft', 'Other Input Type']\n",
    "\n",
    "# This dictionary will hold the concatenated dataframes for each input type\n",
    "concatenated_dataframes = {}\n",
    "\n",
    "for input_type in input_types:\n",
    "    # Get all the file paths for the given input type\n",
    "    files.append(os.path.join(Config.RESULTS_DIR, f\"final_data_joined_{input_type}.xlsx\"))\n",
    "\n",
    "dfs = [pd.read_excel(file, engine='openpyxl') for file in files]\n",
    "keys = ['Individual', 'Entity', 'Place/Location', 'Vessel/Aircraft', 'Other Input Type']\n",
    "\n",
    "dataframes_dic = dict(zip(keys, dfs))\n",
    "\n",
    "dataset_objects = {}\n",
    "\n",
    "for key in keys: \n",
    "    dataset_objects[key] = Dataset.from_pandas(dataframes_dic[key]).train_test_split(test_size=0.2)\n",
    "    \n",
    "    \n",
    "train_ind = dataset_objects['Individual']['train']\n",
    "\n",
    "print(train_ind[1023])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.concat(dfs)\n",
    "df = shuffle(df, random_state=42)\n",
    "df_subset = df.sample(n=1000, random_state=42)\n",
    "\n",
    "columns_to_drop = ['l1_maker_inputtype', 'guideline_action', 'guideline_reason', 'has_address',\n",
    "             'l1_maker_listtype', 'song_list_type', 'song_input_type', 'L2_MAKER_INPUTTYPE', 'L2_MAKER_LISTTYPE',\n",
    "             'SONG_ACTION', 'SONG_NOTE', 'L2_MAKER_ACTION', 'L2_MAKER_NOTE', 'status_input_type',\n",
    "             'status_list_type', 'TRIGRAM_TOKENS', 'start_date', 'has_name']\n",
    "df_subset.drop(columns=columns_to_drop, inplace=True)\n",
    "df.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "\n",
    "df_dataset_subset = Dataset.from_pandas(df_subset)\n",
    "\n",
    "df_dataset_all = Dataset.from_pandas(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_objects = df_dataset_all.train_test_split(test_size=0.2)\n",
    "dataset_objects_subset = df_dataset_subset.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['business_unit', 'match_tag', 'match_value', 'offset', 'match_text', 'match_pattern_value', 'ListName', 'hit_id', 'final_input_type', 'InputName', 'final_list_type', 'label_function_output', 'MATCH_WORD', 'final_action', 'final_reason', '__index_level_0__'],\n",
       "        num_rows: 204723\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['business_unit', 'match_tag', 'match_value', 'offset', 'match_text', 'match_pattern_value', 'ListName', 'hit_id', 'final_input_type', 'InputName', 'final_list_type', 'label_function_output', 'MATCH_WORD', 'final_action', 'final_reason', '__index_level_0__'],\n",
       "        num_rows: 51181\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import cx_Oracle\n",
    "import logging\n",
    "import time\n",
    "#from config import Config\n",
    "\n",
    "# Suppress warnings and set seeds\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ[\"KMP_WARNINGS\"] = \"FALSE\"\n",
    "random_seed = 23\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "\n",
    "# Paths for CS Constants\n",
    "sys.path.append(\"/var/app/cstxuat/tensorflow/song/python/process/common/\")\n",
    "sys.path.append(\"/var/app/cstxuat/tensorflow/song/unix/bin/\")\n",
    "from CSConstants import *\n",
    "sys.path.append(CONTEXT_INPUT_METADATA)\n",
    "sys.path.append(CONTEXT_INPUT_MODEL)\n",
    "sys.path.append(CONTEXT_INPUT_PROCESS)\n",
    "from TXInputTypeProcess import *\n",
    "\n",
    "\n",
    "\n",
    "# Save a large DataFrame to multiple sheets in an Excel file\n",
    "def save_to_multiple_sheets(df, output_filename, split_size=1_000_000):\n",
    "    with pd.ExcelWriter(output_filename, engine='openpyxl') as writer:\n",
    "        for i, start in enumerate(range(0, len(df), split_size)):\n",
    "            df.iloc[start:start+split_size].to_excel(writer, sheet_name=f\"Sheet_{i+1}\", index=False)\n",
    "\n",
    "# Data Extraction for a given month\n",
    "def extract_data_for_month(month_start, month_end):\n",
    "    query = f\"\"\"\n",
    "        SELECT m.HIT_ID,\n",
    "               CATEGORY,\n",
    "               MATCHABLE_DESCRIPTION\n",
    "        FROM match_det m\n",
    "        LEFT JOIN run_result r ON r.hit_id = m.HIT_ID\n",
    "        WHERE m.L1_MAKER_DATE BETWEEN '{month_start}' AND '{month_end}'\n",
    "    \"\"\"\n",
    "    return pd.read_sql(query, con)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The extracted data for all 6 months contained: 1074580 records\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start_time = time.time()\n",
    "\n",
    "logging.basicConfig(filename=\"debug.log\", level=logging.INFO, format=\"%(levelname)s: %(message)s\")\n",
    "logging.info(\"Starting the script!\")\n",
    "\n",
    "all_db_data = []\n",
    "months_to_process = [        \n",
    "        ('01-JAN-2023', '31-JAN-2023'),\n",
    "        ('01-FEB-2023', '28-FEB-2023'),\n",
    "\n",
    "      \n",
    "    ]   \n",
    "for month_start, month_end in months_to_process:\n",
    "    monthly_data_db = extract_data_for_month(month_start, month_end)        \n",
    "    logging.info(f\"Extracted Data from DB Columns are {list(monthly_data_db.columns)} starting in {month_start}.\")\n",
    "    logging.info(f\"Extracted {len(monthly_data_db)} records from database for the specified month range.\")\n",
    "    monthly_data_db.rename(columns={'HIT_ID':'hit_id'}, inplace=True)\n",
    "    logging.info(f\"Extracted Data from DB Columns after renaming are {list(monthly_data_db.columns)}.\")\n",
    "    monthly_data_db[\"start_date\"] = month_start\n",
    "    monthly_data_db[\"end_date\"] = month_end\n",
    "    all_db_data.append(monthly_data_db)\n",
    "\n",
    "all_db_data_df = pd.concat(all_db_data)\n",
    "print(f\"The extracted data for all 6 months contained: {len(all_db_data_df)} records\")\n",
    "logging.info(f\"The extracted data for all 6 months contained: {len(all_db_data_df)} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_location(messy_string):\n",
    "    cleaned= re.sub(r'\\bINPUT_DATE\\b.+','.', messy_string)\n",
    "    cleaned=re.findall(r'[^#!@,~;.-]+', cleaned)\n",
    "    cleaned= ' '.join(cleaned)\n",
    "    #cleaned=re.findall(r'[^-]+',cleaned)\n",
    "    #cleaned=' '.join(cleaned)\n",
    "    loc= re.findall(r'LOCATION(.*?)POB',cleaned)\n",
    "    loc=' '.join(loc)\n",
    "    return loc.strip()\n",
    "\n",
    "def extract_alias(messy_string):\n",
    "    cleaned= re.sub(r'\\bINPUT_DATE\\b.+','.', messy_string)\n",
    "    cleaned=re.findall(r'[^#!@,~;.-]+', cleaned)\n",
    "    cleaned= ' '.join(cleaned)\n",
    "    #cleaned=re.findall(r'[^-]+',cleaned)\n",
    "    #cleaned=' '.join(cleaned)\n",
    "    aka=re.findall(r'AKA (.*?)TYPE', cleaned)\n",
    "    aka=' '.join(aka)\n",
    "    return aka.strip()\n",
    "\n",
    "\n",
    "def extract_fullname(messy_string):\n",
    "    cleaned= re.sub(r'\\bINPUT_DATE\\b.+','.', messy_string)\n",
    "    cleaned=re.findall(r'[^''#!@,]+', cleaned)\n",
    "    cleaned= ' '.join(cleaned)\n",
    "    #cleaned=re.findall(r'[^-]+',cleaned)\n",
    "    #cleaned=' '.join(cleaned)\n",
    "    hit=re.findall(r'(.*?)RISK_ID', cleaned)\n",
    "    hit=' '.join(hit)\n",
    "    hit= hit.title()\n",
    "    return hit.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.concat(dfs)\n",
    "\n",
    "logging.info(f\"All Data Columns: {list(all_data.columns)}.\")\n",
    "logging.info(f\"All Data Records: {len(all_data)}.\")\n",
    "merged_data = pd.merge(all_data, all_db_data_df, on='hit_id', how='left')\n",
    "logging.info(f\"Merged Data Columns after left merge with DB {list(merged_data.columns)}.\")\n",
    "logging.info(f\"All Data Records after left merge with DB: {len(merged_data)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "255904"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "255904"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(merged_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data['place'] = merged_data['MATCHABLE_DESCRIPTION'].apply(extract_location)\n",
    "merged_data['hit_name'] = merged_data['MATCHABLE_DESCRIPTION'].apply(extract_fullname)\n",
    "merged_data['alias'] = merged_data['MATCHABLE_DESCRIPTION'].apply(extract_alias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged Data Columns after left merge with DB ['business_unit', 'match_tag', 'match_value', 'offset', 'match_text', 'match_pattern_value', 'ListName', 'l1_maker_listtype', 'l1_maker_inputtype', 'guideline_action', 'guideline_reason', 'hit_id', 'final_input_type', 'song_list_type', 'song_input_type', 'L2_MAKER_INPUTTYPE', 'L2_MAKER_LISTTYPE', 'InputName', 'SONG_ACTION', 'SONG_NOTE', 'L2_MAKER_ACTION', 'L2_MAKER_NOTE', 'TRIGRAM_TOKENS', 'status_input_type', 'status_list_type', 'final_list_type', 'label_function_output', 'start_date_x', 'MATCH_WORD', 'has_address', 'final_action', 'final_reason', 'has_name', 'CATEGORY', 'MATCHABLE_DESCRIPTION', 'start_date_y', 'end_date', 'place', 'hit_name', 'alias'].\n"
     ]
    }
   ],
   "source": [
    "print(f\"Merged Data Columns after left merge with DB {list(merged_data.columns)}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "columns_to_drop = ['SONG_ACTION', 'SONG_NOTE', 'L2_MAKER_ACTION', 'L2_MAKER_NOTE', 'status_input_type', 'status_list_type','MATCHABLE_DESCRIPTION', 'start_date_y', 'end_date', 'song_list_type', 'song_input_type', 'L2_MAKER_INPUTTYPE', 'L2_MAKER_LISTTYPE', 'l1_maker_listtype', 'l1_maker_inputtype', 'TRIGRAM_TOKENS', 'start_date_x', 'has_address', 'has_name']\n",
    "merged_data.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "merged_data = shuffle(merged_data, random_state=42)\n",
    "merged_dataset_all = Dataset.from_pandas(merged_data)\n",
    "merged_data_subset = merged_data.sample(n=1000, random_state=42)\n",
    "\n",
    "merged_dataset_subset = Dataset.from_pandas(merged_data_subset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_objects = merged_dataset_all.train_test_split(test_size=0.2).remove_columns(['__index_level_0__',])\n",
    "dataset_objects_subset = merged_dataset_subset.train_test_split(test_size=0.2).remove_columns(['__index_level_0__',])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9317177247b04abb982cc43094d0ed74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices:   0%|          | 0/205 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35789852c90e4c74a2847be65e8ffb1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices:   0%|          | 0/52 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd696601fcc5466c80bb03343e396098",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "484ec8e9cdc04821bc746939155311b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Specify the directory to save the datasets\n",
    "save_path = os.path.join(Config.RESULTS_DIR, \"jan_feb_dataset/\")\n",
    "\n",
    "# Save the datasets to disk\n",
    "dataset_objects.save_to_disk(save_path + \"dataset/\")\n",
    "dataset_objects_subset.save_to_disk(save_path + \"subset/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['business_unit', 'match_tag', 'match_value', 'offset', 'match_text', 'match_pattern_value', 'ListName', 'guideline_action', 'guideline_reason', 'hit_id', 'final_input_type', 'InputName', 'final_list_type', 'label_function_output', 'MATCH_WORD', 'final_action', 'final_reason', 'CATEGORY', 'place', 'hit_name', 'alias'],\n",
       "        num_rows: 204723\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['business_unit', 'match_tag', 'match_value', 'offset', 'match_text', 'match_pattern_value', 'ListName', 'guideline_action', 'guideline_reason', 'hit_id', 'final_input_type', 'InputName', 'final_list_type', 'label_function_output', 'MATCH_WORD', 'final_action', 'final_reason', 'CATEGORY', 'place', 'hit_name', 'alias'],\n",
       "        num_rows: 51181\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['business_unit', 'match_tag', 'match_value', 'offset', 'match_text', 'match_pattern_value', 'ListName', 'guideline_action', 'guideline_reason', 'hit_id', 'final_input_type', 'InputName', 'final_list_type', 'label_function_output', 'MATCH_WORD', 'final_action', 'final_reason', 'CATEGORY', 'place', 'hit_name', 'alias'],\n",
       "        num_rows: 800\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['business_unit', 'match_tag', 'match_value', 'offset', 'match_text', 'match_pattern_value', 'ListName', 'guideline_action', 'guideline_reason', 'hit_id', 'final_input_type', 'InputName', 'final_list_type', 'label_function_output', 'MATCH_WORD', 'final_action', 'final_reason', 'CATEGORY', 'place', 'hit_name', 'alias'],\n",
       "        num_rows: 200\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_objects_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "dot = Digraph(comment='Sanction Screening Flow - Input: Individual', format='png')\n",
    "\n",
    "# Define Nodes\n",
    "dot.node('START', 'Start', shape='ellipse')\n",
    "dot.node('IND', 'Input: Individual', color='lightblue')\n",
    "\n",
    "# Nodes for List Types under Input: Individual\n",
    "dot.node('LENT', 'List: Entity', color='green')\n",
    "dot.node('LVESSEL', 'List: Vessel/Aircraft', color='green')\n",
    "dot.node('LPLACE1', 'List: Place/Location (No Address)', color='green')\n",
    "dot.node('LPLACE2', 'List: Place/Location (Defined Field)', color='green')\n",
    "dot.node('LINDIVIDUAL', 'List: Individual', color='green')\n",
    "\n",
    "# Nodes for Match Types & Rules\n",
    "dot.node('FALSEMATCH', 'False Match', shape='box', color='lightcoral')\n",
    "dot.node('POTENTIALMATCH', 'Potential Positive Match', shape='box', color='yellow')\n",
    "dot.node('ESCALATE', 'Escalate', shape='box', color='orange')\n",
    "\n",
    "# Define Edges\n",
    "dot.edge('START', 'IND')\n",
    "\n",
    "# From 'Input: Individual' to List Types\n",
    "dot.edge('IND', 'LENT', label='Cross Match')\n",
    "dot.edge('IND', 'LVESSEL', label='Cross Match')\n",
    "dot.edge('IND', 'LPLACE1', label='No Address or Mismatched Address')\n",
    "dot.edge('IND', 'LPLACE2', label='Defined Field Match')\n",
    "dot.edge('IND', 'LINDIVIDUAL', label='Name Component Checks')\n",
    "\n",
    "# Outcomes\n",
    "dot.edge('LENT', 'FALSEMATCH')\n",
    "dot.edge('LVESSEL', 'FALSEMATCH')\n",
    "dot.edge('LPLACE1', 'FALSEMATCH')\n",
    "dot.edge('LPLACE2', 'FALSEMATCH')\n",
    "dot.edge('LINDIVIDUAL', 'FALSEMATCH', label='Full Name Mismatch')\n",
    "dot.edge('LINDIVIDUAL', 'POTENTIALMATCH', label='Potential Match')\n",
    "dot.edge('LINDIVIDUAL', 'ESCALATE', label='Escalate for Review')\n",
    "\n",
    "#dot.render('sanction_screening_individual_flowchart', view=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"1331pt\" height=\"291pt\"\n",
       " viewBox=\"0.00 0.00 1331.14 291.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 287)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-287 1327.1448,-287 1327.1448,4 -4,4\"/>\n",
       "<!-- START -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>START</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"516.1448\" cy=\"-265\" rx=\"33.2948\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"516.1448\" y=\"-261.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Start</text>\n",
       "</g>\n",
       "<!-- IND -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>IND</title>\n",
       "<ellipse fill=\"none\" stroke=\"#add8e6\" cx=\"516.1448\" cy=\"-192\" rx=\"85.2851\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"516.1448\" y=\"-188.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Input: Individual</text>\n",
       "</g>\n",
       "<!-- START&#45;&gt;IND -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>START&#45;&gt;IND</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M516.1448,-246.9551C516.1448,-238.8828 516.1448,-229.1764 516.1448,-220.1817\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"519.6449,-220.0903 516.1448,-210.0904 512.6449,-220.0904 519.6449,-220.0903\"/>\n",
       "</g>\n",
       "<!-- LENT -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>LENT</title>\n",
       "<ellipse fill=\"none\" stroke=\"#00ff00\" cx=\"59.1448\" cy=\"-105\" rx=\"59.2899\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"59.1448\" y=\"-101.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">List: Entity</text>\n",
       "</g>\n",
       "<!-- IND&#45;&gt;LENT -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>IND&#45;&gt;LENT</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M442.3443,-183.0035C392.8766,-176.5525 326.3803,-167.0683 268.1448,-156 205.3416,-144.0635 190.1889,-138.4028 128.1448,-123 123.5076,-121.8488 118.7004,-120.6356 113.8798,-119.4053\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"114.5728,-115.9697 104.0163,-116.8701 112.8302,-122.7494 114.5728,-115.9697\"/>\n",
       "<text text-anchor=\"middle\" x=\"312.1448\" y=\"-144.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Cross Match</text>\n",
       "</g>\n",
       "<!-- LVESSEL -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>LVESSEL</title>\n",
       "<ellipse fill=\"none\" stroke=\"#00ff00\" cx=\"236.1448\" cy=\"-105\" rx=\"99.3824\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"236.1448\" y=\"-101.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">List: Vessel/Aircraft</text>\n",
       "</g>\n",
       "<!-- IND&#45;&gt;LVESSEL -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>IND&#45;&gt;LVESSEL</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M468.011,-177.0442C420.4179,-162.2563 347.4648,-139.5887 296.1254,-123.6369\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"297.0692,-120.2651 286.481,-120.6402 294.9921,-126.9498 297.0692,-120.2651\"/>\n",
       "<text text-anchor=\"middle\" x=\"437.1448\" y=\"-144.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Cross Match</text>\n",
       "</g>\n",
       "<!-- LPLACE1 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>LPLACE1</title>\n",
       "<ellipse fill=\"none\" stroke=\"#00ff00\" cx=\"516.1448\" cy=\"-105\" rx=\"162.4712\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"516.1448\" y=\"-101.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">List: Place/Location (No Address)</text>\n",
       "</g>\n",
       "<!-- IND&#45;&gt;LPLACE1 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>IND&#45;&gt;LPLACE1</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M516.1448,-173.9735C516.1448,-162.1918 516.1448,-146.5607 516.1448,-133.1581\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"519.6449,-133.0033 516.1448,-123.0034 512.6449,-133.0034 519.6449,-133.0033\"/>\n",
       "<text text-anchor=\"middle\" x=\"643.6448\" y=\"-144.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">No Address or Mismatched Address</text>\n",
       "</g>\n",
       "<!-- LPLACE2 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>LPLACE2</title>\n",
       "<ellipse fill=\"none\" stroke=\"#00ff00\" cx=\"867.1448\" cy=\"-105\" rx=\"170.0701\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"867.1448\" y=\"-101.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">List: Place/Location (Defined Field)</text>\n",
       "</g>\n",
       "<!-- IND&#45;&gt;LPLACE2 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>IND&#45;&gt;LPLACE2</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M593.4458,-184.4115C657.2462,-177.5667 742.4174,-166.9234 775.1448,-156 795.0226,-149.3654 815.5651,-138.4316 832.1824,-128.3858\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"834.3274,-131.174 840.9812,-122.9292 830.6382,-125.2251 834.3274,-131.174\"/>\n",
       "<text text-anchor=\"middle\" x=\"880.1448\" y=\"-144.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Defined Field Match</text>\n",
       "</g>\n",
       "<!-- LINDIVIDUAL -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>LINDIVIDUAL</title>\n",
       "<ellipse fill=\"none\" stroke=\"#00ff00\" cx=\"1133.1448\" cy=\"-105\" rx=\"77.1866\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"1133.1448\" y=\"-101.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">List: Individual</text>\n",
       "</g>\n",
       "<!-- IND&#45;&gt;LINDIVIDUAL -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>IND&#45;&gt;LINDIVIDUAL</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M600.9498,-189.9953C689.8466,-186.7424 833.2726,-178.2313 955.1448,-156 997.9266,-148.1959 1045.1997,-134.3981 1080.2704,-123.1204\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1081.3973,-126.4345 1089.8247,-120.0135 1079.2325,-119.7776 1081.3973,-126.4345\"/>\n",
       "<text text-anchor=\"middle\" x=\"1112.6448\" y=\"-144.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Name Component Checks</text>\n",
       "</g>\n",
       "<!-- FALSEMATCH -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>FALSEMATCH</title>\n",
       "<polygon fill=\"none\" stroke=\"#f08080\" points=\"566.1448,-36 466.1448,-36 466.1448,0 566.1448,0 566.1448,-36\"/>\n",
       "<text text-anchor=\"middle\" x=\"516.1448\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">False Match</text>\n",
       "</g>\n",
       "<!-- LENT&#45;&gt;FALSEMATCH -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>LENT&#45;&gt;FALSEMATCH</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M103.1906,-92.9549C111.4544,-90.853 120.0421,-88.7744 128.1448,-87 243.4369,-61.7519 379.5297,-39.1997 456.0394,-27.1745\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"456.6387,-30.6234 465.9772,-25.6192 455.5563,-23.7076 456.6387,-30.6234\"/>\n",
       "</g>\n",
       "<!-- LVESSEL&#45;&gt;FALSEMATCH -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>LVESSEL&#45;&gt;FALSEMATCH</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M286.2031,-89.4461C333.8357,-74.646 405.5569,-52.3612 456.2118,-36.622\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"457.5374,-39.8753 466.0485,-33.5656 455.4603,-33.1905 457.5374,-39.8753\"/>\n",
       "</g>\n",
       "<!-- LPLACE1&#45;&gt;FALSEMATCH -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>LPLACE1&#45;&gt;FALSEMATCH</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M516.1448,-86.9735C516.1448,-75.1918 516.1448,-59.5607 516.1448,-46.1581\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"519.6449,-46.0033 516.1448,-36.0034 512.6449,-46.0034 519.6449,-46.0033\"/>\n",
       "</g>\n",
       "<!-- LPLACE2&#45;&gt;FALSEMATCH -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>LPLACE2&#45;&gt;FALSEMATCH</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M800.2982,-88.4312C735.6715,-72.4126 638.6369,-48.3613 576.3519,-32.9232\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"576.8327,-29.4365 566.2844,-30.4278 575.1486,-36.2309 576.8327,-29.4365\"/>\n",
       "</g>\n",
       "<!-- LINDIVIDUAL&#45;&gt;FALSEMATCH -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>LINDIVIDUAL&#45;&gt;FALSEMATCH</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M1076.6378,-92.6429C1066.5103,-90.6184 1056.0341,-88.6473 1046.1448,-87 983.3932,-76.5476 967.2373,-77.1453 904.1448,-69 788.3068,-54.0452 653.0739,-36.1766 576.7338,-26.0518\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"576.7344,-22.5213 566.3609,-24.6757 575.8137,-29.4605 576.7344,-22.5213\"/>\n",
       "<text text-anchor=\"middle\" x=\"976.6448\" y=\"-57.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Full Name Mismatch</text>\n",
       "</g>\n",
       "<!-- POTENTIALMATCH -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>POTENTIALMATCH</title>\n",
       "<polygon fill=\"none\" stroke=\"#ffff00\" points=\"1150.1448,-36 966.1448,-36 966.1448,0 1150.1448,0 1150.1448,-36\"/>\n",
       "<text text-anchor=\"middle\" x=\"1058.1448\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Potential Positive Match</text>\n",
       "</g>\n",
       "<!-- LINDIVIDUAL&#45;&gt;POTENTIALMATCH -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>LINDIVIDUAL&#45;&gt;POTENTIALMATCH</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M1083.8782,-91.0188C1073.69,-85.7825 1064.2215,-78.6573 1058.1448,-69 1053.948,-62.3303 1052.6693,-54.134 1052.7869,-46.2788\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1056.2753,-46.5619 1053.6179,-36.3057 1049.2995,-45.9805 1056.2753,-46.5619\"/>\n",
       "<text text-anchor=\"middle\" x=\"1112.6448\" y=\"-57.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Potential Match</text>\n",
       "</g>\n",
       "<!-- ESCALATE -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>ESCALATE</title>\n",
       "<polygon fill=\"none\" stroke=\"#ffa500\" points=\"1245.6448,-36 1168.6448,-36 1168.6448,0 1245.6448,0 1245.6448,-36\"/>\n",
       "<text text-anchor=\"middle\" x=\"1207.1448\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Escalate</text>\n",
       "</g>\n",
       "<!-- LINDIVIDUAL&#45;&gt;ESCALATE -->\n",
       "<g id=\"edge13\" class=\"edge\">\n",
       "<title>LINDIVIDUAL&#45;&gt;ESCALATE</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M1153.2117,-87.389C1159.2258,-81.7483 1165.6726,-75.3211 1171.1448,-69 1177.7085,-61.418 1184.2306,-52.6541 1189.8836,-44.5532\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1192.8979,-46.345 1195.6374,-36.1105 1187.1135,-42.4028 1192.8979,-46.345\"/>\n",
       "<text text-anchor=\"middle\" x=\"1252.6448\" y=\"-57.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Escalate for Review</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x7ff710668198>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sanction_screening_individual_flowchart.png'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot.render('sanction_screening_individual_flowchart', view=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add nuanced details: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: lightgreen is not a known color.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'sanction_screening_individual_flowchart_updated.png'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "dot = Digraph(comment='Sanction Screening Flow - Input: Individual', format='png')\n",
    "\n",
    "# Define Nodes\n",
    "dot.node('START', 'Start', shape='ellipse')\n",
    "dot.node('CHECKLIST', 'Check Final List Type', shape='diamond')\n",
    "dot.node('LENT', 'List: Entity', color='lightgreen', shape='box')\n",
    "dot.node('LVESSEL', 'List: Vessel/Aircraft', color='lightgreen', shape='box')\n",
    "dot.node('LPLACE', 'List: Place/Location', color='lightgreen', shape='box')\n",
    "dot.node('LINDIVIDUAL', 'List: Individual', color='lightgreen', shape='box')\n",
    "\n",
    "# Nodes for Address & Name Checks\n",
    "dot.node('HAS_ADDRESS', 'Has Address?', shape='diamond', color='lightblue')\n",
    "dot.node('HAS_NAME', 'Has Name Tag?', shape='diamond', color='lightblue')\n",
    "\n",
    "# Nodes for Match Types & Rules\n",
    "dot.node('FALSEMATCH1', 'False Match\\nCross Match between Input (Individual) and List (Entity)', shape='box', color='lightcoral')\n",
    "dot.node('FALSEMATCH2', 'False Match\\nCross Match between Input (Individual) and List (Vessel/Aircraft)', shape='box', color='lightcoral')\n",
    "dot.node('FALSEMATCH3', 'False Match\\nName in field with additional identifiers, not an exact match to sanctioned location', shape='box', color='lightcoral')\n",
    "dot.node('FALSEMATCH4', 'False Match\\nName in a defined individual field and list entry is a place/location', shape='box', color='lightcoral')\n",
    "dot.node('FALSEMATCH5', 'False Match\\nSingle name component & list entry is multi name component', shape='box', color='lightcoral')\n",
    "dot.node('FALSEMATCH6', 'False Match\\nInput name has additional components not matching list entry', shape='box', color='lightcoral')\n",
    "dot.node('FALSEMATCH7', 'False Match\\nFull name mismatch', shape='box', color='lightcoral')\n",
    "dot.node('POTENTIALMATCH', 'Potential Positive Match', shape='box', color='yellow')\n",
    "\n",
    "# Define Edges\n",
    "dot.edge('START', 'CHECKLIST')\n",
    "dot.edge('CHECKLIST', 'LENT', label='Entity')\n",
    "dot.edge('CHECKLIST', 'LVESSEL', label='Vessel/Aircraft')\n",
    "dot.edge('CHECKLIST', 'LPLACE', label='Place/Location')\n",
    "dot.edge('CHECKLIST', 'LINDIVIDUAL', label='Individual')\n",
    "\n",
    "# Outcomes for Entity & Vessel/Aircraft\n",
    "dot.edge('LENT', 'FALSEMATCH1')\n",
    "dot.edge('LVESSEL', 'FALSEMATCH2')\n",
    "\n",
    "# Outcomes for Place/Location\n",
    "dot.edge('LPLACE', 'HAS_ADDRESS', label='Branch')\n",
    "dot.edge('HAS_ADDRESS', 'FALSEMATCH3', label='No')\n",
    "dot.edge('HAS_ADDRESS', 'HAS_NAME', label='Yes')\n",
    "dot.edge('HAS_NAME', 'FALSEMATCH4', label='Yes')\n",
    "\n",
    "# Outcomes for Individual\n",
    "dot.edge('LINDIVIDUAL', 'FALSEMATCH5', label='Single Name Component')\n",
    "dot.edge('LINDIVIDUAL', 'FALSEMATCH6', label='Extra Name Components')\n",
    "dot.edge('LINDIVIDUAL', 'FALSEMATCH7', label='Name Mismatch')\n",
    "dot.edge('LINDIVIDUAL', 'POTENTIALMATCH', label='Potential Match')\n",
    "\n",
    "dot.render('sanction_screening_individual_flowchart_updated', view=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: lightgreen is not a known color.\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"2479pt\" height=\"471pt\"\n",
       " viewBox=\"0.00 0.00 2479.00 471.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 467)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-467 2475,-467 2475,4 -4,4\"/>\n",
       "<!-- START -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>START</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"774\" cy=\"-445\" rx=\"33.2948\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"774\" y=\"-441.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Start</text>\n",
       "</g>\n",
       "<!-- CHECKLIST -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>CHECKLIST</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"774,-390 623.016,-372 774,-354 924.984,-372 774,-390\"/>\n",
       "<text text-anchor=\"middle\" x=\"774\" y=\"-368.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Check Final List Type</text>\n",
       "</g>\n",
       "<!-- START&#45;&gt;CHECKLIST -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>START&#45;&gt;CHECKLIST</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M774,-426.9551C774,-418.8828 774,-409.1764 774,-400.1817\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"777.5001,-400.0903 774,-390.0904 770.5001,-400.0904 777.5001,-400.0903\"/>\n",
       "</g>\n",
       "<!-- LENT -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>LENT</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"441.5,-303 350.5,-303 350.5,-267 441.5,-267 441.5,-303\"/>\n",
       "<text text-anchor=\"middle\" x=\"396\" y=\"-281.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">List: Entity</text>\n",
       "</g>\n",
       "<!-- CHECKLIST&#45;&gt;LENT -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>CHECKLIST&#45;&gt;LENT</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M722.2333,-360.0854C651.264,-343.7512 524.2842,-314.5257 451.4397,-297.7599\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"452.062,-294.3117 441.5317,-295.4795 450.4919,-301.1334 452.062,-294.3117\"/>\n",
       "<text text-anchor=\"middle\" x=\"628.5\" y=\"-324.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Entity</text>\n",
       "</g>\n",
       "<!-- LVESSEL -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>LVESSEL</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"772.5,-303 619.5,-303 619.5,-267 772.5,-267 772.5,-303\"/>\n",
       "<text text-anchor=\"middle\" x=\"696\" y=\"-281.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">List: Vessel/Aircraft</text>\n",
       "</g>\n",
       "<!-- CHECKLIST&#45;&gt;LVESSEL -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>CHECKLIST&#45;&gt;LVESSEL</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M759.3288,-355.636C748.0094,-343.0105 732.1374,-325.3071 719.1152,-310.7823\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"721.5023,-308.2018 712.2209,-303.0925 716.2903,-312.8746 721.5023,-308.2018\"/>\n",
       "<text text-anchor=\"middle\" x=\"791.5\" y=\"-324.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Vessel/Aircraft</text>\n",
       "</g>\n",
       "<!-- LPLACE -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>LPLACE</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"1037.5,-303 884.5,-303 884.5,-267 1037.5,-267 1037.5,-303\"/>\n",
       "<text text-anchor=\"middle\" x=\"961\" y=\"-281.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">List: Place/Location</text>\n",
       "</g>\n",
       "<!-- CHECKLIST&#45;&gt;LPLACE -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>CHECKLIST&#45;&gt;LPLACE</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M804.8792,-357.6337C834.274,-343.9581 878.951,-323.1725 912.9914,-307.3355\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"914.6985,-310.4016 922.2889,-303.01 911.7457,-304.0549 914.6985,-310.4016\"/>\n",
       "<text text-anchor=\"middle\" x=\"930.5\" y=\"-324.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Place/Location</text>\n",
       "</g>\n",
       "<!-- LINDIVIDUAL -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>LINDIVIDUAL</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"1917.5,-303 1798.5,-303 1798.5,-267 1917.5,-267 1917.5,-303\"/>\n",
       "<text text-anchor=\"middle\" x=\"1858\" y=\"-281.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">List: Individual</text>\n",
       "</g>\n",
       "<!-- CHECKLIST&#45;&gt;LINDIVIDUAL -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>CHECKLIST&#45;&gt;LINDIVIDUAL</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M864.5568,-364.7321C1076.0342,-347.7592 1598.7663,-305.8057 1788.1043,-290.6097\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1788.5246,-294.0873 1798.2125,-289.7984 1787.9646,-287.1098 1788.5246,-294.0873\"/>\n",
       "<text text-anchor=\"middle\" x=\"1416\" y=\"-324.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Individual</text>\n",
       "</g>\n",
       "<!-- FALSEMATCH1 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>FALSEMATCH1</title>\n",
       "<polygon fill=\"none\" stroke=\"#f08080\" points=\"412,-216 0,-216 0,-178 412,-178 412,-216\"/>\n",
       "<text text-anchor=\"middle\" x=\"206\" y=\"-200.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">False Match</text>\n",
       "<text text-anchor=\"middle\" x=\"206\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Cross Match between Input (Individual) and List (Entity)</text>\n",
       "</g>\n",
       "<!-- LENT&#45;&gt;FALSEMATCH1 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>LENT&#45;&gt;FALSEMATCH1</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M357.0922,-266.9795C327.942,-253.4784 287.8142,-234.8929 256.2387,-220.2684\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"257.7017,-217.0889 247.1568,-216.0621 254.7598,-223.4407 257.7017,-217.0889\"/>\n",
       "</g>\n",
       "<!-- FALSEMATCH2 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>FALSEMATCH2</title>\n",
       "<polygon fill=\"none\" stroke=\"#f08080\" points=\"904,-216 430,-216 430,-178 904,-178 904,-216\"/>\n",
       "<text text-anchor=\"middle\" x=\"667\" y=\"-200.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">False Match</text>\n",
       "<text text-anchor=\"middle\" x=\"667\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Cross Match between Input (Individual) and List (Vessel/Aircraft)</text>\n",
       "</g>\n",
       "<!-- LVESSEL&#45;&gt;FALSEMATCH2 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>LVESSEL&#45;&gt;FALSEMATCH2</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M689.9912,-266.7663C686.1044,-254.9719 680.9607,-239.3636 676.5243,-225.9012\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"679.7562,-224.5256 673.3021,-216.1235 673.1079,-226.7166 679.7562,-224.5256\"/>\n",
       "</g>\n",
       "<!-- HAS_ADDRESS -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>HAS_ADDRESS</title>\n",
       "<polygon fill=\"none\" stroke=\"#add8e6\" points=\"1026,-215 922.2703,-197 1026,-179 1129.7297,-197 1026,-215\"/>\n",
       "<text text-anchor=\"middle\" x=\"1026\" y=\"-193.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Has Address?</text>\n",
       "</g>\n",
       "<!-- LPLACE&#45;&gt;HAS_ADDRESS -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>LPLACE&#45;&gt;HAS_ADDRESS</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M974.468,-266.7663C984.1374,-253.6755 997.2771,-235.8864 1007.868,-221.5479\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1010.9531,-223.2621 1014.0792,-213.139 1005.3225,-219.1032 1010.9531,-223.2621\"/>\n",
       "<text text-anchor=\"middle\" x=\"1023.5\" y=\"-237.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Branch</text>\n",
       "</g>\n",
       "<!-- FALSEMATCH5 -->\n",
       "<g id=\"node13\" class=\"node\">\n",
       "<title>FALSEMATCH5</title>\n",
       "<polygon fill=\"none\" stroke=\"#f08080\" points=\"1608.5,-216 1147.5,-216 1147.5,-178 1608.5,-178 1608.5,-216\"/>\n",
       "<text text-anchor=\"middle\" x=\"1378\" y=\"-200.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">False Match</text>\n",
       "<text text-anchor=\"middle\" x=\"1378\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Single name component &amp; list entry is multi name component</text>\n",
       "</g>\n",
       "<!-- LINDIVIDUAL&#45;&gt;FALSEMATCH5 -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>LINDIVIDUAL&#45;&gt;FALSEMATCH5</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M1798.1782,-274.0327C1721.8416,-260.0376 1588.1942,-235.5356 1492.1041,-217.9191\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1492.515,-214.4362 1482.0477,-216.0754 1491.2526,-221.3214 1492.515,-214.4362\"/>\n",
       "<text text-anchor=\"middle\" x=\"1750.5\" y=\"-237.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Single Name Component</text>\n",
       "</g>\n",
       "<!-- FALSEMATCH6 -->\n",
       "<g id=\"node14\" class=\"node\">\n",
       "<title>FALSEMATCH6</title>\n",
       "<polygon fill=\"none\" stroke=\"#f08080\" points=\"2089.5,-216 1626.5,-216 1626.5,-178 2089.5,-178 2089.5,-216\"/>\n",
       "<text text-anchor=\"middle\" x=\"1858\" y=\"-200.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">False Match</text>\n",
       "<text text-anchor=\"middle\" x=\"1858\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Input name has additional components not matching list entry</text>\n",
       "</g>\n",
       "<!-- LINDIVIDUAL&#45;&gt;FALSEMATCH6 -->\n",
       "<g id=\"edge13\" class=\"edge\">\n",
       "<title>LINDIVIDUAL&#45;&gt;FALSEMATCH6</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M1858,-266.7663C1858,-255.0875 1858,-239.6692 1858,-226.2978\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1861.5001,-226.1235 1858,-216.1235 1854.5001,-226.1235 1861.5001,-226.1235\"/>\n",
       "<text text-anchor=\"middle\" x=\"1948.5\" y=\"-237.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Extra Name Components</text>\n",
       "</g>\n",
       "<!-- FALSEMATCH7 -->\n",
       "<g id=\"node15\" class=\"node\">\n",
       "<title>FALSEMATCH7</title>\n",
       "<polygon fill=\"none\" stroke=\"#f08080\" points=\"2268.5,-216 2107.5,-216 2107.5,-178 2268.5,-178 2268.5,-216\"/>\n",
       "<text text-anchor=\"middle\" x=\"2188\" y=\"-200.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">False Match</text>\n",
       "<text text-anchor=\"middle\" x=\"2188\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Full name mismatch</text>\n",
       "</g>\n",
       "<!-- LINDIVIDUAL&#45;&gt;FALSEMATCH7 -->\n",
       "<g id=\"edge14\" class=\"edge\">\n",
       "<title>LINDIVIDUAL&#45;&gt;FALSEMATCH7</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M1917.6866,-275.759C1954.1946,-269.5219 2001.6693,-260.3614 2043,-249 2072.5119,-240.8875 2104.6161,-229.6359 2131.0968,-219.6733\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"2132.478,-222.8927 2140.5831,-216.0693 2129.992,-216.349 2132.478,-222.8927\"/>\n",
       "<text text-anchor=\"middle\" x=\"2146\" y=\"-237.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Name Mismatch</text>\n",
       "</g>\n",
       "<!-- POTENTIALMATCH -->\n",
       "<g id=\"node16\" class=\"node\">\n",
       "<title>POTENTIALMATCH</title>\n",
       "<polygon fill=\"none\" stroke=\"#ffff00\" points=\"2471,-215 2287,-215 2287,-179 2471,-179 2471,-215\"/>\n",
       "<text text-anchor=\"middle\" x=\"2379\" y=\"-193.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Potential Positive Match</text>\n",
       "</g>\n",
       "<!-- LINDIVIDUAL&#45;&gt;POTENTIALMATCH -->\n",
       "<g id=\"edge15\" class=\"edge\">\n",
       "<title>LINDIVIDUAL&#45;&gt;POTENTIALMATCH</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M1917.6645,-282.3667C1987.7502,-278.4684 2107.2507,-269.3026 2208,-249 2245.4906,-241.4451 2286.5179,-229.1116 2318.9864,-218.3521\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"2320.406,-221.5677 2328.7747,-215.0704 2318.1808,-214.9307 2320.406,-221.5677\"/>\n",
       "<text text-anchor=\"middle\" x=\"2319.5\" y=\"-237.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Potential Match</text>\n",
       "</g>\n",
       "<!-- HAS_NAME -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>HAS_NAME</title>\n",
       "<polygon fill=\"none\" stroke=\"#add8e6\" points=\"708,-126 590.1955,-108 708,-90 825.8045,-108 708,-126\"/>\n",
       "<text text-anchor=\"middle\" x=\"708\" y=\"-104.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Has Name Tag?</text>\n",
       "</g>\n",
       "<!-- HAS_ADDRESS&#45;&gt;HAS_NAME -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>HAS_ADDRESS&#45;&gt;HAS_NAME</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M986.0471,-185.8182C928.6481,-169.7537 822.8177,-140.1345 759.5776,-122.4352\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"760.3809,-119.0256 749.8076,-119.7009 758.4942,-125.7666 760.3809,-119.0256\"/>\n",
       "<text text-anchor=\"middle\" x=\"899.5\" y=\"-148.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Yes</text>\n",
       "</g>\n",
       "<!-- FALSEMATCH3 -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>FALSEMATCH3</title>\n",
       "<polygon fill=\"none\" stroke=\"#f08080\" points=\"1450.5,-127 843.5,-127 843.5,-89 1450.5,-89 1450.5,-127\"/>\n",
       "<text text-anchor=\"middle\" x=\"1147\" y=\"-111.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">False Match</text>\n",
       "<text text-anchor=\"middle\" x=\"1147\" y=\"-96.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Name in field with additional identifiers, not an exact match to sanctioned location</text>\n",
       "</g>\n",
       "<!-- HAS_ADDRESS&#45;&gt;FALSEMATCH3 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>HAS_ADDRESS&#45;&gt;FALSEMATCH3</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M1045.9807,-182.3035C1064.1136,-168.9661 1091.2338,-149.0181 1112.8098,-133.1481\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1115.0932,-135.8135 1121.075,-127.0688 1110.9455,-130.1746 1115.0932,-135.8135\"/>\n",
       "<text text-anchor=\"middle\" x=\"1104\" y=\"-148.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">No</text>\n",
       "</g>\n",
       "<!-- FALSEMATCH4 -->\n",
       "<g id=\"node12\" class=\"node\">\n",
       "<title>FALSEMATCH4</title>\n",
       "<polygon fill=\"none\" stroke=\"#f08080\" points=\"952,-38 464,-38 464,0 952,0 952,-38\"/>\n",
       "<text text-anchor=\"middle\" x=\"708\" y=\"-22.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">False Match</text>\n",
       "<text text-anchor=\"middle\" x=\"708\" y=\"-7.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Name in a defined individual field and list entry is a place/location</text>\n",
       "</g>\n",
       "<!-- HAS_NAME&#45;&gt;FALSEMATCH4 -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>HAS_NAME&#45;&gt;FALSEMATCH4</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M708,-89.9895C708,-78.0087 708,-62.0001 708,-48.2178\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"711.5001,-48.2158 708,-38.2158 704.5001,-48.2158 711.5001,-48.2158\"/>\n",
       "<text text-anchor=\"middle\" x=\"720.5\" y=\"-59.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Yes</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x7ff71929ef98>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sanction_screening_entity_flowchart.png'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "dot = Digraph(comment='Sanction Screening Flow - Input: Entity', format='png')\n",
    "\n",
    "# Define Nodes\n",
    "dot.node('START', 'Start\\n(Input Type: Entity)', shape='ellipse', color='blue')\n",
    "dot.node('CHECKLIST', 'Check Final List Type', shape='diamond')\n",
    "\n",
    "dot.node('LPLACE', 'List: Place/Location', color='green', shape='box')\n",
    "dot.node('LVESSEL', 'List: Vessel/Aircraft', color='green', shape='box')\n",
    "dot.node('LENT', 'List: Entity', color='green', shape='box')\n",
    "dot.node('LINDIVIDUAL', 'List: Individual', color='green', shape='box')\n",
    "\n",
    "# Nodes for Entity Specific Guidelines\n",
    "dot.node('MATCHDIFF', 'False Match\\nMatching components are different', shape='box', color='lightcoral')\n",
    "dot.node('COMMONREF', 'False Match\\nCommon references but additional material name components', shape='box', color='lightcoral')\n",
    "dot.node('FINNONMATCHBIC', 'False Match\\nFinancial institutions with non-matching BIC', shape='box', color='lightcoral')\n",
    "dot.node('NONFINMATCHBIC', 'False Match\\nNon-financial institution matching against financial institution BIC', shape='box', color='lightcoral')\n",
    "\n",
    "# Nodes for Individual Specific Guidelines\n",
    "dot.node('NONEXUS', 'False Match\\nNo nexus or connection between entity and individual', shape='box', color='lightcoral')\n",
    "\n",
    "# Nodes for Escalation & False Matches\n",
    "dot.node('ESCALATE_PLACE', 'Escalate\\nPlace/Location requires escalation', shape='box', color='yellow')\n",
    "dot.node('FALSEMATCH_VESSEL', 'False Match\\nCross Match between Input (Entity) and List (Vessel/Aircraft)', shape='box', color='lightcoral')\n",
    "dot.node('ESCALATE_ENTITY', 'Escalate\\nPotential match for Entity', shape='box', color='yellow')\n",
    "dot.node('ESCALATE_INDIVIDUAL', 'Escalate\\nPotential match for Individual', shape='box', color='yellow')\n",
    "\n",
    "# Define Edges\n",
    "dot.edge('START', 'CHECKLIST')\n",
    "dot.edge('CHECKLIST', 'LPLACE', label='Place/Location')\n",
    "dot.edge('CHECKLIST', 'LVESSEL', label='Vessel/Aircraft')\n",
    "dot.edge('CHECKLIST', 'LENT', label='Entity')\n",
    "dot.edge('CHECKLIST', 'LINDIVIDUAL', label='Individual')\n",
    "\n",
    "# Outcomes for Place/Location & Vessel/Aircraft\n",
    "dot.edge('LPLACE', 'ESCALATE_PLACE')\n",
    "dot.edge('LVESSEL', 'FALSEMATCH_VESSEL')\n",
    "\n",
    "# Outcomes for Entity\n",
    "dot.edge('LENT', 'MATCHDIFF', label='False Match\\nMatching components different')\n",
    "dot.edge('LENT', 'COMMONREF', label='False Match\\nCommon references')\n",
    "dot.edge('LENT', 'FINNONMATCHBIC', label='False Match\\nFinancial institutions non-matching BIC')\n",
    "dot.edge('LENT', 'NONFINMATCHBIC', label='False Match\\nNon-financial vs financial BIC')\n",
    "dot.edge('LENT', 'ESCALATE_ENTITY', label='Potential Match')\n",
    "\n",
    "# Outcomes for Individual\n",
    "dot.edge('LINDIVIDUAL', 'NONEXUS', label='No nexus/connection')\n",
    "dot.edge('LINDIVIDUAL', 'ESCALATE_INDIVIDUAL', label='Potential Match')\n",
    "\n",
    "# Render the graph\n",
    "dot.render('sanction_screening_entity_flowchart')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"3231pt\" height=\"326pt\"\n",
       " viewBox=\"0.00 0.00 3231.00 325.74\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 321.7401)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-321.7401 3227,-321.7401 3227,4 -4,4\"/>\n",
       "<!-- START -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>START</title>\n",
       "<ellipse fill=\"none\" stroke=\"#0000ff\" cx=\"1295\" cy=\"-290.8701\" rx=\"106.5467\" ry=\"26.7407\"/>\n",
       "<text text-anchor=\"middle\" x=\"1295\" y=\"-294.6701\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Start</text>\n",
       "<text text-anchor=\"middle\" x=\"1295\" y=\"-279.6701\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">(Input Type: Entity)</text>\n",
       "</g>\n",
       "<!-- CHECKLIST -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>CHECKLIST</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"1295,-227 1144.016,-209 1295,-191 1445.984,-209 1295,-227\"/>\n",
       "<text text-anchor=\"middle\" x=\"1295\" y=\"-205.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Check Final List Type</text>\n",
       "</g>\n",
       "<!-- START&#45;&gt;CHECKLIST -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>START&#45;&gt;CHECKLIST</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M1295,-263.6085C1295,-255.2357 1295,-246.0006 1295,-237.5444\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1298.5001,-237.3732 1295,-227.3732 1291.5001,-237.3733 1298.5001,-237.3732\"/>\n",
       "</g>\n",
       "<!-- LPLACE -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>LPLACE</title>\n",
       "<polygon fill=\"none\" stroke=\"#00ff00\" points=\"620.5,-140 467.5,-140 467.5,-104 620.5,-104 620.5,-140\"/>\n",
       "<text text-anchor=\"middle\" x=\"544\" y=\"-118.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">List: Place/Location</text>\n",
       "</g>\n",
       "<!-- CHECKLIST&#45;&gt;LPLACE -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>CHECKLIST&#45;&gt;LPLACE</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M1218.1591,-200.0983C1077.2818,-183.7783 778.5737,-149.1743 630.6839,-132.0419\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"631.0687,-128.5632 620.7323,-130.8891 630.2631,-135.5167 631.0687,-128.5632\"/>\n",
       "<text text-anchor=\"middle\" x=\"1016.5\" y=\"-161.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Place/Location</text>\n",
       "</g>\n",
       "<!-- LVESSEL -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>LVESSEL</title>\n",
       "<polygon fill=\"none\" stroke=\"#00ff00\" points=\"1202.5,-140 1049.5,-140 1049.5,-104 1202.5,-104 1202.5,-140\"/>\n",
       "<text text-anchor=\"middle\" x=\"1126\" y=\"-118.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">List: Vessel/Aircraft</text>\n",
       "</g>\n",
       "<!-- CHECKLIST&#45;&gt;LVESSEL -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>CHECKLIST&#45;&gt;LVESSEL</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M1266.3315,-194.2416C1240.0455,-180.7098 1200.7126,-160.4615 1170.3706,-144.8417\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1171.7602,-141.6205 1161.2672,-140.1553 1168.5563,-147.8443 1171.7602,-141.6205\"/>\n",
       "<text text-anchor=\"middle\" x=\"1272.5\" y=\"-161.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Vessel/Aircraft</text>\n",
       "</g>\n",
       "<!-- LENT -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>LENT</title>\n",
       "<polygon fill=\"none\" stroke=\"#00ff00\" points=\"1516.5,-140 1425.5,-140 1425.5,-104 1516.5,-104 1516.5,-140\"/>\n",
       "<text text-anchor=\"middle\" x=\"1471\" y=\"-118.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">List: Entity</text>\n",
       "</g>\n",
       "<!-- CHECKLIST&#45;&gt;LENT -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>CHECKLIST&#45;&gt;LENT</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M1324.4584,-194.4382C1351.9796,-180.8339 1393.4814,-160.3188 1425.3011,-144.5898\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1427.0047,-147.652 1434.4183,-140.083 1423.9028,-141.3768 1427.0047,-147.652\"/>\n",
       "<text text-anchor=\"middle\" x=\"1414.5\" y=\"-161.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Entity</text>\n",
       "</g>\n",
       "<!-- LINDIVIDUAL -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>LINDIVIDUAL</title>\n",
       "<polygon fill=\"none\" stroke=\"#00ff00\" points=\"2837.5,-140 2718.5,-140 2718.5,-104 2837.5,-104 2837.5,-140\"/>\n",
       "<text text-anchor=\"middle\" x=\"2778\" y=\"-118.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">List: Individual</text>\n",
       "</g>\n",
       "<!-- CHECKLIST&#45;&gt;LINDIVIDUAL -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>CHECKLIST&#45;&gt;LINDIVIDUAL</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M1396.3489,-203.0544C1678.5649,-186.4982 2468.4229,-140.1613 2708.1401,-126.0983\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"2708.6851,-129.5725 2718.4629,-125.4927 2708.2751,-122.5845 2708.6851,-129.5725\"/>\n",
       "<text text-anchor=\"middle\" x=\"2161\" y=\"-161.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Individual</text>\n",
       "</g>\n",
       "<!-- ESCALATE_PLACE -->\n",
       "<g id=\"node12\" class=\"node\">\n",
       "<title>ESCALATE_PLACE</title>\n",
       "<polygon fill=\"none\" stroke=\"#ffff00\" points=\"260,-38 0,-38 0,0 260,0 260,-38\"/>\n",
       "<text text-anchor=\"middle\" x=\"130\" y=\"-22.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Escalate</text>\n",
       "<text text-anchor=\"middle\" x=\"130\" y=\"-7.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Place/Location requires escalation</text>\n",
       "</g>\n",
       "<!-- LPLACE&#45;&gt;ESCALATE_PLACE -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>LPLACE&#45;&gt;ESCALATE_PLACE</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M471.413,-103.9409C400.3932,-86.2717 292.1839,-59.3501 216.6838,-40.5663\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"217.2677,-37.1049 206.7185,-38.087 215.5777,-43.8978 217.2677,-37.1049\"/>\n",
       "</g>\n",
       "<!-- FALSEMATCH_VESSEL -->\n",
       "<g id=\"node13\" class=\"node\">\n",
       "<title>FALSEMATCH_VESSEL</title>\n",
       "<polygon fill=\"none\" stroke=\"#f08080\" points=\"724,-38 278,-38 278,0 724,0 724,-38\"/>\n",
       "<text text-anchor=\"middle\" x=\"501\" y=\"-22.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">False Match</text>\n",
       "<text text-anchor=\"middle\" x=\"501\" y=\"-7.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Cross Match between Input (Entity) and List (Vessel/Aircraft)</text>\n",
       "</g>\n",
       "<!-- LVESSEL&#45;&gt;FALSEMATCH_VESSEL -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>LVESSEL&#45;&gt;FALSEMATCH_VESSEL</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M1049.3627,-109.3702C943.9965,-92.0058 753.1021,-60.5464 626.6722,-39.7108\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"626.9814,-36.2146 616.5453,-38.0419 625.8431,-43.1214 626.9814,-36.2146\"/>\n",
       "</g>\n",
       "<!-- MATCHDIFF -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>MATCHDIFF</title>\n",
       "<polygon fill=\"none\" stroke=\"#f08080\" points=\"1010,-38 742,-38 742,0 1010,0 1010,-38\"/>\n",
       "<text text-anchor=\"middle\" x=\"876\" y=\"-22.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">False Match</text>\n",
       "<text text-anchor=\"middle\" x=\"876\" y=\"-7.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Matching components are different</text>\n",
       "</g>\n",
       "<!-- LENT&#45;&gt;MATCHDIFF -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>LENT&#45;&gt;MATCHDIFF</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M1425.2506,-119.8675C1357.2915,-116.1358 1225.8741,-106.7495 1116,-86 1056.7447,-74.8098 990.8606,-55.9406 943.136,-41.0731\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"944.1281,-37.7162 933.5389,-38.0611 942.0319,-44.395 944.1281,-37.7162\"/>\n",
       "<text text-anchor=\"middle\" x=\"1227.5\" y=\"-74.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">False Match</text>\n",
       "<text text-anchor=\"middle\" x=\"1227.5\" y=\"-59.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Matching components different</text>\n",
       "</g>\n",
       "<!-- COMMONREF -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>COMMONREF</title>\n",
       "<polygon fill=\"none\" stroke=\"#f08080\" points=\"1498,-38 1028,-38 1028,0 1498,0 1498,-38\"/>\n",
       "<text text-anchor=\"middle\" x=\"1263\" y=\"-22.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">False Match</text>\n",
       "<text text-anchor=\"middle\" x=\"1263\" y=\"-7.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Common references but additional material name components</text>\n",
       "</g>\n",
       "<!-- LENT&#45;&gt;COMMONREF -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>LENT&#45;&gt;COMMONREF</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M1434.2926,-103.8228C1399.8878,-86.7858 1348.2645,-61.2223 1310.6858,-42.6136\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1311.9681,-39.3431 1301.4535,-38.0419 1308.8617,-45.6161 1311.9681,-39.3431\"/>\n",
       "<text text-anchor=\"middle\" x=\"1472\" y=\"-74.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">False Match</text>\n",
       "<text text-anchor=\"middle\" x=\"1472\" y=\"-59.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Common references</text>\n",
       "</g>\n",
       "<!-- FINNONMATCHBIC -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>FINNONMATCHBIC</title>\n",
       "<polygon fill=\"none\" stroke=\"#f08080\" points=\"1844,-38 1516,-38 1516,0 1844,0 1844,-38\"/>\n",
       "<text text-anchor=\"middle\" x=\"1680\" y=\"-22.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">False Match</text>\n",
       "<text text-anchor=\"middle\" x=\"1680\" y=\"-7.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Financial institutions with non&#45;matching BIC</text>\n",
       "</g>\n",
       "<!-- LENT&#45;&gt;FINNONMATCHBIC -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>LENT&#45;&gt;FINNONMATCHBIC</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M1511.7711,-103.8743C1524.1108,-98.2668 1537.6601,-91.9872 1550,-86 1578.585,-72.1308 1610.2856,-55.8121 1635.2263,-42.7523\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1636.8636,-45.8458 1644.0909,-38.0988 1633.6099,-39.6479 1636.8636,-45.8458\"/>\n",
       "<text text-anchor=\"middle\" x=\"1746.5\" y=\"-74.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">False Match</text>\n",
       "<text text-anchor=\"middle\" x=\"1746.5\" y=\"-59.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Financial institutions non&#45;matching BIC</text>\n",
       "</g>\n",
       "<!-- NONFINMATCHBIC -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>NONFINMATCHBIC</title>\n",
       "<polygon fill=\"none\" stroke=\"#f08080\" points=\"2344,-38 1862,-38 1862,0 2344,0 2344,-38\"/>\n",
       "<text text-anchor=\"middle\" x=\"2103\" y=\"-22.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">False Match</text>\n",
       "<text text-anchor=\"middle\" x=\"2103\" y=\"-7.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Non&#45;financial institution matching against financial institution BIC</text>\n",
       "</g>\n",
       "<!-- LENT&#45;&gt;NONFINMATCHBIC -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>LENT&#45;&gt;NONFINMATCHBIC</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M1516.5109,-119.3348C1609.5556,-113.6803 1818.6989,-99.8607 1889,-86 1942.1533,-75.5202 2000.7549,-56.6358 2043.1748,-41.5809\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"2044.5496,-44.8062 2052.7829,-38.1383 2042.1884,-38.2165 2044.5496,-44.8062\"/>\n",
       "<text text-anchor=\"middle\" x=\"2100\" y=\"-74.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">False Match</text>\n",
       "<text text-anchor=\"middle\" x=\"2100\" y=\"-59.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Non&#45;financial vs financial BIC</text>\n",
       "</g>\n",
       "<!-- ESCALATE_ENTITY -->\n",
       "<g id=\"node14\" class=\"node\">\n",
       "<title>ESCALATE_ENTITY</title>\n",
       "<polygon fill=\"none\" stroke=\"#ffff00\" points=\"2559.5,-38 2362.5,-38 2362.5,0 2559.5,0 2559.5,-38\"/>\n",
       "<text text-anchor=\"middle\" x=\"2461\" y=\"-22.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Escalate</text>\n",
       "<text text-anchor=\"middle\" x=\"2461\" y=\"-7.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Potential match for Entity</text>\n",
       "</g>\n",
       "<!-- LENT&#45;&gt;ESCALATE_ENTITY -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>LENT&#45;&gt;ESCALATE_ENTITY</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M1516.9391,-121.0192C1656.4454,-117.8413 2073.8136,-106.8082 2208,-86 2271.8788,-76.0944 2342.8936,-56.5067 2393.3853,-41.0573\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"2394.6793,-44.3209 2403.2019,-38.0267 2392.6144,-37.6324 2394.6793,-44.3209\"/>\n",
       "<text text-anchor=\"middle\" x=\"2395.5\" y=\"-67.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Potential Match</text>\n",
       "</g>\n",
       "<!-- NONEXUS -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>NONEXUS</title>\n",
       "<polygon fill=\"none\" stroke=\"#f08080\" points=\"2978.5,-38 2577.5,-38 2577.5,0 2978.5,0 2978.5,-38\"/>\n",
       "<text text-anchor=\"middle\" x=\"2778\" y=\"-22.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">False Match</text>\n",
       "<text text-anchor=\"middle\" x=\"2778\" y=\"-7.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">No nexus or connection between entity and individual</text>\n",
       "</g>\n",
       "<!-- LINDIVIDUAL&#45;&gt;NONEXUS -->\n",
       "<g id=\"edge13\" class=\"edge\">\n",
       "<title>LINDIVIDUAL&#45;&gt;NONEXUS</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M2778,-103.5857C2778,-88.2756 2778,-66.2004 2778,-48.4407\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"2781.5001,-48.3036 2778,-38.3036 2774.5001,-48.3037 2781.5001,-48.3036\"/>\n",
       "<text text-anchor=\"middle\" x=\"2853\" y=\"-67.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">No nexus/connection</text>\n",
       "</g>\n",
       "<!-- ESCALATE_INDIVIDUAL -->\n",
       "<g id=\"node15\" class=\"node\">\n",
       "<title>ESCALATE_INDIVIDUAL</title>\n",
       "<polygon fill=\"none\" stroke=\"#ffff00\" points=\"3223,-38 2997,-38 2997,0 3223,0 3223,-38\"/>\n",
       "<text text-anchor=\"middle\" x=\"3110\" y=\"-22.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Escalate</text>\n",
       "<text text-anchor=\"middle\" x=\"3110\" y=\"-7.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Potential match for Individual</text>\n",
       "</g>\n",
       "<!-- LINDIVIDUAL&#45;&gt;ESCALATE_INDIVIDUAL -->\n",
       "<g id=\"edge14\" class=\"edge\">\n",
       "<title>LINDIVIDUAL&#45;&gt;ESCALATE_INDIVIDUAL</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M2837.7148,-109.9412C2866.4527,-103.6562 2901.2615,-95.3344 2932,-86 2973.8754,-73.2836 3020.1025,-55.7755 3054.9507,-41.8459\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"3056.4935,-44.998 3064.4649,-38.0189 3053.8812,-38.5037 3056.4935,-44.998\"/>\n",
       "<text text-anchor=\"middle\" x=\"3069.5\" y=\"-67.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Potential Match</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x7ff71929e438>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sanction_screening_place_location_flowchart.png'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "dot = Digraph(comment='Sanction Screening Flow - Input: Place/Location', format='png')\n",
    "\n",
    "# Define Nodes\n",
    "dot.node('START', 'Start\\nInput Type: Place/Location', shape='ellipse', color='blue')\n",
    "dot.node('CHECKLIST', 'Check Final List Type', shape='diamond')\n",
    "\n",
    "dot.node('LPLACE', 'List: Place/Location', color='green', shape='box')\n",
    "dot.node('LVESSEL', 'List: Vessel/Aircraft', color='green', shape='box')\n",
    "dot.node('LENT', 'List: Entity', color='green', shape='box')\n",
    "dot.node('LINDIVIDUAL', 'List: Individual', color='green', shape='box')\n",
    "\n",
    "# Nodes for Place/Location Specific Guidelines\n",
    "dot.node('VALIDLOC', 'False Match\\ninput address is a valid location that is not the same location as referenced in the list entry nor otherwise in a sanctioned jurisdiction', shape='box', color='lightcoral')\n",
    "\n",
    "# Nodes for Cross Match scenarios\n",
    "dot.node('CROSSMATCH_ENT', 'False Match\\nCross Match between Input (Place/Location) and List (Entity)', shape='box', color='lightcoral')\n",
    "dot.node('CROSSMATCH_IND', 'False Match\\nCross Match between Input (Place/Location) and List (Individual)', shape='box', color='lightcoral')\n",
    "dot.node('CROSSMATCH_VES', 'False Match\\nCross Match between Input (Place/Location) and List (Vessel/Aircraft)', shape='box', color='lightcoral')\n",
    "\n",
    "# Nodes for Potential Positive Match\n",
    "dot.node('POTMATCH', 'Potential Positive Match\\nEscalated for further review', shape='box', color='yellow')\n",
    "\n",
    "# Define Edges\n",
    "dot.edge('START', 'CHECKLIST')\n",
    "dot.edge('CHECKLIST', 'LPLACE', label='Place/Location')\n",
    "dot.edge('CHECKLIST', 'LVESSEL', label='Vessel/Aircraft')\n",
    "dot.edge('CHECKLIST', 'LENT', label='Entity')\n",
    "dot.edge('CHECKLIST', 'LINDIVIDUAL', label='Individual')\n",
    "\n",
    "# Outcomes for Place/Location\n",
    "dot.edge('LPLACE', 'VALIDLOC', label='Valid location different from list entry')\n",
    "dot.edge('LPLACE', 'POTMATCH', label='Potential Match')\n",
    "\n",
    "# Cross Match Scenarios for Place/Location\n",
    "dot.edge('LVESSEL', 'CROSSMATCH_VES')\n",
    "dot.edge('LENT', 'CROSSMATCH_ENT')\n",
    "dot.edge('LINDIVIDUAL', 'CROSSMATCH_IND')\n",
    "\n",
    "# Render the graph\n",
    "dot.render('sanction_screening_place_location_flowchart')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"2692pt\" height=\"311pt\"\n",
       " viewBox=\"0.00 0.00 2692.00 310.74\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 306.7401)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-306.7401 2688,-306.7401 2688,4 -4,4\"/>\n",
       "<!-- START -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>START</title>\n",
       "<ellipse fill=\"none\" stroke=\"#0000ff\" cx=\"1718\" cy=\"-275.8701\" rx=\"143.5854\" ry=\"26.7407\"/>\n",
       "<text text-anchor=\"middle\" x=\"1718\" y=\"-279.6701\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Start</text>\n",
       "<text text-anchor=\"middle\" x=\"1718\" y=\"-264.6701\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Input Type: Place/Location</text>\n",
       "</g>\n",
       "<!-- CHECKLIST -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>CHECKLIST</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"1718,-212 1567.016,-194 1718,-176 1868.984,-194 1718,-212\"/>\n",
       "<text text-anchor=\"middle\" x=\"1718\" y=\"-190.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Check Final List Type</text>\n",
       "</g>\n",
       "<!-- START&#45;&gt;CHECKLIST -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>START&#45;&gt;CHECKLIST</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M1718,-248.6085C1718,-240.2357 1718,-231.0006 1718,-222.5444\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1721.5001,-222.3732 1718,-212.3732 1714.5001,-222.3733 1721.5001,-222.3732\"/>\n",
       "</g>\n",
       "<!-- LPLACE -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>LPLACE</title>\n",
       "<polygon fill=\"none\" stroke=\"#00ff00\" points=\"1169.5,-125 1016.5,-125 1016.5,-89 1169.5,-89 1169.5,-125\"/>\n",
       "<text text-anchor=\"middle\" x=\"1093\" y=\"-103.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">List: Place/Location</text>\n",
       "</g>\n",
       "<!-- CHECKLIST&#45;&gt;LPLACE -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>CHECKLIST&#45;&gt;LPLACE</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M1648.1238,-184.2732C1533.2172,-168.2782 1304.6658,-136.4639 1179.646,-119.0611\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1180.1081,-115.5918 1169.721,-117.6796 1179.1429,-122.5249 1180.1081,-115.5918\"/>\n",
       "<text text-anchor=\"middle\" x=\"1494.5\" y=\"-146.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Place/Location</text>\n",
       "</g>\n",
       "<!-- LVESSEL -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>LVESSEL</title>\n",
       "<polygon fill=\"none\" stroke=\"#00ff00\" points=\"1639.5,-125 1486.5,-125 1486.5,-89 1639.5,-89 1639.5,-125\"/>\n",
       "<text text-anchor=\"middle\" x=\"1563\" y=\"-103.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">List: Vessel/Aircraft</text>\n",
       "</g>\n",
       "<!-- CHECKLIST&#45;&gt;LVESSEL -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>CHECKLIST&#45;&gt;LVESSEL</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M1691.3546,-179.0442C1667.4028,-165.6003 1631.8484,-145.644 1604.2295,-130.1417\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1605.6289,-126.9136 1595.1955,-125.071 1602.2026,-133.0178 1605.6289,-126.9136\"/>\n",
       "<text text-anchor=\"middle\" x=\"1701.5\" y=\"-146.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Vessel/Aircraft</text>\n",
       "</g>\n",
       "<!-- LENT -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>LENT</title>\n",
       "<polygon fill=\"none\" stroke=\"#00ff00\" points=\"1925.5,-125 1834.5,-125 1834.5,-89 1925.5,-89 1925.5,-125\"/>\n",
       "<text text-anchor=\"middle\" x=\"1880\" y=\"-103.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">List: Entity</text>\n",
       "</g>\n",
       "<!-- CHECKLIST&#45;&gt;LENT -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>CHECKLIST&#45;&gt;LENT</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M1745.4811,-179.2416C1770.5697,-165.7681 1808.0572,-145.6359 1837.0906,-130.0439\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1839.0396,-132.9701 1846.1936,-125.1553 1835.7277,-126.8031 1839.0396,-132.9701\"/>\n",
       "<text text-anchor=\"middle\" x=\"1829.5\" y=\"-146.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Entity</text>\n",
       "</g>\n",
       "<!-- LINDIVIDUAL -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>LINDIVIDUAL</title>\n",
       "<polygon fill=\"none\" stroke=\"#00ff00\" points=\"2328.5,-125 2209.5,-125 2209.5,-89 2328.5,-89 2328.5,-125\"/>\n",
       "<text text-anchor=\"middle\" x=\"2269\" y=\"-103.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">List: Individual</text>\n",
       "</g>\n",
       "<!-- CHECKLIST&#45;&gt;LINDIVIDUAL -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>CHECKLIST&#45;&gt;LINDIVIDUAL</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M1783.0986,-183.7213C1887.5167,-167.2342 2091.5844,-135.013 2199.0184,-118.0497\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"2199.7752,-121.4737 2209.107,-116.4568 2198.6834,-114.5593 2199.7752,-121.4737\"/>\n",
       "<text text-anchor=\"middle\" x=\"2062\" y=\"-146.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Individual</text>\n",
       "</g>\n",
       "<!-- VALIDLOC -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>VALIDLOC</title>\n",
       "<polygon fill=\"none\" stroke=\"#f08080\" points=\"966,-38 0,-38 0,0 966,0 966,-38\"/>\n",
       "<text text-anchor=\"middle\" x=\"483\" y=\"-22.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">False Match</text>\n",
       "<text text-anchor=\"middle\" x=\"483\" y=\"-7.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">input address is a valid location that is not the same location as referenced in the list entry nor otherwise in a sanctioned jurisdiction</text>\n",
       "</g>\n",
       "<!-- LPLACE&#45;&gt;VALIDLOC -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>LPLACE&#45;&gt;VALIDLOC</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M1016.4319,-97.2464C962.1239,-90.2263 887.5417,-80.3791 822,-71 753.0429,-61.1321 676.3134,-49.4234 613.3167,-39.6173\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"613.7471,-36.1422 603.3273,-38.0603 612.669,-43.0587 613.7471,-36.1422\"/>\n",
       "<text text-anchor=\"middle\" x=\"955.5\" y=\"-59.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Valid location different from list entry</text>\n",
       "</g>\n",
       "<!-- POTMATCH -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>POTMATCH</title>\n",
       "<polygon fill=\"none\" stroke=\"#ffff00\" points=\"1201.5,-38 984.5,-38 984.5,0 1201.5,0 1201.5,-38\"/>\n",
       "<text text-anchor=\"middle\" x=\"1093\" y=\"-22.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Potential Positive Match</text>\n",
       "<text text-anchor=\"middle\" x=\"1093\" y=\"-7.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Escalated for further review</text>\n",
       "</g>\n",
       "<!-- LPLACE&#45;&gt;POTMATCH -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>LPLACE&#45;&gt;POTMATCH</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M1093,-88.7663C1093,-77.0875 1093,-61.6692 1093,-48.2978\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1096.5001,-48.1235 1093,-38.1235 1089.5001,-48.1235 1096.5001,-48.1235\"/>\n",
       "<text text-anchor=\"middle\" x=\"1147.5\" y=\"-59.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Potential Match</text>\n",
       "</g>\n",
       "<!-- CROSSMATCH_VES -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>CROSSMATCH_VES</title>\n",
       "<polygon fill=\"none\" stroke=\"#f08080\" points=\"1728,-38 1220,-38 1220,0 1728,0 1728,-38\"/>\n",
       "<text text-anchor=\"middle\" x=\"1474\" y=\"-22.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">False Match</text>\n",
       "<text text-anchor=\"middle\" x=\"1474\" y=\"-7.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Cross Match between Input (Place/Location) and List (Vessel/Aircraft)</text>\n",
       "</g>\n",
       "<!-- LVESSEL&#45;&gt;CROSSMATCH_VES -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>LVESSEL&#45;&gt;CROSSMATCH_VES</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M1544.5591,-88.7663C1531.812,-76.1624 1514.6601,-59.2033 1500.4591,-45.1618\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1502.9126,-42.6657 1493.3408,-38.1235 1497.9909,-47.6434 1502.9126,-42.6657\"/>\n",
       "</g>\n",
       "<!-- CROSSMATCH_ENT -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>CROSSMATCH_ENT</title>\n",
       "<polygon fill=\"none\" stroke=\"#f08080\" points=\"2192,-38 1746,-38 1746,0 2192,0 2192,-38\"/>\n",
       "<text text-anchor=\"middle\" x=\"1969\" y=\"-22.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">False Match</text>\n",
       "<text text-anchor=\"middle\" x=\"1969\" y=\"-7.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Cross Match between Input (Place/Location) and List (Entity)</text>\n",
       "</g>\n",
       "<!-- LENT&#45;&gt;CROSSMATCH_ENT -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>LENT&#45;&gt;CROSSMATCH_ENT</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M1898.4409,-88.7663C1911.188,-76.1624 1928.3399,-59.2033 1942.5409,-45.1618\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1945.0091,-47.6434 1949.6592,-38.1235 1940.0874,-42.6657 1945.0091,-47.6434\"/>\n",
       "</g>\n",
       "<!-- CROSSMATCH_IND -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>CROSSMATCH_IND</title>\n",
       "<polygon fill=\"none\" stroke=\"#f08080\" points=\"2684,-38 2210,-38 2210,0 2684,0 2684,-38\"/>\n",
       "<text text-anchor=\"middle\" x=\"2447\" y=\"-22.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">False Match</text>\n",
       "<text text-anchor=\"middle\" x=\"2447\" y=\"-7.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Cross Match between Input (Place/Location) and List (Individual)</text>\n",
       "</g>\n",
       "<!-- LINDIVIDUAL&#45;&gt;CROSSMATCH_IND -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>LINDIVIDUAL&#45;&gt;CROSSMATCH_IND</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M2305.4505,-88.9795C2332.5252,-75.5943 2369.7084,-57.2116 2399.1708,-42.6459\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"2401.0294,-45.6315 2408.4426,-38.0621 2397.9271,-39.3564 2401.0294,-45.6315\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x7ff711e30be0>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sanction_screening_vessel_aircraft_flowchart.png'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "dot = Digraph(comment='Sanction Screening Flow - Input: Vessel/Aircraft', format='png')\n",
    "\n",
    "# Define Nodes\n",
    "dot.node('START', 'Start\\nInput Type: Vessel/Aircraft', shape='ellipse', color='blue')\n",
    "dot.node('CHECKLIST', 'Check Final List Type', shape='diamond')\n",
    "\n",
    "dot.node('LPLACE', 'List: Place/Location', color='green', shape='box')\n",
    "dot.node('LVESSEL', 'List: Vessel/Aircraft', color='green', shape='box')\n",
    "dot.node('LENT', 'List: Entity', color='green', shape='box')\n",
    "dot.node('LINDIVIDUAL', 'List: Individual', color='green', shape='box')\n",
    "\n",
    "# Nodes for Vessel/Aircraft Specific Guidelines\n",
    "dot.node('FALSEMATCH_INDIVIDUAL', 'False Match\\nCross Match between Input (Vessel/Aircraft) and List (Individual)', shape='box', color='lightcoral')\n",
    "dot.node('ESCALATE_PLACE', 'Escalate\\nPotential Positive Match', shape='box', color='yellow')\n",
    "dot.node('PHASE2_ENTITY', 'Phase 2: Review Needed for Entity', shape='box', color='orange')\n",
    "dot.node('PHASE2_VESSEL', 'Phase 2: Review Needed for Vessel/Aircraft', shape='box', color='orange')\n",
    "\n",
    "# Define Edges\n",
    "dot.edge('START', 'CHECKLIST')\n",
    "dot.edge('CHECKLIST', 'LPLACE', label='Place/Location')\n",
    "dot.edge('CHECKLIST', 'LVESSEL', label='Vessel/Aircraft (Phase 2)')\n",
    "dot.edge('CHECKLIST', 'LENT', label='Entity (Phase 2)')\n",
    "dot.edge('CHECKLIST', 'LINDIVIDUAL', label='Individual')\n",
    "\n",
    "# Outcomes for Vessel/Aircraft\n",
    "dot.edge('LPLACE', 'ESCALATE_PLACE')\n",
    "dot.edge('LVESSEL', 'PHASE2_VESSEL')\n",
    "dot.edge('LENT', 'PHASE2_ENTITY')\n",
    "dot.edge('LINDIVIDUAL', 'FALSEMATCH_INDIVIDUAL')\n",
    "\n",
    "# Render the graph\n",
    "dot.render('sanction_screening_vessel_aircraft_flowchart')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sanction_screening_other_input_type_flowchart.png'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "dot = Digraph(comment='Sanction Screening Flow - Input: Other Input Type', format='png')\n",
    "\n",
    "# Define Nodes\n",
    "dot.node('START', 'Start\\nInput Type: Other', shape='ellipse', color='blue')\n",
    "dot.node('CHECKLIST', 'Check Final List Type', shape='diamond')\n",
    "\n",
    "dot.node('LPLACE', 'List: Place/Location', color='green', shape='box')\n",
    "dot.node('LVESSEL', 'List: Vessel/Aircraft', color='green', shape='box')\n",
    "dot.node('LENT', 'List: Entity', color='green', shape='box')\n",
    "dot.node('LINDIVIDUAL', 'List: Individual', color='green', shape='box')\n",
    "\n",
    "# Nodes for Specific False Match Scenarios\n",
    "dot.node('CURRENCY', 'False Match\\nCurrency Reference', shape='box', color='lightcoral')\n",
    "dot.node('DATE', 'False Match\\nDate Reference', shape='box', color='lightcoral')\n",
    "dot.node('PARTIALACC', 'False Match\\nPartial Account Number', shape='box', color='lightcoral')\n",
    "dot.node('ALPHANUM', 'False Match\\nAlphanumeric Reference', shape='box', color='lightcoral')\n",
    "dot.node('KNOWNINPUT', 'False Match\\nKnown Input Type', shape='box', color='lightcoral')\n",
    "dot.node('SANCTIONINV', 'False Match\\nSanctions Investigation', shape='box', color='lightcoral')\n",
    "\n",
    "# Node for Escalation\n",
    "dot.node('ESCALATE', 'Escalate\\nPotential Positive Match scenario', shape='box', color='yellow')\n",
    "\n",
    "# Node for Review\n",
    "dot.node('REVIEW', 'Review Needed\\nNo conditions met', shape='box', color='lightyellow')\n",
    "\n",
    "# Define Edges\n",
    "dot.edge('START', 'CHECKLIST')\n",
    "dot.edge('CHECKLIST', 'LPLACE')\n",
    "dot.edge('CHECKLIST', 'LVESSEL')\n",
    "dot.edge('CHECKLIST', 'LENT')\n",
    "dot.edge('CHECKLIST', 'LINDIVIDUAL')\n",
    "\n",
    "# Outcomes for Specific False Match Scenarios\n",
    "dot.edge('LPLACE', 'CURRENCY')\n",
    "dot.edge('LPLACE', 'DATE')\n",
    "dot.edge('LPLACE', 'PARTIALACC')\n",
    "dot.edge('LPLACE', 'ALPHANUM')\n",
    "dot.edge('LPLACE', 'KNOWNINPUT')\n",
    "dot.edge('LPLACE', 'SANCTIONINV')\n",
    "\n",
    "dot.edge('LVESSEL', 'CURRENCY')\n",
    "dot.edge('LVESSEL', 'DATE')\n",
    "dot.edge('LVESSEL', 'PARTIALACC')\n",
    "dot.edge('LVESSEL', 'ALPHANUM')\n",
    "dot.edge('LVESSEL', 'KNOWNINPUT')\n",
    "dot.edge('LVESSEL', 'SANCTIONINV')\n",
    "\n",
    "dot.edge('LENT', 'CURRENCY')\n",
    "dot.edge('LENT', 'DATE')\n",
    "dot.edge('LENT', 'PARTIALACC')\n",
    "dot.edge('LENT', 'ALPHANUM')\n",
    "dot.edge('LENT', 'KNOWNINPUT')\n",
    "dot.edge('LENT', 'SANCTIONINV')\n",
    "\n",
    "dot.edge('LINDIVIDUAL', 'CURRENCY')\n",
    "dot.edge('LINDIVIDUAL', 'DATE')\n",
    "dot.edge('LINDIVIDUAL', 'PARTIALACC')\n",
    "dot.edge('LINDIVIDUAL', 'ALPHANUM')\n",
    "dot.edge('LINDIVIDUAL', 'KNOWNINPUT')\n",
    "dot.edge('LINDIVIDUAL', 'SANCTIONINV')\n",
    "\n",
    "# Outcomes for Escalation\n",
    "dot.edge('LPLACE', 'ESCALATE')\n",
    "dot.edge('LVESSEL', 'ESCALATE')\n",
    "dot.edge('LENT', 'ESCALATE')\n",
    "dot.edge('LINDIVIDUAL', 'ESCALATE')\n",
    "\n",
    "# Outcomes for Review Needed\n",
    "dot.edge('LPLACE', 'REVIEW')\n",
    "dot.edge('LVESSEL', 'REVIEW')\n",
    "dot.edge('LENT', 'REVIEW')\n",
    "dot.edge('LINDIVIDUAL', 'REVIEW')\n",
    "\n",
    "# Render the graph\n",
    "dot.render('sanction_screening_other_input_type_flowchart')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

